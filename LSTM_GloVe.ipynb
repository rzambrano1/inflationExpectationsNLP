{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 641 - NLP\n",
    "## Final Project\n",
    "\n",
    "### Prepared By: Ricardo Zambrano\n",
    "### NN Models with GloVe Embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import codecs\n",
    "import gzip\n",
    "\n",
    "import gensim\n",
    "import smart_open\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import collections\n",
    "from typing import NamedTuple\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import LSTM, Activation, Dropout, Dense, Input\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Labeled Data and Stop Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the path where labeled text files are located \n",
    "txtFilesPath = r\"C:\\Users\\rzamb\\Desktop\\Desktop\\UMD\\641_Natural_Language_Processing\\finalProject\\txtFiles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractLalebedFile(filePath):\n",
    "    \"\"\"Assumes a file path for a labeled news article saved as a .txt file, a string.\n",
    "    Returns a dictionary with the news article data\"\"\"\n",
    "    \n",
    "    # Define regular expression to recognize labels in the labeled text files\n",
    "    re1 = re.compile(r\"^source\")\n",
    "    re2 = re.compile(r\"^date\")\n",
    "    re3 = re.compile(r\"^section\")\n",
    "    re4 = re.compile(r\"^byline\")\n",
    "    re5 = re.compile(r\"^inflationPosition\")\n",
    "    re6 = re.compile(r\"^title\")\n",
    "    re7 = re.compile(r\"^subtitle\")\n",
    "    re8 = re.compile(r\"body\")\n",
    "    re9 = re.compile(r\"end\")\n",
    "    \n",
    "    # Open the file in the given path and read the lines in the .txt file\n",
    "    with open(filePath,'r',encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        textLines = [] # Initializes a list to store the lines of the body of the news article\n",
    "\n",
    "        for indx in range(len(lines)):\n",
    "            if bool(re1.search(lines[indx])):\n",
    "                source = re.split(r\":\",lines[indx])[1].strip().replace('\"', '') # Extracts the source of the article\n",
    "            if bool(re2.search(lines[indx])):\n",
    "                dateTxt = re.split(r\":\",lines[indx])[1].strip().replace('\"', '') # Extracts the date of the publication...\n",
    "                try:\n",
    "                    datePublish = datetime.strptime(dateTxt, '%Y-%m-%d')             # ... and saves it as a datetime\n",
    "                except:\n",
    "                    print(\"Cannot cast date at \",filePath,\" article as datetime\")\n",
    "                    datePublish = np.nan\n",
    "            if bool(re3.search(lines[indx])):\n",
    "                paperSection = re.split(r\":\",lines[indx])[1].strip().replace('\"', '') # Extracts the section of the publication\n",
    "            if bool(re4.search(lines[indx])):\n",
    "                byline = re.split(r\":\",lines[indx])[1].strip().replace('\"', '') # Extracts the author name(s)\n",
    "            if bool(re5.search(lines[indx])):\n",
    "                inflationLabel = re.split(r\":\",lines[indx])[1].strip().replace('\"', '') # Extracts the label given to the article\n",
    "            if bool(re6.search(lines[indx])):\n",
    "                currTitle = re.split(r\":\",lines[indx])[1].strip().replace('\"', '') # Extracts the title of the news piece\n",
    "            if bool(re7.search(lines[indx])):\n",
    "                currSubtitle = re.split(r\":\",lines[indx])[1].strip().replace('\"', '') # Extracts the subtitle of the news piece\n",
    "            if bool(re8.search(lines[indx])):\n",
    "                bodyStart = indx+1 # Recoords the line number where the body of the article starts\n",
    "            if bool(re9.search(lines[indx])):\n",
    "                bodyEnd = indx # Recoords the line number where the body of the article ends\n",
    "                \n",
    "        for indx in range(bodyStart,bodyEnd): # Extracts and format the main text of the article\n",
    "            currLine = lines[indx].lower() # .lower() replaces capital letters with lower case letters\n",
    "            currLine = currLine.translate(str.maketrans('', '', string.punctuation)) # To remove punctuation\n",
    "            currLine = currLine.replace(\"\\n\", \"\") # To remove new line \\n\n",
    "\n",
    "            textLines.append(currLine)\n",
    "            \n",
    "        #articleInfo = (source,datePublish,paperSection,currTitle,currSubtitle,byline,inflationLabel)\n",
    "        mainText = ' '.join(textLines) # Joins the lines of the article in a single string\n",
    "\n",
    "        # Final touches to pre-process the main text of the article\n",
    "        # To eliminate trailing or leading space\n",
    "\n",
    "        if mainText[0].isspace():\n",
    "            mainText = mainText[1:]\n",
    "\n",
    "        if mainText[len(mainText)-1].isspace():\n",
    "            mainText = mainText[:(len(mainText)-1)]\n",
    "        \n",
    "        #Builds the dictionary with the news article and the metadata associated with the article\n",
    "        articleData = {'authors':byline,\n",
    "                       'date_publish':datePublish,\n",
    "                       'section':paperSection,\n",
    "                       'publisher':source,\n",
    "                       'title':currTitle,\n",
    "                       'subtitle':currSubtitle,\n",
    "                       'label':inflationLabel,\n",
    "                       'maintext':mainText\n",
    "                      }\n",
    "\n",
    "        f.close()\n",
    "        \n",
    "        return(articleData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLabeledFiles(directory):\n",
    "    \"\"\"Assumes a path to a directory where labeled news articles are saved in .txt format, a strig\n",
    "    Returns a dictionary with all article data, the article data is a dict\"\"\"\n",
    "    \n",
    "    txtFilesPath =  directory\n",
    "    listTxtFiles = os.listdir(txtFilesPath)\n",
    "    \n",
    "    labeledArticles = {}\n",
    "    \n",
    "    for indx in range(len(listTxtFiles)):\n",
    "        currFilePath = os.path.join(txtFilesPath,listTxtFiles[indx]) # Creates the path for a specific file\n",
    "        articleMetadata = extractLalebedFile(currFilePath) # Extracts the data from the labeled news article\n",
    "        labeledArticles[indx] = articleMetadata # Adds article to dict\n",
    "    \n",
    "    return(labeledArticles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a set of stoplist words from filename, assuming it contains one word per line\n",
    "# Return a python Set data structure (https://www.w3schools.com/python/python_sets.asp)\n",
    "def load_stopwords(filename):\n",
    "    stopwords = []\n",
    "    with codecs.open(filename, 'r', encoding='ascii', errors='ignore') as fp:\n",
    "        stopwords = fp.read().split('\\n')\n",
    "    return set(stopwords)\n",
    "\n",
    "# Loading stopwords in order to get smaller vectors\n",
    "stopwords_file = 'mallet_en_stoplist.txt'\n",
    "stop_words = load_stopwords(stopwords_file)\n",
    "stop_words = list(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot cast date at  C:\\Users\\rzamb\\Desktop\\Desktop\\UMD\\641_Natural_Language_Processing\\finalProject\\txtFiles\\file18.txt  article as datetime\n"
     ]
    }
   ],
   "source": [
    "labeledNews = readLabeledFiles(txtFilesPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'authors': 'Ana Swanson',\n",
       " 'date_publish': datetime.datetime(2022, 8, 25, 0, 0),\n",
       " 'section': 'Section B; Column 0; Business/Financial Desk; Pg. 2',\n",
       " 'publisher': 'The New York Times',\n",
       " 'title': 'Consumer Demand Is Key To Rise in U.S.',\n",
       " 'subtitle': '',\n",
       " 'label': 'Expect Inflation',\n",
       " 'maintext': 'research has found that americans spending during the pandemic accounted for about 60 of inflation from 201921  supply chain bottlenecks and labor shortages have been a major factor driving inflation in the united states though surging consumer demand ultimately did more to drive up prices in the last two years according to researchers at the federal reserve bank of new york the university of maryland and harvard university  in a blog post on wednesday julian di giovanni the head of climate risk studies in the new york feds research and statistics group summarized findings from a paper presented in june that found higher consumer demand for all types of products during the pandemic was responsible for roughly 60 percent of the inflation in the united states between 2019 and 2021  supply shocks  which include shortages of workers raw materials and shipping containers needed to produce and move goods globally  accounted for the remaining 40 percent of inflation in the model with 58 of 66 industrial sectors that the research identified experiencing supply constraints  the researchers concluded that without supply bottlenecks inflation in the united states would have been 6 percent at the end of 2021 instead of 9 percent the research finds that demand shocks played a larger role in explaining inflation in the united states whereas supply chain bottlenecks have done more to fuel inflation in europe  the bottom line of this decomposition is that supply constraints magnified the impact of higher demand in inflation mr di giovanni wrote the findings provide one answer to a debate that policymakers and politicians have been wrestling with about the nature of inflation which slowed slightly to 85 percent in july while many economists point to the governments generous spending to support americans during the pandemic as a key factor fueling inflation the biden administration has often blamed global supply chain issues and rising fuel prices stemming from the russian invasion of ukraine  the debate has important implications for the actions policymakers can take to fight price increases the federal reserve has aggressively raised interest rates to try to cool consumer demand and the economy but it has no tools to alleviate supply constraints  congress and the biden administration have begun making big investments in infrastructure and providing incentives for manufacturers of key products like semiconductors to invest in the united states but the impact from those policies will take years to be felt  there have been signs recently that supply chain shocks are easing and mr di giovanni said that could be good news for the us inflation rate in the absence of new energy shocks or other surprises its possible that the easing of bottlenecks in the supply chain will cause a substantial drop in inflation in the near term he wrote wednesday'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the dict was loaded correctly\n",
    "labeledNews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am creating this class in order to keep the labeled articls in an inmutable data structure\n",
    "class labeledArticle(NamedTuple):\n",
    "    \"\"\"A class for news articles that have been labeled by a human\"\"\"\n",
    "    articleID: int\n",
    "    label: str\n",
    "    body: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating instances of labeledArticle. The idea is to be able to shuffle and split the articles into \n",
    "# training set and test set while keeping track of the labels and article IDs\n",
    "articlesSample = []\n",
    "for key,value in labeledNews.items():\n",
    "    articlesSample.append(labeledArticle(articleID=key,label=value.get('label'),body=value.get('maintext')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[labeledArticle(articleID=0, label='Expect Inflation', body='research has found that americans spending during the pandemic accounted for about 60 of inflation from 201921  supply chain bottlenecks and labor shortages have been a major factor driving inflation in the united states though surging consumer demand ultimately did more to drive up prices in the last two years according to researchers at the federal reserve bank of new york the university of maryland and harvard university  in a blog post on wednesday julian di giovanni the head of climate risk studies in the new york feds research and statistics group summarized findings from a paper presented in june that found higher consumer demand for all types of products during the pandemic was responsible for roughly 60 percent of the inflation in the united states between 2019 and 2021  supply shocks  which include shortages of workers raw materials and shipping containers needed to produce and move goods globally  accounted for the remaining 40 percent of inflation in the model with 58 of 66 industrial sectors that the research identified experiencing supply constraints  the researchers concluded that without supply bottlenecks inflation in the united states would have been 6 percent at the end of 2021 instead of 9 percent the research finds that demand shocks played a larger role in explaining inflation in the united states whereas supply chain bottlenecks have done more to fuel inflation in europe  the bottom line of this decomposition is that supply constraints magnified the impact of higher demand in inflation mr di giovanni wrote the findings provide one answer to a debate that policymakers and politicians have been wrestling with about the nature of inflation which slowed slightly to 85 percent in july while many economists point to the governments generous spending to support americans during the pandemic as a key factor fueling inflation the biden administration has often blamed global supply chain issues and rising fuel prices stemming from the russian invasion of ukraine  the debate has important implications for the actions policymakers can take to fight price increases the federal reserve has aggressively raised interest rates to try to cool consumer demand and the economy but it has no tools to alleviate supply constraints  congress and the biden administration have begun making big investments in infrastructure and providing incentives for manufacturers of key products like semiconductors to invest in the united states but the impact from those policies will take years to be felt  there have been signs recently that supply chain shocks are easing and mr di giovanni said that could be good news for the us inflation rate in the absence of new energy shocks or other surprises its possible that the easing of bottlenecks in the supply chain will cause a substantial drop in inflation in the near term he wrote wednesday'),\n",
       " labeledArticle(articleID=1, label='Neutral', body='so does the rich world’s current bout of inflation have minimal costs or even none at all the trouble for economists is that there is a world outside their research few people know or care about their results but they know what they think about inflation they utterly implacably loathe it  inflation seems to hold a special place in the public consciousness our analysis of englishlanguage newspapers and blog posts suggests that during the 2010s media organisations mentioned inflation 50 more frequently than they mentioned unemployment even though joblessness during that decade was a farbigger economic problem in the 1990s robert shiller of yale university asked people in a number of countries about their opinions on inflation and compared them with the views of economists he found that ordinary people held much more extreme views on the subject than the academics who study it for a living  people believe that inflation makes them poorer they worry it makes it harder to plan and they believe that inflation is a sign that unscrupulous companies are taking advantage of them twothirds of americans ascribe the recent rise in inflation to corporate greed economists by contrast are more equivocal in their answers more than half of americans “fully agreed” that preventing high inflation was as important as stopping drug abuse or maintaining educational standards compared with just 18 of economists in the same survey mr shiller found that 46 of people wanted the government to lower the price level after an inflation spike ie to engineer deflation something few economists would recommend  get real  perhaps policymakers should just ignore the views of ordinary people if experts find that inflation has surprisingly low costs then what more information is needed to guide policy another way of looking at it however is that the psychological costs of high inflation are real and that central bankers and governments should take them into account fighting inflation by forcefully tightening fiscal or monetary policy is often seen as a hardnosed choice because it cools the economy and risks provoking a recession in fact it is one of the most populist policies out there')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articlesSample[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:  Expect Inflation\n",
      "body:  research has found that americans spending during the pandemic accounted for about 60 of inflation from 201921  supply chain bottlenecks and labor shortages have been a major factor driving inflation in the united states though surging consumer demand ultimately did more to drive up prices in the last two years according to researchers at the federal reserve bank of new york the university of maryland and harvard university  in a blog post on wednesday julian di giovanni the head of climate risk studies in the new york feds research and statistics group summarized findings from a paper presented in june that found higher consumer demand for all types of products during the pandemic was responsible for roughly 60 percent of the inflation in the united states between 2019 and 2021  supply shocks  which include shortages of workers raw materials and shipping containers needed to produce and move goods globally  accounted for the remaining 40 percent of inflation in the model with 58 of 66 industrial sectors that the research identified experiencing supply constraints  the researchers concluded that without supply bottlenecks inflation in the united states would have been 6 percent at the end of 2021 instead of 9 percent the research finds that demand shocks played a larger role in explaining inflation in the united states whereas supply chain bottlenecks have done more to fuel inflation in europe  the bottom line of this decomposition is that supply constraints magnified the impact of higher demand in inflation mr di giovanni wrote the findings provide one answer to a debate that policymakers and politicians have been wrestling with about the nature of inflation which slowed slightly to 85 percent in july while many economists point to the governments generous spending to support americans during the pandemic as a key factor fueling inflation the biden administration has often blamed global supply chain issues and rising fuel prices stemming from the russian invasion of ukraine  the debate has important implications for the actions policymakers can take to fight price increases the federal reserve has aggressively raised interest rates to try to cool consumer demand and the economy but it has no tools to alleviate supply constraints  congress and the biden administration have begun making big investments in infrastructure and providing incentives for manufacturers of key products like semiconductors to invest in the united states but the impact from those policies will take years to be felt  there have been signs recently that supply chain shocks are easing and mr di giovanni said that could be good news for the us inflation rate in the absence of new energy shocks or other surprises its possible that the easing of bottlenecks in the supply chain will cause a substantial drop in inflation in the near term he wrote wednesday\n"
     ]
    }
   ],
   "source": [
    "print('label: ',articlesSample[0][1])\n",
    "print('body: ',articlesSample[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of list of labels: [articlesSample[x][1] for x in range(len(articlesSample))]\n",
    "# Example of list of article bodies: [articlesSample[x][2] for x in range(len(articlesSample))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading word2vec pre-trained embedings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyedVectors<vector_size=300, 3000000 keys>\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Inflation', 0.8240750432014465), ('inflationary_pressures', 0.8009414076805115), ('inflationary', 0.7951276302337646), ('inflationary_pressure', 0.7671657800674438), ('inflation_pressures', 0.7616896629333496), ('inflationary_expectations', 0.733973503112793), ('CPI', 0.6960082650184631), ('deflation', 0.6912639737129211), ('disinflation', 0.6745432615280151), ('interest_rates', 0.6639971733093262)]\n"
     ]
    }
   ],
   "source": [
    "sims = model.most_similar('inflation', topn=10)\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.07421875e-01, -2.01171875e-01,  1.23046875e-01,  2.11914062e-01,\n",
       "       -9.13085938e-02,  2.16796875e-01, -1.31835938e-01,  8.30078125e-02,\n",
       "        2.02148438e-01,  4.78515625e-02,  3.66210938e-02, -2.45361328e-02,\n",
       "        2.39257812e-02, -1.60156250e-01, -2.61230469e-02,  9.71679688e-02,\n",
       "       -6.34765625e-02,  1.84570312e-01,  1.70898438e-01, -1.63085938e-01,\n",
       "       -1.09375000e-01,  1.49414062e-01, -4.65393066e-04,  9.61914062e-02,\n",
       "        1.68945312e-01,  2.60925293e-03,  8.93554688e-02,  6.49414062e-02,\n",
       "        3.56445312e-02, -6.93359375e-02, -1.46484375e-01, -1.21093750e-01,\n",
       "       -2.27539062e-01,  2.45361328e-02, -1.24511719e-01, -3.18359375e-01,\n",
       "       -2.20703125e-01,  1.30859375e-01,  3.66210938e-02, -3.63769531e-02,\n",
       "       -1.13281250e-01,  1.95312500e-01,  9.76562500e-02,  1.26953125e-01,\n",
       "        6.59179688e-02,  6.93359375e-02,  1.02539062e-02,  1.75781250e-01,\n",
       "       -1.68945312e-01,  1.21307373e-03, -2.98828125e-01, -1.15234375e-01,\n",
       "        5.66406250e-02, -1.77734375e-01, -2.08984375e-01,  1.76757812e-01,\n",
       "        2.38037109e-02, -2.57812500e-01, -4.46777344e-02,  1.88476562e-01,\n",
       "        5.51757812e-02,  5.02929688e-02, -1.06933594e-01,  1.89453125e-01,\n",
       "       -1.16210938e-01,  8.49609375e-02, -1.71875000e-01,  2.45117188e-01,\n",
       "       -1.73828125e-01, -8.30078125e-03,  4.56542969e-02, -1.61132812e-02,\n",
       "        1.86523438e-01, -6.05468750e-02, -4.17480469e-02,  1.82617188e-01,\n",
       "        2.20703125e-01, -1.22558594e-01, -2.55126953e-02, -3.08593750e-01,\n",
       "        9.13085938e-02,  1.60156250e-01,  1.70898438e-01,  1.19628906e-01,\n",
       "        7.08007812e-02, -2.64892578e-02, -3.08837891e-02,  4.06250000e-01,\n",
       "       -1.01562500e-01,  5.71289062e-02, -7.26318359e-03, -9.17968750e-02,\n",
       "       -1.50390625e-01, -2.55859375e-01,  2.16796875e-01, -3.63769531e-02,\n",
       "        2.24609375e-01,  8.00781250e-02,  1.56250000e-01,  5.27343750e-02,\n",
       "        1.50390625e-01, -1.14746094e-01, -8.64257812e-02,  1.19140625e-01,\n",
       "       -7.17773438e-02,  2.73437500e-01, -1.64062500e-01,  7.29370117e-03,\n",
       "        4.21875000e-01, -1.12792969e-01, -1.35742188e-01, -1.31835938e-01,\n",
       "       -1.37695312e-01, -7.66601562e-02,  6.25000000e-02,  4.98046875e-02,\n",
       "       -1.91406250e-01, -6.03027344e-02,  2.27539062e-01,  5.88378906e-02,\n",
       "       -3.24218750e-01,  5.41992188e-02, -1.35742188e-01,  8.17871094e-03,\n",
       "       -5.24902344e-02, -1.74713135e-03, -9.81445312e-02, -2.86865234e-02,\n",
       "        3.61328125e-02,  2.15820312e-01,  5.98144531e-02, -3.08593750e-01,\n",
       "       -2.27539062e-01,  2.61718750e-01,  9.86328125e-02, -5.07812500e-02,\n",
       "        1.78222656e-02,  1.31835938e-01, -5.35156250e-01, -1.81640625e-01,\n",
       "        1.38671875e-01, -3.10546875e-01, -9.71679688e-02,  1.31835938e-01,\n",
       "       -1.16210938e-01,  7.03125000e-02,  2.85156250e-01,  3.51562500e-02,\n",
       "       -1.01562500e-01, -3.75976562e-02,  1.41601562e-01,  1.42578125e-01,\n",
       "       -5.68847656e-02,  2.65625000e-01, -2.09960938e-01,  9.64355469e-03,\n",
       "       -6.68945312e-02, -4.83398438e-02, -6.10351562e-02,  2.45117188e-01,\n",
       "       -9.66796875e-02,  1.78222656e-02, -1.27929688e-01, -4.78515625e-02,\n",
       "       -7.26318359e-03,  1.79687500e-01,  2.78320312e-02, -2.10937500e-01,\n",
       "       -1.43554688e-01, -1.27929688e-01,  1.73339844e-02, -3.60107422e-03,\n",
       "       -2.04101562e-01,  3.63159180e-03, -1.19628906e-01, -6.15234375e-02,\n",
       "        5.93261719e-02, -3.23486328e-03, -1.70898438e-01, -3.14941406e-02,\n",
       "       -8.88671875e-02, -2.89062500e-01,  3.44238281e-02, -1.87500000e-01,\n",
       "        2.94921875e-01,  1.58203125e-01, -1.19628906e-01,  7.61718750e-02,\n",
       "        6.39648438e-02, -4.68750000e-02, -6.83593750e-02,  1.21459961e-02,\n",
       "       -1.44531250e-01,  4.54101562e-02,  3.68652344e-02,  3.88671875e-01,\n",
       "        1.45507812e-01, -2.55859375e-01, -4.46777344e-02, -1.33789062e-01,\n",
       "       -1.38671875e-01,  6.59179688e-02,  1.37695312e-01,  1.14746094e-01,\n",
       "        2.03125000e-01, -4.78515625e-02,  1.80664062e-02, -8.54492188e-02,\n",
       "       -2.48046875e-01, -3.39843750e-01, -2.83203125e-02,  1.05468750e-01,\n",
       "       -2.14843750e-01, -8.74023438e-02,  7.12890625e-02,  1.87500000e-01,\n",
       "       -1.12304688e-01,  2.73437500e-01, -3.26171875e-01, -1.77734375e-01,\n",
       "       -4.24804688e-02, -2.69531250e-01,  6.64062500e-02, -6.88476562e-02,\n",
       "       -1.99218750e-01, -7.03125000e-02, -2.43164062e-01, -3.66210938e-02,\n",
       "       -7.37304688e-02, -1.77734375e-01,  9.17968750e-02, -1.25000000e-01,\n",
       "       -1.65039062e-01, -3.57421875e-01, -2.85156250e-01, -1.66992188e-01,\n",
       "        1.97265625e-01, -1.53320312e-01,  2.31933594e-02,  2.06054688e-01,\n",
       "        1.80664062e-01, -2.74658203e-02, -1.92382812e-01, -9.61914062e-02,\n",
       "       -1.06811523e-02, -4.73632812e-02,  6.54296875e-02, -1.25732422e-02,\n",
       "        1.78222656e-02, -8.00781250e-02, -2.59765625e-01,  9.37500000e-02,\n",
       "       -7.81250000e-02,  4.68750000e-02, -2.22167969e-02,  1.86767578e-02,\n",
       "        3.11279297e-02,  1.04980469e-02, -1.69921875e-01,  2.58789062e-02,\n",
       "       -3.41796875e-02, -1.44042969e-02, -5.46875000e-02, -8.78906250e-02,\n",
       "        1.96838379e-03,  2.23632812e-01, -1.36718750e-01,  1.75781250e-01,\n",
       "       -1.63085938e-01,  1.87500000e-01,  3.44238281e-02, -5.63964844e-02,\n",
       "       -2.27689743e-05,  4.27246094e-02,  5.81054688e-02, -1.07910156e-01,\n",
       "       -3.88183594e-02, -2.69531250e-01,  3.34472656e-02,  9.81445312e-02,\n",
       "        5.63964844e-02,  2.23632812e-01, -5.49316406e-02,  1.46484375e-01,\n",
       "        5.93261719e-02, -2.19726562e-01,  6.39648438e-02,  1.66015625e-02,\n",
       "        4.56542969e-02,  3.26171875e-01, -3.80859375e-01,  1.70898438e-01,\n",
       "        5.66406250e-02, -1.04492188e-01,  1.38671875e-01, -1.57226562e-01,\n",
       "        3.23486328e-03, -4.80957031e-02, -2.48046875e-01, -6.20117188e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = model['computer']  # Get numpy vector of a word\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading GloVe pre-trained embedings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_input_file = '.\\glove.6B\\glove.6B.100d.txt'\n",
    "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Stanford GloVe model\n",
    "filename = 'glove.6B.100d.txt.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyedVectors<vector_size=100, 400000 keys>\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rate', 0.7825247645378113), ('rates', 0.7814398407936096), ('unemployment', 0.7501060366630554), ('inflationary', 0.7441107630729675), ('growth', 0.7435855269432068), ('deflation', 0.7362003326416016), ('rising', 0.732311487197876), ('rise', 0.7258995771408081), ('slowing', 0.715856671333313), ('prices', 0.7068049311637878)]\n"
     ]
    }
   ],
   "source": [
    "sims = model.most_similar('inflation', topn=10)\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.6298e-01,  3.0141e-01,  5.7978e-01,  6.6548e-02,  4.5835e-01,\n",
       "       -1.5329e-01,  4.3258e-01, -8.9215e-01,  5.7747e-01,  3.6375e-01,\n",
       "        5.6524e-01, -5.6281e-01,  3.5659e-01, -3.6096e-01, -9.9662e-02,\n",
       "        5.2753e-01,  3.8839e-01,  9.6185e-01,  1.8841e-01,  3.0741e-01,\n",
       "       -8.7842e-01, -3.2442e-01,  1.1202e+00,  7.5126e-02,  4.2661e-01,\n",
       "       -6.0651e-01, -1.3893e-01,  4.7862e-02, -4.5158e-01,  9.3723e-02,\n",
       "        1.7463e-01,  1.0962e+00, -1.0044e+00,  6.3889e-02,  3.8002e-01,\n",
       "        2.1109e-01, -6.6247e-01, -4.0736e-01,  8.9442e-01, -6.0974e-01,\n",
       "       -1.8577e-01, -1.9913e-01, -6.9226e-01, -3.1806e-01, -7.8565e-01,\n",
       "        2.3831e-01,  1.2992e-01,  8.7721e-02,  4.3205e-01, -2.2662e-01,\n",
       "        3.1549e-01, -3.1748e-01, -2.4632e-03,  1.6615e-01,  4.2358e-01,\n",
       "       -1.8087e+00, -3.6699e-01,  2.3949e-01,  2.5458e+00,  3.6111e-01,\n",
       "        3.9486e-02,  4.8607e-01, -3.6974e-01,  5.7282e-02, -4.9317e-01,\n",
       "        2.2765e-01,  7.9966e-01,  2.1428e-01,  6.9811e-01,  1.1262e+00,\n",
       "       -1.3526e-01,  7.1972e-01, -9.9605e-04, -2.6842e-01, -8.3038e-01,\n",
       "        2.1780e-01,  3.4355e-01,  3.7731e-01, -4.0251e-01,  3.3124e-01,\n",
       "        1.2576e+00, -2.7196e-01, -8.6093e-01,  9.0053e-02, -2.4876e+00,\n",
       "        4.5200e-01,  6.6945e-01, -5.4648e-01, -1.0324e-01, -1.6979e-01,\n",
       "        5.9437e-01,  1.1280e+00,  7.5755e-01, -5.9160e-02,  1.5152e-01,\n",
       "       -2.8388e-01,  4.9452e-01, -9.1703e-01,  9.1289e-01, -3.0927e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = model['computer']  # Get numpy vector of a word\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split([articlesSample[x][2] for x in range(len(articlesSample))], \n",
    "                                                    [articlesSample[x][1] for x in range(len(articlesSample))], \n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function encodes the categorical labels\n",
    "Encoder = LabelEncoder()\n",
    "y_train_encoded = Encoder.fit_transform(y_train)\n",
    "y_test_encoded = Encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert integers to dummy variables. Keras expects \"one hot\" encoding for labels in multiclass problems\n",
    "y_train_labels = to_categorical(y_train_encoded)\n",
    "y_test_labels = to_categorical(y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareArticles(article,stopWords):\n",
    "    punct = string.punctuation+'“'+'”'+'’'\n",
    "    rawTokens = word_tokenize(article)\n",
    "    tokens = [token for token in rawTokens if token not in punct]\n",
    "    return [word for word in tokens if not word in stopWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean = [prepareArticles(item,stop_words) for item in X_train]\n",
    "X_test_clean = [prepareArticles(item,stop_words) for item in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step tokenize all the words in the training set. The Tokenizer basically indexes all the words in the text and each word gets a unique index. This helps in creating dictionary mapping words to an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prepros = [' '.join(item) for item in X_train_clean]\n",
    "X_test_prepros = [' '.join(item) for item in X_test_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(X_train_prepros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inflation': 1,\n",
       " 'fed': 2,\n",
       " 'prices': 3,\n",
       " 'rates': 4,\n",
       " 'year': 5,\n",
       " 'central': 6,\n",
       " 'higher': 7,\n",
       " 'price': 8,\n",
       " 'rate': 9,\n",
       " 'economy': 10,\n",
       " 'interest': 11,\n",
       " '2': 12,\n",
       " 'policy': 13,\n",
       " 'america': 14,\n",
       " 'growth': 15,\n",
       " 'federal': 16,\n",
       " 'bank': 17,\n",
       " 'supply': 18,\n",
       " 'years': 19,\n",
       " 'expectations': 20,\n",
       " 'investors': 21,\n",
       " 'market': 22,\n",
       " 'month': 23,\n",
       " 'target': 24,\n",
       " 'markets': 25,\n",
       " 'high': 26,\n",
       " 'energy': 27,\n",
       " 'low': 28,\n",
       " 'mr': 29,\n",
       " 'demand': 30,\n",
       " 'spending': 31,\n",
       " 'reserve': 32,\n",
       " 'labor': 33,\n",
       " 'economists': 34,\n",
       " 'economic': 35,\n",
       " 'world': 36,\n",
       " 'pandemic': 37,\n",
       " 'workers': 38,\n",
       " 'banks': 39,\n",
       " 'officials': 40,\n",
       " 'consumer': 41,\n",
       " 'time': 42,\n",
       " 'index': 43,\n",
       " 'months': 44,\n",
       " 'government': 45,\n",
       " 'percent': 46,\n",
       " 'bond': 47,\n",
       " 'recent': 48,\n",
       " 'wages': 49,\n",
       " 'powell': 50,\n",
       " 'rising': 51,\n",
       " 'past': 52,\n",
       " 'people': 53,\n",
       " 'increase': 54,\n",
       " 'consumers': 55,\n",
       " 'wage': 56,\n",
       " 'jobs': 57,\n",
       " 'rose': 58,\n",
       " 'raise': 59,\n",
       " 'goods': 60,\n",
       " 'increases': 61,\n",
       " 'money': 62,\n",
       " 'long': 63,\n",
       " 'fiscal': 64,\n",
       " 'monetary': 65,\n",
       " 'unemployment': 66,\n",
       " 'big': 67,\n",
       " 'manufacturing': 68,\n",
       " 'average': 69,\n",
       " 'rise': 70,\n",
       " 'lower': 71,\n",
       " 'costs': 72,\n",
       " 'change': 73,\n",
       " 'treasury': 74,\n",
       " 'global': 75,\n",
       " 'annual': 76,\n",
       " 'december': 77,\n",
       " 'governments': 78,\n",
       " 'point': 79,\n",
       " 'level': 80,\n",
       " 'expected': 81,\n",
       " 'financial': 82,\n",
       " 'food': 83,\n",
       " 'make': 84,\n",
       " 'services': 85,\n",
       " 'expect': 86,\n",
       " 'businesses': 87,\n",
       " 'biden': 88,\n",
       " 'problem': 89,\n",
       " 'back': 90,\n",
       " 'future': 91,\n",
       " 'economies': 92,\n",
       " 'march': 93,\n",
       " 'data': 94,\n",
       " 'department': 95,\n",
       " 'stimulus': 96,\n",
       " 'january': 97,\n",
       " 'early': 98,\n",
       " 'yield': 99,\n",
       " 'investment': 100,\n",
       " 'real': 101,\n",
       " 'rich': 102,\n",
       " 'good': 103,\n",
       " 'raising': 104,\n",
       " 'public': 105,\n",
       " 'pace': 106,\n",
       " 'bonds': 107,\n",
       " 'bankers': 108,\n",
       " 'measure': 109,\n",
       " 'chief': 110,\n",
       " '4': 111,\n",
       " 'tax': 112,\n",
       " 'economist': 113,\n",
       " 'president': 114,\n",
       " 'fell': 115,\n",
       " 'pay': 116,\n",
       " 'recession': 117,\n",
       " 'debt': 118,\n",
       " 'september': 119,\n",
       " 'earlier': 120,\n",
       " 'report': 121,\n",
       " 'decades': 122,\n",
       " 'companies': 123,\n",
       " 'falling': 124,\n",
       " '2021': 125,\n",
       " 'research': 126,\n",
       " 'theory': 127,\n",
       " 'hit': 128,\n",
       " 'cost': 129,\n",
       " 'core': 130,\n",
       " 'today': 131,\n",
       " 'measures': 132,\n",
       " '5': 133,\n",
       " 'american': 134,\n",
       " 'compared': 135,\n",
       " 'decade': 136,\n",
       " 'chain': 137,\n",
       " 'august': 138,\n",
       " 'meeting': 139,\n",
       " 'china': 140,\n",
       " 'states': 141,\n",
       " 'run': 142,\n",
       " 'university': 143,\n",
       " 'news': 144,\n",
       " 'november': 145,\n",
       " 'targets': 146,\n",
       " 'surge': 147,\n",
       " 'labour': 148,\n",
       " 'quarter': 149,\n",
       " '10year': 150,\n",
       " '12': 151,\n",
       " 'making': 152,\n",
       " 'pressures': 153,\n",
       " 'pressure': 154,\n",
       " 'policymakers': 155,\n",
       " 'july': 156,\n",
       " '20': 157,\n",
       " 'yields': 158,\n",
       " 'europe': 159,\n",
       " 'subsidies': 160,\n",
       " 'ira': 161,\n",
       " 'recently': 162,\n",
       " 'production': 163,\n",
       " 'end': 164,\n",
       " 'start': 165,\n",
       " 'short': 166,\n",
       " 'part': 167,\n",
       " '2020': 168,\n",
       " 'wrote': 169,\n",
       " 'week': 170,\n",
       " 'view': 171,\n",
       " 'work': 172,\n",
       " 'jerome': 173,\n",
       " 'wednesday': 174,\n",
       " 'current': 175,\n",
       " 'result': 176,\n",
       " 'support': 177,\n",
       " 'drop': 178,\n",
       " 'half': 179,\n",
       " 'inflationary': 180,\n",
       " 'administration': 181,\n",
       " 'monthly': 182,\n",
       " 'survey': 183,\n",
       " '25': 184,\n",
       " 'october': 185,\n",
       " 'roughly': 186,\n",
       " 'suggests': 187,\n",
       " 'factories': 188,\n",
       " 'isn': 189,\n",
       " 'risk': 190,\n",
       " 'americans': 191,\n",
       " 'gdp': 192,\n",
       " 'state': 193,\n",
       " 'levels': 194,\n",
       " 'slowed': 195,\n",
       " 'effects': 196,\n",
       " 'ago': 197,\n",
       " 'june': 198,\n",
       " 'underlying': 199,\n",
       " 'power': 200,\n",
       " '10': 201,\n",
       " 'slow': 202,\n",
       " 'highest': 203,\n",
       " 'act': 204,\n",
       " 'period': 205,\n",
       " 'chairman': 206,\n",
       " 'previous': 207,\n",
       " 'question': 208,\n",
       " 'late': 209,\n",
       " 'policies': 210,\n",
       " 'united': 211,\n",
       " 'term': 212,\n",
       " '1970s': 213,\n",
       " 'april': 214,\n",
       " 'european': 215,\n",
       " 'crisis': 216,\n",
       " 'idea': 217,\n",
       " 'tuesday': 218,\n",
       " '15': 219,\n",
       " 'show': 220,\n",
       " 'business': 221,\n",
       " 'things': 222,\n",
       " 'released': 223,\n",
       " 'remains': 224,\n",
       " 'percentage': 225,\n",
       " 'spring': 226,\n",
       " 'chair': 227,\n",
       " 'rises': 228,\n",
       " 'move': 229,\n",
       " 'head': 230,\n",
       " 'emerging': 231,\n",
       " 'bottlenecks': 232,\n",
       " 'job': 233,\n",
       " 'signs': 234,\n",
       " 'raised': 235,\n",
       " 'goal': 236,\n",
       " 'credits': 237,\n",
       " 'reported': 238,\n",
       " 'bigger': 239,\n",
       " 'gains': 240,\n",
       " 'taxes': 241,\n",
       " 'push': 242,\n",
       " 'york': 243,\n",
       " 'lead': 244,\n",
       " 'industries': 245,\n",
       " 'products': 246,\n",
       " 'decline': 247,\n",
       " 'shift': 248,\n",
       " 'share': 249,\n",
       " 'remain': 250,\n",
       " 'hold': 251,\n",
       " 'friday': 252,\n",
       " 'germany': 253,\n",
       " 'significant': 254,\n",
       " '2010s': 255,\n",
       " 'cochrane': 256,\n",
       " 'key': 257,\n",
       " 'fuel': 258,\n",
       " 'lot': 259,\n",
       " 'interestrate': 260,\n",
       " 'including': 261,\n",
       " 'chips': 262,\n",
       " 'important': 263,\n",
       " 'industrial': 264,\n",
       " 'congress': 265,\n",
       " 'basis': 266,\n",
       " 'bring': 267,\n",
       " 'made': 268,\n",
       " 'points': 269,\n",
       " 'shocks': 270,\n",
       " 'output': 271,\n",
       " 'productivity': 272,\n",
       " 'countries': 273,\n",
       " 'factory': 274,\n",
       " 'cut': 275,\n",
       " 'oil': 276,\n",
       " 'faster': 277,\n",
       " 'working': 278,\n",
       " 'firms': 279,\n",
       " 'spend': 280,\n",
       " 'income': 281,\n",
       " 'paper': 282,\n",
       " 'makes': 283,\n",
       " '1': 284,\n",
       " 'boom': 285,\n",
       " 'fall': 286,\n",
       " 'beginning': 287,\n",
       " 'buy': 288,\n",
       " 'slightly': 289,\n",
       " 'number': 290,\n",
       " '2022': 291,\n",
       " 'concerned': 292,\n",
       " 'coming': 293,\n",
       " 'gas': 294,\n",
       " 'households': 295,\n",
       " 'private': 296,\n",
       " 'stock': 297,\n",
       " 'longer': 298,\n",
       " 'car': 299,\n",
       " 'middle': 300,\n",
       " 'times': 301,\n",
       " 'employment': 302,\n",
       " 'stay': 303,\n",
       " 'cpi': 304,\n",
       " 'preferred': 305,\n",
       " 'expenditures': 306,\n",
       " '1980s': 307,\n",
       " '2019': 308,\n",
       " 'find': 309,\n",
       " 'found': 310,\n",
       " 'stocks': 311,\n",
       " 'housing': 312,\n",
       " 'fallen': 313,\n",
       " 'trend': 314,\n",
       " 'committee': 315,\n",
       " 'potential': 316,\n",
       " 'wrong': 317,\n",
       " 'ms': 318,\n",
       " 'political': 319,\n",
       " 'earnings': 320,\n",
       " 'return': 321,\n",
       " 'firm': 322,\n",
       " '40': 323,\n",
       " 'sectors': 324,\n",
       " 'set': 325,\n",
       " 'provide': 326,\n",
       " 'offer': 327,\n",
       " 'debts': 328,\n",
       " 'constraints': 329,\n",
       " 'hyperinflation': 330,\n",
       " 'true': 331,\n",
       " 'cuts': 332,\n",
       " 'eventually': 333,\n",
       " 'running': 334,\n",
       " 'normal': 335,\n",
       " 'putting': 336,\n",
       " 'evidence': 337,\n",
       " 'story': 338,\n",
       " 'cash': 339,\n",
       " 'debate': 340,\n",
       " 'japan': 341,\n",
       " 'quickly': 342,\n",
       " 'analysts': 343,\n",
       " 'continue': 344,\n",
       " 'major': 345,\n",
       " 'materials': 346,\n",
       " 'doesn': 347,\n",
       " 'range': 348,\n",
       " 'longterm': 349,\n",
       " 'tightening': 350,\n",
       " 'worry': 351,\n",
       " 'boost': 352,\n",
       " 'growing': 353,\n",
       " 'increased': 354,\n",
       " 'based': 355,\n",
       " 'economics': 356,\n",
       " 'harder': 357,\n",
       " 'shortterm': 358,\n",
       " 'rent': 359,\n",
       " 'politicians': 360,\n",
       " 'series': 361,\n",
       " 'hard': 362,\n",
       " 'investments': 363,\n",
       " 'solar': 364,\n",
       " 'yellen': 365,\n",
       " 'note': 366,\n",
       " 'commodity': 367,\n",
       " 'warned': 368,\n",
       " 'open': 369,\n",
       " 'won': 370,\n",
       " '3': 371,\n",
       " 'clear': 372,\n",
       " 'era': 373,\n",
       " 'put': 374,\n",
       " 'close': 375,\n",
       " 'shortages': 376,\n",
       " 'borrowing': 377,\n",
       " 'risen': 378,\n",
       " 'staff': 379,\n",
       " 'technology': 380,\n",
       " 'consumerprice': 381,\n",
       " 'euro': 382,\n",
       " 'large': 383,\n",
       " 'reasons': 384,\n",
       " 'impact': 385,\n",
       " 'started': 386,\n",
       " 'changed': 387,\n",
       " 'fact': 388,\n",
       " 'problems': 389,\n",
       " 'understand': 390,\n",
       " 'qe': 391,\n",
       " 'britain': 392,\n",
       " 'household': 393,\n",
       " 'consistent': 394,\n",
       " 'securities': 395,\n",
       " 'increasingly': 396,\n",
       " 'temporary': 397,\n",
       " 'gauge': 398,\n",
       " 'fastest': 399,\n",
       " 'spent': 400,\n",
       " 'began': 401,\n",
       " 'median': 402,\n",
       " 'manufacturers': 403,\n",
       " 'downturn': 404,\n",
       " 'biggest': 405,\n",
       " 'fear': 406,\n",
       " 'clarida': 407,\n",
       " 'readings': 408,\n",
       " 'infrastructure': 409,\n",
       " 'slowdown': 410,\n",
       " 'prior': 411,\n",
       " '02': 412,\n",
       " 'control': 413,\n",
       " 'begun': 414,\n",
       " 'models': 415,\n",
       " 'makers': 416,\n",
       " 'similar': 417,\n",
       " 'fight': 418,\n",
       " 'approach': 419,\n",
       " 'revised': 420,\n",
       " 'continued': 421,\n",
       " 'starting': 422,\n",
       " 'trade': 423,\n",
       " 'semiconductors': 424,\n",
       " 'ahead': 425,\n",
       " 'ways': 426,\n",
       " 'bottom': 427,\n",
       " 'williams': 428,\n",
       " 'targeting': 429,\n",
       " 'volcker': 430,\n",
       " 'industry': 431,\n",
       " 'losses': 432,\n",
       " 'speech': 433,\n",
       " 'reform': 434,\n",
       " 'tight': 435,\n",
       " 'happened': 436,\n",
       " 'covid19': 437,\n",
       " 'finance': 438,\n",
       " 'sharply': 439,\n",
       " 'instance': 440,\n",
       " 'consumption': 441,\n",
       " 'figures': 442,\n",
       " 'suggest': 443,\n",
       " 'february': 444,\n",
       " 'creating': 445,\n",
       " 'war': 446,\n",
       " '7': 447,\n",
       " 'transitory': 448,\n",
       " 'ease': 449,\n",
       " 'sharp': 450,\n",
       " 'caused': 451,\n",
       " 'didn': 452,\n",
       " 'numbers': 453,\n",
       " 'statistics': 454,\n",
       " 'gasoline': 455,\n",
       " 'easing': 456,\n",
       " 'form': 457,\n",
       " 'difficult': 458,\n",
       " 'implications': 459,\n",
       " 'showed': 460,\n",
       " 'joe': 461,\n",
       " 'credit': 462,\n",
       " 'country': 463,\n",
       " 'net': 464,\n",
       " 'socalled': 465,\n",
       " 'bit': 466,\n",
       " '1990s': 467,\n",
       " 'john': 468,\n",
       " 'management': 469,\n",
       " 'moving': 470,\n",
       " 'jumped': 471,\n",
       " 'elevated': 472,\n",
       " 'wall': 473,\n",
       " 'keeping': 474,\n",
       " 'told': 475,\n",
       " 'expectation': 476,\n",
       " 'capital': 477,\n",
       " 'add': 478,\n",
       " 'bloomberg': 479,\n",
       " 'conclude': 480,\n",
       " 'home': 481,\n",
       " 'domestic': 482,\n",
       " 'commerce': 483,\n",
       " 'vice': 484,\n",
       " 'professional': 485,\n",
       " 'forecasters': 486,\n",
       " 'turn': 487,\n",
       " 'weeks': 488,\n",
       " 'helped': 489,\n",
       " '44': 490,\n",
       " 'driving': 491,\n",
       " 'track': 492,\n",
       " 'breakeven': 493,\n",
       " 'personal': 494,\n",
       " 'summer': 495,\n",
       " 'dropped': 496,\n",
       " 'forecast': 497,\n",
       " 'predict': 498,\n",
       " 'natural': 499,\n",
       " '35': 500,\n",
       " 'published': 501,\n",
       " 'terms': 502,\n",
       " 'review': 503,\n",
       " 'house': 504,\n",
       " 'social': 505,\n",
       " 'factor': 506,\n",
       " 'shipping': 507,\n",
       " 'sales': 508,\n",
       " 'credibility': 509,\n",
       " 'needed': 510,\n",
       " '2023': 511,\n",
       " 'tools': 512,\n",
       " 'parts': 513,\n",
       " 'bill': 514,\n",
       " 'chipmakers': 515,\n",
       " 'climbed': 516,\n",
       " '29': 517,\n",
       " '03': 518,\n",
       " 'amid': 519,\n",
       " 'haven': 520,\n",
       " 'reason': 521,\n",
       " 'struggle': 522,\n",
       " 'soaring': 523,\n",
       " 'created': 524,\n",
       " 'ability': 525,\n",
       " 'means': 526,\n",
       " 'shock': 527,\n",
       " 'purchases': 528,\n",
       " '—': 529,\n",
       " 'generous': 530,\n",
       " 'possibility': 531,\n",
       " 'yearoveryear': 532,\n",
       " 'peak': 533,\n",
       " 'days': 534,\n",
       " '23': 535,\n",
       " 'notes': 536,\n",
       " 'surveys': 537,\n",
       " 'republicans': 538,\n",
       " 'difference': 539,\n",
       " 'declared': 540,\n",
       " 'volatile': 541,\n",
       " 'cool': 542,\n",
       " 'room': 543,\n",
       " 'huge': 544,\n",
       " 'focused': 545,\n",
       " 'reducing': 546,\n",
       " 'deliver': 547,\n",
       " 'shows': 548,\n",
       " 'worried': 549,\n",
       " 'friedman': 550,\n",
       " 'assets': 551,\n",
       " 'step': 552,\n",
       " 'thought': 553,\n",
       " 'surging': 554,\n",
       " 'argue': 555,\n",
       " 'national': 556,\n",
       " 'process': 557,\n",
       " 'surpluses': 558,\n",
       " 'fixed': 559,\n",
       " 'ultimately': 560,\n",
       " 'break': 561,\n",
       " 'great': 562,\n",
       " 'grew': 563,\n",
       " 'sign': 564,\n",
       " 'street': 565,\n",
       " 'called': 566,\n",
       " 'invest': 567,\n",
       " 'selffulfilling': 568,\n",
       " 'begin': 569,\n",
       " '60': 570,\n",
       " 'excluding': 571,\n",
       " 'worse': 572,\n",
       " 'small': 573,\n",
       " 'chart': 574,\n",
       " 'runaway': 575,\n",
       " 'case': 576,\n",
       " '8': 577,\n",
       " 'benefits': 578,\n",
       " 'plan': 579,\n",
       " 'added': 580,\n",
       " 'post': 581,\n",
       " 'causing': 582,\n",
       " 'reflects': 583,\n",
       " 'security': 584,\n",
       " 'personalconsumption': 585,\n",
       " 'headline': 586,\n",
       " 'reduce': 587,\n",
       " '46': 588,\n",
       " 'signaled': 589,\n",
       " 'line': 590,\n",
       " 'don': 591,\n",
       " '‘': 592,\n",
       " '2012': 593,\n",
       " 'lose': 594,\n",
       " 'lowest': 595,\n",
       " 'ukraine': 596,\n",
       " '50': 597,\n",
       " 'path': 598,\n",
       " 'projections': 599,\n",
       " '45': 600,\n",
       " 'leading': 601,\n",
       " 'weight': 602,\n",
       " 'estimates': 603,\n",
       " 'colleagues': 604,\n",
       " 'goals': 605,\n",
       " 'led': 606,\n",
       " 'taking': 607,\n",
       " 'anticipated': 608,\n",
       " 'risks': 609,\n",
       " 'building': 610,\n",
       " 'rebound': 611,\n",
       " 'returns': 612,\n",
       " 'drive': 613,\n",
       " 'order': 614,\n",
       " 'stop': 615,\n",
       " 'blog': 616,\n",
       " 'di': 617,\n",
       " 'giovanni': 618,\n",
       " 'fund': 619,\n",
       " 'cea': 620,\n",
       " 'announced': 621,\n",
       " 'asset': 622,\n",
       " '05': 623,\n",
       " '21': 624,\n",
       " 'general': 625,\n",
       " 'ended': 626,\n",
       " '66': 627,\n",
       " 'deflation': 628,\n",
       " 'worries': 629,\n",
       " 'plans': 630,\n",
       " '2018': 631,\n",
       " 'play': 632,\n",
       " 'hourly': 633,\n",
       " 'involved': 634,\n",
       " 'heavily': 635,\n",
       " 'rapid': 636,\n",
       " 'sustained': 637,\n",
       " 'lockdowns': 638,\n",
       " 'restaurant': 639,\n",
       " 'absence': 640,\n",
       " 'customers': 641,\n",
       " 'adopted': 642,\n",
       " 'hiring': 643,\n",
       " 'weighing': 644,\n",
       " 'century': 645,\n",
       " 'double': 646,\n",
       " 'savings': 647,\n",
       " 'expansion': 648,\n",
       " 'hot': 649,\n",
       " 'reach': 650,\n",
       " 'straight': 651,\n",
       " '28': 652,\n",
       " 'bureau': 653,\n",
       " 'latest': 654,\n",
       " '85': 655,\n",
       " 'fundamental': 656,\n",
       " 'issue': 657,\n",
       " 'monday': 658,\n",
       " 'substantial': 659,\n",
       " 'experience': 660,\n",
       " 'sees': 661,\n",
       " 'largely': 662,\n",
       " 'history': 663,\n",
       " 'equivalent': 664,\n",
       " 'threat': 665,\n",
       " 'milton': 666,\n",
       " 'wealth': 667,\n",
       " 'giving': 668,\n",
       " 'bondbuying': 669,\n",
       " 'zone': 670,\n",
       " 'deposits': 671,\n",
       " 'pointed': 672,\n",
       " 'includes': 673,\n",
       " 'special': 674,\n",
       " 'periods': 675,\n",
       " 'enormous': 676,\n",
       " 'meaning': 677,\n",
       " 'cars': 678,\n",
       " 'bills': 679,\n",
       " 'matter': 680,\n",
       " 'tend': 681,\n",
       " 'climbing': 682,\n",
       " 'strategist': 683,\n",
       " 'crude': 684,\n",
       " 'lots': 685,\n",
       " 'treasuries': 686,\n",
       " 'types': 687,\n",
       " 'trading': 688,\n",
       " 'feds': 689,\n",
       " 'closely': 690,\n",
       " 'gain': 691,\n",
       " 'manner': 692,\n",
       " 'finally': 693,\n",
       " 'reached': 694,\n",
       " 'costly': 695,\n",
       " 'employers': 696,\n",
       " 'record': 697,\n",
       " 'indexes': 698,\n",
       " 'shot': 699,\n",
       " '2013': 700,\n",
       " 'answer': 701,\n",
       " 'held': 702,\n",
       " 'calls': 703,\n",
       " 'officer': 704,\n",
       " 'product': 705,\n",
       " 'talk': 706,\n",
       " 'richard': 707,\n",
       " 'easier': 708,\n",
       " '14': 709,\n",
       " 'group': 710,\n",
       " 'globally': 711,\n",
       " 'grow': 712,\n",
       " 'face': 713,\n",
       " 'strong': 714,\n",
       " 'mark': 715,\n",
       " 'confidence': 716,\n",
       " 'corporate': 717,\n",
       " 'eased': 718,\n",
       " 'declined': 719,\n",
       " 'signal': 720,\n",
       " 'restrain': 721,\n",
       " 'direction': 722,\n",
       " 'slowing': 723,\n",
       " 'fully': 724,\n",
       " 'prove': 725,\n",
       " 'message': 726,\n",
       " 'consistently': 727,\n",
       " '1960s': 728,\n",
       " 'explain': 729,\n",
       " 'actual': 730,\n",
       " 'll': 731,\n",
       " 'russia': 732,\n",
       " 'turned': 733,\n",
       " 'reduction': 734,\n",
       " 'privatesector': 735,\n",
       " 'total': 736,\n",
       " 'curve': 737,\n",
       " 'providing': 738,\n",
       " 'framework': 739,\n",
       " 'role': 740,\n",
       " 'chains': 741,\n",
       " 'struggling': 742,\n",
       " 'combination': 743,\n",
       " 'suppliers': 744,\n",
       " 'rest': 745,\n",
       " 'ageing': 746,\n",
       " 'budget': 747,\n",
       " 'bear': 748,\n",
       " 'full': 749,\n",
       " 'forecasts': 750,\n",
       " 'expensive': 751,\n",
       " 'accounted': 752,\n",
       " 'climate': 753,\n",
       " 'thinks': 754,\n",
       " 'imf': 755,\n",
       " 'prevent': 756,\n",
       " 'white': 757,\n",
       " 'supercore': 758,\n",
       " 'shifts': 759,\n",
       " 'club': 760,\n",
       " 'margins': 761,\n",
       " 'system': 762,\n",
       " 'cycle': 763,\n",
       " 'electric': 764,\n",
       " 'clean': 765,\n",
       " 'rules': 766,\n",
       " 'green': 767,\n",
       " 'build': 768,\n",
       " 'emissions': 769,\n",
       " 'predicted': 770,\n",
       " 'modest': 771,\n",
       " 'stuck': 772,\n",
       " 'operating': 773,\n",
       " 'accelerate': 774,\n",
       " 'bad': 775,\n",
       " 'avoid': 776,\n",
       " 'trump': 777,\n",
       " 'modestly': 778,\n",
       " 'aren': 779,\n",
       " 'landscape': 780,\n",
       " 'uncertainty': 781,\n",
       " 'amount': 782,\n",
       " 'service': 783,\n",
       " 'conditions': 784,\n",
       " 'translate': 785,\n",
       " 'printing': 786,\n",
       " 'equipment': 787,\n",
       " 'scarce': 788,\n",
       " 'broad': 789,\n",
       " 'larger': 790,\n",
       " 'harvard': 791,\n",
       " 'institute': 792,\n",
       " 'dramatically': 793,\n",
       " 'substitutes': 794,\n",
       " 'falls': 795,\n",
       " 'sense': 796,\n",
       " '17': 797,\n",
       " 'incomes': 798,\n",
       " 'account': 799,\n",
       " 'forced': 800,\n",
       " 'adding': 801,\n",
       " 'life': 802,\n",
       " 'forces': 803,\n",
       " 'hands': 804,\n",
       " 'prefer': 805,\n",
       " 've': 806,\n",
       " 'pentup': 807,\n",
       " 'hope': 808,\n",
       " 'continues': 809,\n",
       " 'historical': 810,\n",
       " 'deficits': 811,\n",
       " 'prosperity': 812,\n",
       " 'korea': 813,\n",
       " 'feel': 814,\n",
       " 'involve': 815,\n",
       " 'indication': 816,\n",
       " 'declines': 817,\n",
       " 'anticipate': 818,\n",
       " 'fast': 819,\n",
       " 'facts': 820,\n",
       " 'struck': 821,\n",
       " 'phenomenon': 822,\n",
       " 'hitting': 823,\n",
       " 'balancesheets': 824,\n",
       " 'raises': 825,\n",
       " 'receive': 826,\n",
       " 'subject': 827,\n",
       " 'reality': 828,\n",
       " 'experiment': 829,\n",
       " 'boosted': 830,\n",
       " 'suggested': 831,\n",
       " 'passed': 832,\n",
       " 'create': 833,\n",
       " 'accept': 834,\n",
       " 'reflect': 835,\n",
       " 'expecting': 836,\n",
       " 'watch': 837,\n",
       " 'signals': 838,\n",
       " 'senior': 839,\n",
       " 'game': 840,\n",
       " 'concern': 841,\n",
       " '54': 842,\n",
       " 'rents': 843,\n",
       " 'pickup': 844,\n",
       " 'benchmark': 845,\n",
       " 'recovery': 846,\n",
       " 'factors': 847,\n",
       " 'payments': 848,\n",
       " 'happen': 849,\n",
       " 'respond': 850,\n",
       " 'worst': 851,\n",
       " 'programs': 852,\n",
       " 'moves': 853,\n",
       " 'dollar': 854,\n",
       " '2014': 855,\n",
       " '53': 856,\n",
       " 'nonetheless': 857,\n",
       " 'shortage': 858,\n",
       " 'danger': 859,\n",
       " 'issues': 860,\n",
       " 'paul': 861,\n",
       " '9': 862,\n",
       " 'pushing': 863,\n",
       " 'gross': 864,\n",
       " '19': 865,\n",
       " 'seeking': 866,\n",
       " 'funds': 867,\n",
       " 'figure': 868,\n",
       " 'distorted': 869,\n",
       " 'stubbornly': 870,\n",
       " 'bound': 871,\n",
       " 'manager': 872,\n",
       " 'treasurys': 873,\n",
       " '22': 874,\n",
       " 'selling': 875,\n",
       " 'historic': 876,\n",
       " 'overseas': 877,\n",
       " 'views': 878,\n",
       " 'holding': 879,\n",
       " 'regular': 880,\n",
       " 'noted': 881,\n",
       " 'battle': 882,\n",
       " '01': 883,\n",
       " 'jump': 884,\n",
       " '48': 885,\n",
       " 'cheaper': 886,\n",
       " 'unusually': 887,\n",
       " 'meaningful': 888,\n",
       " 'interview': 889,\n",
       " 'pain': 890,\n",
       " 'surveyed': 891,\n",
       " 'capacity': 892,\n",
       " 'sustainable': 893,\n",
       " 'longrun': 894,\n",
       " 'phillips': 895,\n",
       " 'allowing': 896,\n",
       " 'coronavirus': 897,\n",
       " 'patterns': 898,\n",
       " 'dollars': 899,\n",
       " 'arizona': 900,\n",
       " 'law': 901,\n",
       " 'soared': 902,\n",
       " 'works': 903,\n",
       " 'dining': 904,\n",
       " 'side': 905,\n",
       " 'wouldn': 906,\n",
       " 'hawkish': 907,\n",
       " 'positive': 908,\n",
       " 'brainard': 909,\n",
       " 'hopes': 910,\n",
       " 'gap': 911,\n",
       " '6': 912,\n",
       " 'temporarily': 913,\n",
       " 'suggesting': 914,\n",
       " '2024': 915,\n",
       " 'significantly': 916,\n",
       " 'overheated': 917,\n",
       " 'living': 918,\n",
       " 'spiral': 919,\n",
       " 'strategy': 920,\n",
       " 'conference': 921,\n",
       " 'words': 922,\n",
       " 'rapidly': 923,\n",
       " 'symmetric': 924,\n",
       " 'aiming': 925,\n",
       " 'misses': 926,\n",
       " 'meet': 927,\n",
       " '2015': 928,\n",
       " 'overshooting': 929,\n",
       " 'reopened': 930,\n",
       " 'excludes': 931,\n",
       " 'largest': 932,\n",
       " 'disrupted': 933,\n",
       " 'paying': 934,\n",
       " 'prompt': 935,\n",
       " 'vehicles': 936,\n",
       " 'projects': 937,\n",
       " 'components': 938,\n",
       " 'alternative': 939,\n",
       " '16': 940,\n",
       " 'driver': 941,\n",
       " 'lost': 942,\n",
       " '200709': 943,\n",
       " 'globalisation': 944,\n",
       " 'places': 945,\n",
       " 'invasion': 946,\n",
       " 'top': 947,\n",
       " '41': 948,\n",
       " 'cooled': 949,\n",
       " '51': 950,\n",
       " 'asked': 951,\n",
       " 'trends': 952,\n",
       " 'cutting': 953,\n",
       " 'england': 954,\n",
       " 'shopping': 955,\n",
       " 'increasing': 956,\n",
       " 'actions': 957,\n",
       " 'researchers': 958,\n",
       " 'findings': 959,\n",
       " 'produce': 960,\n",
       " 'model': 961,\n",
       " 'incentives': 962,\n",
       " 'intended': 963,\n",
       " 'twothirds': 964,\n",
       " 'chance': 965,\n",
       " 'option': 966,\n",
       " 'regime': 967,\n",
       " 'benefit': 968,\n",
       " 'rosengren': 969,\n",
       " 'tracking': 970,\n",
       " 'profit': 971,\n",
       " 'hikes': 972,\n",
       " 'prepared': 973,\n",
       " 'cooler': 974,\n",
       " 'trouble': 975,\n",
       " 'dip': 976,\n",
       " 'dangers': 977,\n",
       " 'ford': 978,\n",
       " 'advanced': 979,\n",
       " 'nominal': 980,\n",
       " 'longerterm': 981,\n",
       " 'democrats': 982,\n",
       " 'panels': 983,\n",
       " 'facilities': 984,\n",
       " 'carmakers': 985,\n",
       " 'freetrade': 986,\n",
       " 'elements': 987,\n",
       " 'south': 988,\n",
       " 'prime': 989,\n",
       " 'greater': 990,\n",
       " 'creation': 991,\n",
       " 'dobirr': 992,\n",
       " 'spike': 993,\n",
       " '2000s': 994,\n",
       " 'warning': 995,\n",
       " 'mistakes': 996,\n",
       " 'victory': 997,\n",
       " 'wanted': 998,\n",
       " 'mix': 999,\n",
       " 'summers': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4692"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This snippet of code creates a dictionary that maps the words to their respective word embeddings.\n",
    "word_to_vec_map = {}\n",
    "wordsNotInGlove = 0\n",
    "for word in words_to_index.keys():\n",
    "    try:\n",
    "        word_to_vec_map[word] = model[word]\n",
    "    except:\n",
    "        wordsNotInGlove = wordsNotInGlove + 1\n",
    "\n",
    "wordsNotInGlove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating an embeding matrix**\n",
    "https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3283.925\n",
      "14756\n",
      "3114.5\n"
     ]
    }
   ],
   "source": [
    "# Checking some metrics on the training set to define input maximum length \n",
    "print(np.mean([len(item) for item in X_train_prepros]))\n",
    "print(max([len(item) for item in X_train_prepros]))\n",
    "print(np.median([len(item) for item in X_train_prepros]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = 300 # originally 150 ### 5000 was too large, trying between 150 300 and 500 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(words_to_index)+1#-wordsNotInGlove\n",
    "embed_vector_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embeding matrix\n",
    "emb_matrix = np.zeros((vocab_len, embed_vector_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the embeding matrix. Words which are not in the GloVe dictionary being assigned a zero vector.\n",
    "for word, index in words_to_index.items():\n",
    "    embedding_vector = word_to_vec_map.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        emb_matrix[index] = embedding_vector\n",
    "        #emb_matrix[index, :] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An Embedding layer should be fed sequences of integers, i.e. a 2D input of shape (samples, indices). \n",
    "# These input sequences should be padded so that they all have the same length in a batch of input data \n",
    "# (although an Embedding layer is capable of processing sequence of heterogenous length, if you don't \n",
    "# pass an explicit input_length argument to the layer).\n",
    "\n",
    "embedding_layer = Embedding(input_dim=vocab_len, output_dim=embed_vector_len, \n",
    "                            input_length=maxLen, weights = [emb_matrix], trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting up LSTM RNN and GRU RNN for text classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function defines our model architecture, first, we use the embedding layer to map the words to their GloVe vectors, and then those vectors are input to the LSTM layers followed by a Dense layer with ‘sigmoid’ activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.layers import GRU\n",
    "#GRU(max_len, activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstmArchitecture(input_shape):\n",
    "\n",
    "    X_indices = Input(input_shape)\n",
    "\n",
    "    embeddings = embedding_layer(X_indices) \n",
    "\n",
    "    lstmLayer = LSTM(128, return_sequences=False,activation='relu')(embeddings)\n",
    "\n",
    "    dropoutLayer = Dropout(0.1)(lstmLayer) # Experiment with 0.2. Originally I used 0.6 too much few data\n",
    "    \n",
    "    dense_1 = Dense(128, activation='relu')(dropoutLayer)\n",
    "\n",
    "    dense_2 = Dense(3, activation='softmax')(dense_1)\n",
    "\n",
    "    model = Model(inputs=X_indices, outputs=dense_2) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gruArchitecture(input_shape):\n",
    "\n",
    "    X_indices = Input(input_shape)\n",
    "\n",
    "    embeddings = embedding_layer(X_indices) \n",
    "\n",
    "    gruLayer = GRU(128,activation='relu')(embeddings) # Removed return_sequences=False kwarg\n",
    "\n",
    "    dropoutLayer = Dropout(0.1)(gruLayer) # Experiment with 0.2. Originally I used 0.6 too much few data\n",
    "    \n",
    "    dense_1 = Dense(128, activation='relu')(dropoutLayer)\n",
    "\n",
    "    dense_2 = Dense(3, activation='softmax')(dense_1)\n",
    "\n",
    "    model = Model(inputs=X_indices, outputs=dense_2) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will convert the reviews in the dataset to their index form by using the texts_to_sequences function available with tokenizer. After that, we will pad the sequences so all of them have the same length.\n",
    "Source: https://towardsdatascience.com/sentiment-analysis-using-lstm-and-glove-embeddings-99223a87fe8e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = tokenizer.texts_to_sequences(X_train_prepros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16,\n",
       " 32,\n",
       " 206,\n",
       " 173,\n",
       " 50,\n",
       " 2055,\n",
       " 1354,\n",
       " 365,\n",
       " 2056,\n",
       " 2057,\n",
       " 51,\n",
       " 3,\n",
       " 33,\n",
       " 95,\n",
       " 238,\n",
       " 174,\n",
       " 41,\n",
       " 1,\n",
       " 58,\n",
       " 623,\n",
       " 97,\n",
       " 624,\n",
       " 52,\n",
       " 5,\n",
       " 239,\n",
       " 993,\n",
       " 34,\n",
       " 770,\n",
       " 25,\n",
       " 2058,\n",
       " 81,\n",
       " 311,\n",
       " 51,\n",
       " 98,\n",
       " 432,\n",
       " 99,\n",
       " 2059,\n",
       " 74,\n",
       " 47,\n",
       " 1355,\n",
       " 2060,\n",
       " 207,\n",
       " 2061,\n",
       " 52,\n",
       " 19,\n",
       " 99,\n",
       " 150,\n",
       " 74,\n",
       " 366,\n",
       " 516,\n",
       " 517,\n",
       " 129,\n",
       " 62,\n",
       " 189,\n",
       " 2062,\n",
       " 35,\n",
       " 15,\n",
       " 2063,\n",
       " 239,\n",
       " 208,\n",
       " 2,\n",
       " 2064,\n",
       " 1356,\n",
       " 98,\n",
       " 994,\n",
       " 11,\n",
       " 4,\n",
       " 28,\n",
       " 63,\n",
       " 10,\n",
       " 1357,\n",
       " 2065,\n",
       " 112,\n",
       " 275,\n",
       " 2,\n",
       " 312,\n",
       " 1358,\n",
       " 367,\n",
       " 8,\n",
       " 2066,\n",
       " 625,\n",
       " 82,\n",
       " 2067,\n",
       " 626,\n",
       " 2068,\n",
       " 2069,\n",
       " 97,\n",
       " 121,\n",
       " 995,\n",
       " 130,\n",
       " 3,\n",
       " 1359,\n",
       " 83,\n",
       " 27,\n",
       " 2070,\n",
       " 51,\n",
       " 518,\n",
       " 23,\n",
       " 2071,\n",
       " 52,\n",
       " 151,\n",
       " 44,\n",
       " 8,\n",
       " 276,\n",
       " 58,\n",
       " 627,\n",
       " 1360,\n",
       " 97,\n",
       " 313,\n",
       " 1361,\n",
       " 54,\n",
       " 2072,\n",
       " 2073,\n",
       " 519,\n",
       " 771,\n",
       " 56,\n",
       " 240,\n",
       " 8,\n",
       " 314,\n",
       " 48,\n",
       " 44,\n",
       " 2074,\n",
       " 2,\n",
       " 2075,\n",
       " 628,\n",
       " 629,\n",
       " 2076,\n",
       " 2077,\n",
       " 65,\n",
       " 996,\n",
       " 2078,\n",
       " 2079,\n",
       " 2080,\n",
       " 1362,\n",
       " 2081,\n",
       " 433,\n",
       " 209,\n",
       " 2082,\n",
       " 368,\n",
       " 628,\n",
       " 1363,\n",
       " 1364,\n",
       " 1365,\n",
       " 997,\n",
       " 630,\n",
       " 275,\n",
       " 241,\n",
       " 1366,\n",
       " 175,\n",
       " 2,\n",
       " 772,\n",
       " 2083,\n",
       " 210,\n",
       " 3,\n",
       " 520,\n",
       " 516,\n",
       " 12,\n",
       " 1,\n",
       " 24,\n",
       " 16,\n",
       " 369,\n",
       " 22,\n",
       " 315,\n",
       " 998,\n",
       " 242,\n",
       " 52,\n",
       " 12,\n",
       " 316,\n",
       " 2084,\n",
       " 277,\n",
       " 35,\n",
       " 15,\n",
       " 13,\n",
       " 999,\n",
       " 112,\n",
       " 434,\n",
       " 1367,\n",
       " 2,\n",
       " 773,\n",
       " 1368,\n",
       " 2085,\n",
       " 2086,\n",
       " 2087,\n",
       " 113,\n",
       " 1369,\n",
       " 1000,\n",
       " 2088,\n",
       " 131,\n",
       " 1001,\n",
       " 112,\n",
       " 434,\n",
       " 370,\n",
       " 15,\n",
       " 243,\n",
       " 2,\n",
       " 114,\n",
       " 2089,\n",
       " 1370,\n",
       " 162,\n",
       " 15,\n",
       " 631,\n",
       " 54,\n",
       " 2090,\n",
       " 112,\n",
       " 434,\n",
       " 2091,\n",
       " 10,\n",
       " 317,\n",
       " 15,\n",
       " 774,\n",
       " 371,\n",
       " 29,\n",
       " 1370,\n",
       " 318,\n",
       " 365,\n",
       " 1371,\n",
       " 29,\n",
       " 50,\n",
       " 632,\n",
       " 775,\n",
       " 2092,\n",
       " 59,\n",
       " 4,\n",
       " 277,\n",
       " 776,\n",
       " 2093,\n",
       " 2094,\n",
       " 2095,\n",
       " 1362,\n",
       " 1372,\n",
       " 319,\n",
       " 190,\n",
       " 51,\n",
       " 3,\n",
       " 372,\n",
       " 33,\n",
       " 95,\n",
       " 121,\n",
       " 174,\n",
       " 101,\n",
       " 320,\n",
       " 69,\n",
       " 633,\n",
       " 320,\n",
       " 163,\n",
       " 2096,\n",
       " 2097,\n",
       " 777,\n",
       " 2098,\n",
       " 623,\n",
       " 23,\n",
       " 69,\n",
       " 49,\n",
       " 58,\n",
       " 778,\n",
       " 2099,\n",
       " 54,\n",
       " 41,\n",
       " 3,\n",
       " 521,\n",
       " 191,\n",
       " 2100,\n",
       " 192,\n",
       " 15,\n",
       " 2101,\n",
       " 373,\n",
       " 320,\n",
       " 240,\n",
       " 1373,\n",
       " 51,\n",
       " 83,\n",
       " 27,\n",
       " 3,\n",
       " 277,\n",
       " 15,\n",
       " 519,\n",
       " 435,\n",
       " 33,\n",
       " 25,\n",
       " 244,\n",
       " 7,\n",
       " 49,\n",
       " 2,\n",
       " 84,\n",
       " 240,\n",
       " 779,\n",
       " 2102,\n",
       " 7,\n",
       " 3,\n",
       " 436,\n",
       " 211,\n",
       " 2103,\n",
       " 1,\n",
       " 371,\n",
       " 2104,\n",
       " 1002,\n",
       " 5,\n",
       " 1365,\n",
       " 318,\n",
       " 365,\n",
       " 162,\n",
       " 2105,\n",
       " 2106,\n",
       " 212,\n",
       " 2107,\n",
       " 1374,\n",
       " 1003,\n",
       " 1371,\n",
       " 29,\n",
       " 50]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = pad_sequences(X_train_indices, maxlen=maxLen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  51,    3,   33, ..., 1371,   29,   50],\n",
       "       [  14,  440, 2163, ..., 2221, 2222,  531],\n",
       "       [2223,    7,    1, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [4462, 4463,  110, ...,   79,  622,    3],\n",
       "       [  52,  122,  118, ...,  270, 1541, 4603],\n",
       "       [4615,  992, 1281, ...,    1, 4692,   53]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 300)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_indices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the LSTM model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmGloVe_model = lstmArchitecture((maxLen,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 300, 100)          469300    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 603,447\n",
      "Trainable params: 603,447\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstmGloVe_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "40/40 [==============================] - 4s 98ms/step - loss: 1.0708 - acc: 0.4750\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.0538 - acc: 0.5500\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 1.0292 - acc: 0.5750\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 3s 86ms/step - loss: 1.0190 - acc: 0.5500\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 4s 93ms/step - loss: 1.0160 - acc: 0.5500\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 4s 100ms/step - loss: 1.0072 - acc: 0.5750\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 4s 97ms/step - loss: 0.9925 - acc: 0.5500\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 0.9741 - acc: 0.5500\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 0.9606 - acc: 0.5750\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 4s 94ms/step - loss: 0.9596 - acc: 0.5750\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 0.9396 - acc: 0.6000\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.9268 - acc: 0.5500\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.9195 - acc: 0.6250\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 0.8797 - acc: 0.7000\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 4s 89ms/step - loss: 4.5905 - acc: 0.6000\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 4s 93ms/step - loss: 0.8612 - acc: 0.7000\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 4s 94ms/step - loss: 0.8663 - acc: 0.7000\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 4s 93ms/step - loss: 0.8482 - acc: 0.7000\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 4s 101ms/step - loss: 0.8033 - acc: 0.7500\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 1.8658 - acc: 0.7250\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 4s 110ms/step - loss: 7.8015 - acc: 0.4750\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 5s 114ms/step - loss: 8.9466 - acc: 0.4250\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 5s 120ms/step - loss: 8.1381 - acc: 0.4750\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 5s 126ms/step - loss: 7.7333 - acc: 0.5000\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 4s 111ms/step - loss: 8.5351 - acc: 0.4500\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 5s 118ms/step - loss: 8.1295 - acc: 0.5000\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 5s 114ms/step - loss: 6.9665 - acc: 0.5500\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 4s 101ms/step - loss: 6.9620 - acc: 0.5500\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 4s 96ms/step - loss: 6.9617 - acc: 0.5500\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - 4s 98ms/step - loss: 8.9769 - acc: 0.4250\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - 4s 98ms/step - loss: 10.1514 - acc: 0.3750\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - 4s 92ms/step - loss: 9.7350 - acc: 0.4000\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - 4s 92ms/step - loss: 10.9456 - acc: 0.3250\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - 4s 92ms/step - loss: 9.3298 - acc: 0.4250\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - 4s 98ms/step - loss: 10.1379 - acc: 0.3750\n",
      "Epoch 36/100\n",
      "40/40 [==============================] - 4s 100ms/step - loss: 9.2910 - acc: 0.4250\n",
      "Epoch 37/100\n",
      "40/40 [==============================] - 4s 100ms/step - loss: 12.0886 - acc: 0.2500\n",
      "Epoch 38/100\n",
      "40/40 [==============================] - 4s 97ms/step - loss: 11.2827 - acc: 0.3000\n",
      "Epoch 39/100\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 9.2679 - acc: 0.4250\n",
      "Epoch 40/100\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 11.6856 - acc: 0.2750\n",
      "Epoch 41/100\n",
      "40/40 [==============================] - 4s 89ms/step - loss: 8.8650 - acc: 0.4500\n",
      "Epoch 42/100\n",
      "40/40 [==============================] - 4s 89ms/step - loss: 10.8797 - acc: 0.3250\n",
      "Epoch 43/100\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 9.6709 - acc: 0.4000\n",
      "Epoch 44/100\n",
      "40/40 [==============================] - 4s 89ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 45/100\n",
      "40/40 [==============================] - 3s 87ms/step - loss: 8.8650 - acc: 0.4500\n",
      "Epoch 46/100\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 10.4768 - acc: 0.3500\n",
      "Epoch 47/100\n",
      "40/40 [==============================] - 4s 92ms/step - loss: 10.4768 - acc: 0.3500\n",
      "Epoch 48/100\n",
      "40/40 [==============================] - 4s 96ms/step - loss: 10.8797 - acc: 0.3250\n",
      "Epoch 49/100\n",
      "40/40 [==============================] - 5s 114ms/step - loss: 10.0738 - acc: 0.3750\n",
      "Epoch 50/100\n",
      "40/40 [==============================] - 5s 116ms/step - loss: 9.6709 - acc: 0.4000\n",
      "Epoch 51/100\n",
      "40/40 [==============================] - 4s 109ms/step - loss: 9.6709 - acc: 0.4000\n",
      "Epoch 52/100\n",
      "40/40 [==============================] - 5s 114ms/step - loss: 9.6709 - acc: 0.4000\n",
      "Epoch 53/100\n",
      "40/40 [==============================] - 4s 111ms/step - loss: 8.8650 - acc: 0.4500\n",
      "Epoch 54/100\n",
      "40/40 [==============================] - 5s 117ms/step - loss: 9.6709 - acc: 0.4000\n",
      "Epoch 55/100\n",
      "40/40 [==============================] - 5s 115ms/step - loss: 9.6709 - acc: 0.4000\n",
      "Epoch 56/100\n",
      "40/40 [==============================] - 5s 113ms/step - loss: 9.2679 - acc: 0.4250\n",
      "Epoch 57/100\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 10.8797 - acc: 0.3250\n",
      "Epoch 58/100\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 10.4768 - acc: 0.3500\n",
      "Epoch 59/100\n",
      "40/40 [==============================] - 4s 96ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 60/100\n",
      "40/40 [==============================] - 4s 94ms/step - loss: 9.6709 - acc: 0.4000\n",
      "Epoch 61/100\n",
      "40/40 [==============================] - 4s 99ms/step - loss: 9.6709 - acc: 0.4000\n",
      "Epoch 62/100\n",
      "40/40 [==============================] - 4s 94ms/step - loss: 10.0738 - acc: 0.3750\n",
      "Epoch 63/100\n",
      "40/40 [==============================] - 4s 99ms/step - loss: 10.4768 - acc: 0.3500\n",
      "Epoch 64/100\n",
      "40/40 [==============================] - 4s 101ms/step - loss: 10.0738 - acc: 0.3750\n",
      "Epoch 65/100\n",
      "40/40 [==============================] - 4s 97ms/step - loss: 10.4768 - acc: 0.3500\n",
      "Epoch 66/100\n",
      "40/40 [==============================] - 4s 98ms/step - loss: 10.4768 - acc: 0.3500\n",
      "Epoch 67/100\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 9.2679 - acc: 0.4250\n",
      "Epoch 68/100\n",
      "40/40 [==============================] - 4s 92ms/step - loss: 9.6709 - acc: 0.4000\n",
      "Epoch 69/100\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 10.8797 - acc: 0.3250\n",
      "Epoch 70/100\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 10.0738 - acc: 0.3750\n",
      "Epoch 71/100\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 72/100\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 10.0738 - acc: 0.3750\n",
      "Epoch 73/100\n",
      "40/40 [==============================] - 3s 87ms/step - loss: 11.2827 - acc: 0.3000\n",
      "Epoch 74/100\n",
      "40/40 [==============================] - 4s 89ms/step - loss: 10.4768 - acc: 0.3500\n",
      "Epoch 75/100\n",
      "40/40 [==============================] - 4s 89ms/step - loss: 9.6709 - acc: 0.4000\n",
      "Epoch 76/100\n",
      "40/40 [==============================] - 4s 89ms/step - loss: 8.8650 - acc: 0.4500\n",
      "Epoch 77/100\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 10.0738 - acc: 0.3750\n",
      "Epoch 78/100\n",
      "40/40 [==============================] - 4s 94ms/step - loss: 9.2679 - acc: 0.4250\n",
      "Epoch 79/100\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 9.6709 - acc: 0.4000\n",
      "Epoch 80/100\n",
      "40/40 [==============================] - 4s 111ms/step - loss: 10.4768 - acc: 0.3500\n",
      "Epoch 81/100\n",
      "40/40 [==============================] - 4s 111ms/step - loss: 9.2679 - acc: 0.4250\n",
      "Epoch 82/100\n",
      "40/40 [==============================] - 5s 117ms/step - loss: 9.2679 - acc: 0.4250\n",
      "Epoch 83/100\n",
      "40/40 [==============================] - 5s 120ms/step - loss: 9.6709 - acc: 0.4000\n",
      "Epoch 84/100\n",
      "40/40 [==============================] - 5s 118ms/step - loss: 9.6709 - acc: 0.4000\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 5s 116ms/step - loss: 9.2679 - acc: 0.4250\n",
      "Epoch 86/100\n",
      "40/40 [==============================] - 5s 116ms/step - loss: 10.0738 - acc: 0.3750\n",
      "Epoch 87/100\n",
      "40/40 [==============================] - 4s 109ms/step - loss: 10.0738 - acc: 0.3750\n",
      "Epoch 88/100\n",
      "40/40 [==============================] - 4s 97ms/step - loss: 9.6709 - acc: 0.4000\n",
      "Epoch 89/100\n",
      "40/40 [==============================] - 4s 89ms/step - loss: 10.0738 - acc: 0.3750\n",
      "Epoch 90/100\n",
      "40/40 [==============================] - 4s 92ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 91/100\n",
      "40/40 [==============================] - 4s 92ms/step - loss: 9.6709 - acc: 0.4000\n",
      "Epoch 92/100\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 93/100\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 94/100\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 10.0738 - acc: 0.3750\n",
      "Epoch 95/100\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 9.2679 - acc: 0.4250\n",
      "Epoch 96/100\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 8.8650 - acc: 0.4500\n",
      "Epoch 97/100\n",
      "40/40 [==============================] - 4s 93ms/step - loss: 10.4768 - acc: 0.3500\n",
      "Epoch 98/100\n",
      "40/40 [==============================] - 4s 98ms/step - loss: 10.0738 - acc: 0.3750\n",
      "Epoch 99/100\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 10.4768 - acc: 0.3500\n",
      "Epoch 100/100\n",
      "40/40 [==============================] - 4s 96ms/step - loss: 9.2679 - acc: 0.4250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1aaaf5e5f28>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = keras.optimizers.Adam(lr = 0.0001) # 0.0001, 0.0002, 0.00001\n",
    "lstmGloVe_model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "lstmGloVe_model.fit(X_train_indices, y_train_labels, batch_size=4, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_indices = tokenizer.texts_to_sequences(X_test_prepros)\n",
    "X_test_indices = pad_sequences(X_test_indices, maxlen=maxLen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "10/10 [==============================] - 0s 18ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.4285683631896973, 0.699999988079071]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstmGloVe_model.evaluate(X_test_indices, y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lstmGloVe_model.predict(X_test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  [[0.         1.         0.        ]\n",
      " [0.         1.         0.        ]\n",
      " [0.4017076  0.27765962 0.32063282]\n",
      " [0.         1.         0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [0.         1.         0.        ]\n",
      " [0.         1.         0.        ]\n",
      " [0.4017076  0.27765965 0.32063282]\n",
      " [0.         1.         0.        ]\n",
      " [0.         1.         0.        ]]\n",
      "actual:  [[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print('predicted: ',preds)\n",
    "print('actual: ',y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  [1 1 0 1 0 1 1 0 1 1]\n",
      "actual:  [1 0 0 0 0 1 1 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "print('predicted: ',np.argmax(preds,axis=1))\n",
    "print('actual: ',np.argmax(y_test_labels,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "      Expect inflation       0.67      0.50      0.57         4\n",
      "Inflation will go away       0.71      1.00      0.83         5\n",
      "               Neutral       0.00      0.00      0.00         1\n",
      "\n",
      "              accuracy                           0.70        10\n",
      "             macro avg       0.46      0.50      0.47        10\n",
      "          weighted avg       0.62      0.70      0.65        10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Expect inflation', 'Inflation will go away', 'Neutral']\n",
    "clsf_rep_1 = classification_report(np.argmax(y_test_labels,axis=1),np.argmax(preds,axis=1),target_names=target_names)\n",
    "print(clsf_rep_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "we still say the course until the job is done” said jerome powell the federal reserve’s chairman on december 14th shortly after the central bank’s latest interestrate rise as a statement of intent his words were both straightforward and utterly sensible but what it means for the job to be done is becoming a matter of controversy inflation remains uncomfortably high meanwhile the aggressive monetary tightening of the past year is only now filtering through to the economy complicating assessments of whether the fed has in fact done enough to rein in prices  promisingly after a difficult two years inflation does appear to be easing its grip on the american economy overall prices increased by a mere 01 monthonmonth in november according to data published on december 13th making for that rarest of recent occurrences a downside surprise most encouraging was a breakdown showing that core inflation which strips out volatile food and energy costs had decelerated for a second consecutive month see chart  investors and analysts scarred by america’s relentless run of inflation have learned to restrain their hopes after a single month of rosy data yearonyear rates of inflation remain elevated at 71 for headline inflation but the disinflation in november follows a similarly cheerful batch of data for october optimism is on the rise albeit still mostly of the cautious rather than the unbridled kind since midoctober the sp500 index of leading american firms has recovered some of the ground it lost earlier this year concerns are shifting to the prospect of weaker growth many economists forecast a recession early next year  for the fed these countervailing forces create a tricky balance on the one hand it has just administered the sharpest tightening of monetary policy in four decades lifting interest rates from a floor of 0 in march to more than 4 today with inflation ebbing it is prudent to slow the pace of rate increases on the other hand a perception of fed softening risks adding fuel to the market rally that in turn would cause financial conditions to ease thereby placing upward pressure on inflation  the fed has tried to resolve this conundrum by maintaining its hawkish tone at the same time as tweaking its policies on december 14th the fed raised rates by half a percentage point ending a string of jumbo threequarterpoint increases most fed officials believe that they will raise rates to more than 5 next year and refrain from cutting rates until 2024 according to the central bank’s latest projections “historical experience cautions strongly against prematurely easing” said mr powell  strikingly many investors think the fed will end up being more dovish bond pricing suggests that rates will peak at less than 5 and that the central bank will start cutting them before the end of 2023  ultimately the decision will come down to the data prices of consumer goods have started falling as pandemicera shortages melt away housing prices are also trending lower the big lingering concern is whether a tight labour market will push incomes and by extension prices higher investors are betting that wage increases will slow as the economy weakens the fed understandably is not popping the champagne just yet\n",
      "Inflation will fade away\n",
      "1\n",
      "acual:  [0. 1. 0.]\n",
      "predicted:  [0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "n = np.random.randint(0,10)\n",
    "print(n)\n",
    "print(X_test[n])\n",
    "print(y_test[n])\n",
    "print(y_test_encoded[n])\n",
    "print('acual: ',y_test_labels[n])\n",
    "print('predicted: ',preds[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeledArticle(articleID=44, label='Inflation will fade away', body='we still say the course until the job is done” said jerome powell the federal reserve’s chairman on december 14th shortly after the central bank’s latest interestrate rise as a statement of intent his words were both straightforward and utterly sensible but what it means for the job to be done is becoming a matter of controversy inflation remains uncomfortably high meanwhile the aggressive monetary tightening of the past year is only now filtering through to the economy complicating assessments of whether the fed has in fact done enough to rein in prices  promisingly after a difficult two years inflation does appear to be easing its grip on the american economy overall prices increased by a mere 01 monthonmonth in november according to data published on december 13th making for that rarest of recent occurrences a downside surprise most encouraging was a breakdown showing that core inflation which strips out volatile food and energy costs had decelerated for a second consecutive month see chart  investors and analysts scarred by america’s relentless run of inflation have learned to restrain their hopes after a single month of rosy data yearonyear rates of inflation remain elevated at 71 for headline inflation but the disinflation in november follows a similarly cheerful batch of data for october optimism is on the rise albeit still mostly of the cautious rather than the unbridled kind since midoctober the sp500 index of leading american firms has recovered some of the ground it lost earlier this year concerns are shifting to the prospect of weaker growth many economists forecast a recession early next year  for the fed these countervailing forces create a tricky balance on the one hand it has just administered the sharpest tightening of monetary policy in four decades lifting interest rates from a floor of 0 in march to more than 4 today with inflation ebbing it is prudent to slow the pace of rate increases on the other hand a perception of fed softening risks adding fuel to the market rally that in turn would cause financial conditions to ease thereby placing upward pressure on inflation  the fed has tried to resolve this conundrum by maintaining its hawkish tone at the same time as tweaking its policies on december 14th the fed raised rates by half a percentage point ending a string of jumbo threequarterpoint increases most fed officials believe that they will raise rates to more than 5 next year and refrain from cutting rates until 2024 according to the central bank’s latest projections “historical experience cautions strongly against prematurely easing” said mr powell  strikingly many investors think the fed will end up being more dovish bond pricing suggests that rates will peak at less than 5 and that the central bank will start cutting them before the end of 2023  ultimately the decision will come down to the data prices of consumer goods have started falling as pandemicera shortages melt away housing prices are also trending lower the big lingering concern is whether a tight labour market will push incomes and by extension prices higher investors are betting that wage increases will slow as the economy weakens the fed understandably is not popping the champagne just yet')\n"
     ]
    }
   ],
   "source": [
    "sampleBody = X_test[n]\n",
    "for i in range(len(articlesSample)):\n",
    "    if articlesSample[i][2] == sampleBody:\n",
    "        print(articlesSample[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstmGloVe_model.save_weights('\\lstmGloVe_model_weights.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine tuning LSTM model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1- Learning Rate from 0.0001 to 0.00001**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 300, 100)          469300    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 603,447\n",
      "Trainable params: 603,447\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "maxLenV2 = 300 ## The variable name change is for reference only, since it was not changing from the original maxLen\n",
    "lstmGloVe_modelV2 = lstmArchitecture((maxLenV2,))\n",
    "lstmGloVe_modelV2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "40/40 [==============================] - 4s 112ms/step - loss: 1.0896 - acc: 0.2500\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 1.0860 - acc: 0.2750\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 4s 109ms/step - loss: 1.0806 - acc: 0.2250\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 4s 104ms/step - loss: 1.0686 - acc: 0.2500\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 4s 89ms/step - loss: 1.0715 - acc: 0.2500\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 4s 95ms/step - loss: 1.0752 - acc: 0.2750\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 4s 110ms/step - loss: 1.0823 - acc: 0.2250\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 1.0677 - acc: 0.2500\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 4s 93ms/step - loss: 1.0737 - acc: 0.2750\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 1.0723 - acc: 0.3000\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 4s 99ms/step - loss: 1.0703 - acc: 0.3000\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 4s 104ms/step - loss: 1.0714 - acc: 0.4000\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 4s 95ms/step - loss: 1.0535 - acc: 0.5000\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 3s 85ms/step - loss: 1.0685 - acc: 0.4250\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 4s 93ms/step - loss: 1.0565 - acc: 0.5250\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 4s 100ms/step - loss: 1.0588 - acc: 0.5000\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 4s 93ms/step - loss: 1.0594 - acc: 0.5250\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 1.0562 - acc: 0.5000\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 4s 89ms/step - loss: 1.0573 - acc: 0.5000\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 4s 98ms/step - loss: 1.0602 - acc: 0.5500\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 4s 96ms/step - loss: 1.0460 - acc: 0.5000\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 1.0564 - acc: 0.5000\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 3s 85ms/step - loss: 1.0550 - acc: 0.5000\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 1.0417 - acc: 0.5500\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 4s 110ms/step - loss: 1.0513 - acc: 0.5250\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 4s 101ms/step - loss: 1.0600 - acc: 0.5250\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 4s 101ms/step - loss: 1.0481 - acc: 0.5750\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 1.0474 - acc: 0.5750\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 4s 92ms/step - loss: 1.0376 - acc: 0.5750\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - 4s 102ms/step - loss: 1.0406 - acc: 0.5500\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - 5s 122ms/step - loss: 1.0373 - acc: 0.5750\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 1.0399 - acc: 0.5750\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - 4s 103ms/step - loss: 1.0371 - acc: 0.5500\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - 5s 113ms/step - loss: 1.0525 - acc: 0.5250\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - 6s 147ms/step - loss: 1.0372 - acc: 0.6250\n",
      "Epoch 36/100\n",
      "40/40 [==============================] - 6s 143ms/step - loss: 1.0343 - acc: 0.5500\n",
      "Epoch 37/100\n",
      "40/40 [==============================] - 5s 128ms/step - loss: 1.0327 - acc: 0.6250\n",
      "Epoch 38/100\n",
      "40/40 [==============================] - 5s 121ms/step - loss: 1.0335 - acc: 0.5750\n",
      "Epoch 39/100\n",
      "40/40 [==============================] - 5s 129ms/step - loss: 1.0361 - acc: 0.6250\n",
      "Epoch 40/100\n",
      "40/40 [==============================] - 4s 111ms/step - loss: 1.0289 - acc: 0.5750\n",
      "Epoch 41/100\n",
      "40/40 [==============================] - 4s 104ms/step - loss: 1.0417 - acc: 0.6000\n",
      "Epoch 42/100\n",
      "40/40 [==============================] - 4s 104ms/step - loss: 1.0260 - acc: 0.6250\n",
      "Epoch 43/100\n",
      "40/40 [==============================] - 4s 101ms/step - loss: 1.0228 - acc: 0.6500\n",
      "Epoch 44/100\n",
      "40/40 [==============================] - 5s 136ms/step - loss: 1.0325 - acc: 0.5750\n",
      "Epoch 45/100\n",
      "40/40 [==============================] - 5s 131ms/step - loss: 1.0257 - acc: 0.6500\n",
      "Epoch 46/100\n",
      "40/40 [==============================] - 5s 119ms/step - loss: 1.0235 - acc: 0.6500\n",
      "Epoch 47/100\n",
      "40/40 [==============================] - 5s 131ms/step - loss: 1.0195 - acc: 0.7250\n",
      "Epoch 48/100\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 1.0214 - acc: 0.6500\n",
      "Epoch 49/100\n",
      "40/40 [==============================] - 5s 118ms/step - loss: 1.0194 - acc: 0.6500\n",
      "Epoch 50/100\n",
      "40/40 [==============================] - 5s 123ms/step - loss: 1.0158 - acc: 0.6750\n",
      "Epoch 51/100\n",
      "40/40 [==============================] - 5s 124ms/step - loss: 1.0159 - acc: 0.6750\n",
      "Epoch 52/100\n",
      "40/40 [==============================] - 5s 121ms/step - loss: 1.0125 - acc: 0.6750\n",
      "Epoch 53/100\n",
      "40/40 [==============================] - 5s 115ms/step - loss: 1.0198 - acc: 0.7000\n",
      "Epoch 54/100\n",
      "40/40 [==============================] - 4s 100ms/step - loss: 1.0042 - acc: 0.7500\n",
      "Epoch 55/100\n",
      "40/40 [==============================] - 4s 102ms/step - loss: 1.0162 - acc: 0.7250\n",
      "Epoch 56/100\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 1.0054 - acc: 0.7250\n",
      "Epoch 57/100\n",
      "40/40 [==============================] - 5s 123ms/step - loss: 1.0081 - acc: 0.7500\n",
      "Epoch 58/100\n",
      "40/40 [==============================] - 6s 143ms/step - loss: 1.0148 - acc: 0.7250\n",
      "Epoch 59/100\n",
      "40/40 [==============================] - 7s 168ms/step - loss: 1.0049 - acc: 0.7000\n",
      "Epoch 60/100\n",
      "40/40 [==============================] - 7s 187ms/step - loss: 0.9960 - acc: 0.7750\n",
      "Epoch 61/100\n",
      "40/40 [==============================] - 6s 140ms/step - loss: 1.0092 - acc: 0.7500\n",
      "Epoch 62/100\n",
      "40/40 [==============================] - 5s 132ms/step - loss: 1.0047 - acc: 0.7250\n",
      "Epoch 63/100\n",
      "40/40 [==============================] - 5s 119ms/step - loss: 1.0069 - acc: 0.7000\n",
      "Epoch 64/100\n",
      "40/40 [==============================] - 4s 99ms/step - loss: 0.9958 - acc: 0.7250\n",
      "Epoch 65/100\n",
      "40/40 [==============================] - 4s 104ms/step - loss: 0.9929 - acc: 0.7250\n",
      "Epoch 66/100\n",
      "40/40 [==============================] - 4s 96ms/step - loss: 1.0053 - acc: 0.7500\n",
      "Epoch 67/100\n",
      "40/40 [==============================] - 4s 98ms/step - loss: 0.9985 - acc: 0.6750\n",
      "Epoch 68/100\n",
      "40/40 [==============================] - 4s 96ms/step - loss: 0.9972 - acc: 0.7500\n",
      "Epoch 69/100\n",
      "40/40 [==============================] - 4s 99ms/step - loss: 0.9884 - acc: 0.7500\n",
      "Epoch 70/100\n",
      "40/40 [==============================] - 4s 92ms/step - loss: 0.9897 - acc: 0.7750\n",
      "Epoch 71/100\n",
      "40/40 [==============================] - 4s 97ms/step - loss: 0.9965 - acc: 0.7250\n",
      "Epoch 72/100\n",
      "40/40 [==============================] - 4s 95ms/step - loss: 0.9893 - acc: 0.7500\n",
      "Epoch 73/100\n",
      "40/40 [==============================] - 4s 101ms/step - loss: 0.9900 - acc: 0.7250\n",
      "Epoch 74/100\n",
      "40/40 [==============================] - 4s 97ms/step - loss: 0.9865 - acc: 0.7500\n",
      "Epoch 75/100\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 0.9888 - acc: 0.7250\n",
      "Epoch 76/100\n",
      "40/40 [==============================] - 4s 89ms/step - loss: 0.9975 - acc: 0.6750\n",
      "Epoch 77/100\n",
      "40/40 [==============================] - 4s 93ms/step - loss: 0.9809 - acc: 0.7000\n",
      "Epoch 78/100\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 0.9761 - acc: 0.7250\n",
      "Epoch 79/100\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.9870 - acc: 0.7250\n",
      "Epoch 80/100\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.9762 - acc: 0.7500\n",
      "Epoch 81/100\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 0.9715 - acc: 0.7250\n",
      "Epoch 82/100\n",
      "40/40 [==============================] - 4s 92ms/step - loss: 0.9834 - acc: 0.7000\n",
      "Epoch 83/100\n",
      "40/40 [==============================] - 4s 98ms/step - loss: 0.9710 - acc: 0.7500\n",
      "Epoch 84/100\n",
      "40/40 [==============================] - 5s 115ms/step - loss: 0.9708 - acc: 0.7000\n",
      "Epoch 85/100\n",
      "40/40 [==============================] - 5s 115ms/step - loss: 0.9758 - acc: 0.7250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100\n",
      "40/40 [==============================] - 4s 104ms/step - loss: 0.9755 - acc: 0.7250\n",
      "Epoch 87/100\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.9760 - acc: 0.7500\n",
      "Epoch 88/100\n",
      "40/40 [==============================] - 4s 111ms/step - loss: 0.9788 - acc: 0.6750\n",
      "Epoch 89/100\n",
      "40/40 [==============================] - 5s 120ms/step - loss: 0.9677 - acc: 0.7500\n",
      "Epoch 90/100\n",
      "40/40 [==============================] - 4s 111ms/step - loss: 0.9744 - acc: 0.7000\n",
      "Epoch 91/100\n",
      "40/40 [==============================] - 4s 110ms/step - loss: 0.9529 - acc: 0.7500\n",
      "Epoch 92/100\n",
      "40/40 [==============================] - 4s 101ms/step - loss: 0.9695 - acc: 0.6750\n",
      "Epoch 93/100\n",
      "40/40 [==============================] - 4s 94ms/step - loss: 0.9627 - acc: 0.7000\n",
      "Epoch 94/100\n",
      "40/40 [==============================] - 4s 92ms/step - loss: 0.9522 - acc: 0.7500\n",
      "Epoch 95/100\n",
      "40/40 [==============================] - 4s 89ms/step - loss: 0.9691 - acc: 0.7250\n",
      "Epoch 96/100\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.9620 - acc: 0.7250\n",
      "Epoch 97/100\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 0.9453 - acc: 0.7750\n",
      "Epoch 98/100\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.9541 - acc: 0.7750\n",
      "Epoch 99/100\n",
      "40/40 [==============================] - 4s 95ms/step - loss: 0.9493 - acc: 0.7500\n",
      "Epoch 100/100\n",
      "40/40 [==============================] - 4s 93ms/step - loss: 0.9544 - acc: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1aab1726ba8>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Previous iteration lr = 0.0001 now lr = 0.00001\n",
    "adamV2 = keras.optimizers.Adam(lr = 0.00001) # 0.0001, 0.0002, 0.00001\n",
    "lstmGloVe_modelV2.compile(optimizer=adamV2, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "lstmGloVe_modelV2.fit(X_train_indices, y_train_labels, batch_size=4, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "10/10 [==============================] - 0s 25ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0086145401000977, 0.6000000238418579]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstmGloVe_modelV2.evaluate(X_test_indices, y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  [[0.35376057 0.32390282 0.32233655]\n",
      " [0.35376057 0.32390282 0.32233655]\n",
      " [0.35376057 0.32390282 0.32233655]\n",
      " [0.35376504 0.32390186 0.3223331 ]\n",
      " [0.4382766  0.3815464  0.18017696]\n",
      " [0.3537606  0.32390285 0.32233655]\n",
      " [0.4290003  0.37065172 0.20034799]\n",
      " [0.35376057 0.32390282 0.32233655]\n",
      " [0.3487999  0.37174937 0.27945074]\n",
      " [0.38032869 0.46068618 0.15898512]]\n",
      "actual:  [[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "predsV2 = lstmGloVe_modelV2.predict(X_test_indices)\n",
    "print('predicted: ',predsV2)\n",
    "print('actual: ',y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "      Expect inflation       0.50      1.00      0.67         4\n",
      "Inflation will go away       1.00      0.40      0.57         5\n",
      "               Neutral       0.00      0.00      0.00         1\n",
      "\n",
      "              accuracy                           0.60        10\n",
      "             macro avg       0.50      0.47      0.41        10\n",
      "          weighted avg       0.70      0.60      0.55        10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Expect inflation', 'Inflation will go away', 'Neutral']\n",
    "clsf_rep_2 = classification_report(np.argmax(y_test_labels,axis=1),np.argmax(predsV2,axis=1), target_names=target_names)\n",
    "print(clsf_rep_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2- Epochs from 100 to 150**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 300, 100)          469300    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 603,447\n",
      "Trainable params: 603,447\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "maxLenV3 = 300 ## The variable name change is for reference only, since it was not changing from the original maxLen\n",
    "lstmGloVe_modelV3 = lstmArchitecture((maxLenV3,))\n",
    "lstmGloVe_modelV3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "40/40 [==============================] - 6s 154ms/step - loss: 1.1144 - acc: 0.4000\n",
      "Epoch 2/150\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 1.0874 - acc: 0.4750\n",
      "Epoch 3/150\n",
      "40/40 [==============================] - 4s 101ms/step - loss: 1.0714 - acc: 0.5500\n",
      "Epoch 4/150\n",
      "40/40 [==============================] - 4s 97ms/step - loss: 1.0616 - acc: 0.5250\n",
      "Epoch 5/150\n",
      "40/40 [==============================] - 4s 109ms/step - loss: 1.0432 - acc: 0.6000\n",
      "Epoch 6/150\n",
      "40/40 [==============================] - 5s 124ms/step - loss: 1.0264 - acc: 0.5500\n",
      "Epoch 7/150\n",
      "40/40 [==============================] - 4s 109ms/step - loss: 1.0185 - acc: 0.6750\n",
      "Epoch 8/150\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 1.0038 - acc: 0.6750\n",
      "Epoch 9/150\n",
      "40/40 [==============================] - 5s 124ms/step - loss: 1.0052 - acc: 0.6250\n",
      "Epoch 10/150\n",
      "40/40 [==============================] - 5s 136ms/step - loss: 0.9815 - acc: 0.7500\n",
      "Epoch 11/150\n",
      "40/40 [==============================] - 6s 149ms/step - loss: 0.9772 - acc: 0.7250\n",
      "Epoch 12/150\n",
      "40/40 [==============================] - 5s 128ms/step - loss: 0.9511 - acc: 0.7250\n",
      "Epoch 13/150\n",
      "40/40 [==============================] - 4s 112ms/step - loss: 0.9337 - acc: 0.7000\n",
      "Epoch 14/150\n",
      "40/40 [==============================] - 4s 97ms/step - loss: 0.9333 - acc: 0.7500\n",
      "Epoch 15/150\n",
      "40/40 [==============================] - 4s 97ms/step - loss: 0.8972 - acc: 0.7250\n",
      "Epoch 16/150\n",
      "40/40 [==============================] - 4s 99ms/step - loss: 0.8759 - acc: 0.7000\n",
      "Epoch 17/150\n",
      "40/40 [==============================] - 4s 95ms/step - loss: 3.7629 - acc: 0.6750\n",
      "Epoch 18/150\n",
      "40/40 [==============================] - 4s 92ms/step - loss: 7.6793 - acc: 0.5250\n",
      "Epoch 19/150\n",
      "40/40 [==============================] - 4s 99ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 20/150\n",
      "40/40 [==============================] - 4s 102ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 21/150\n",
      "40/40 [==============================] - 4s 99ms/step - loss: 8.8650 - acc: 0.4500\n",
      "Epoch 22/150\n",
      "40/40 [==============================] - 4s 96ms/step - loss: 7.2531 - acc: 0.5500\n",
      "Epoch 23/150\n",
      "40/40 [==============================] - 4s 101ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 24/150\n",
      "40/40 [==============================] - 4s 99ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 25/150\n",
      "40/40 [==============================] - 4s 96ms/step - loss: 7.2531 - acc: 0.5500\n",
      "Epoch 26/150\n",
      "40/40 [==============================] - 4s 93ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 27/150\n",
      "40/40 [==============================] - 4s 95ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 28/150\n",
      "40/40 [==============================] - 4s 98ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 29/150\n",
      "40/40 [==============================] - 4s 94ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 30/150\n",
      "40/40 [==============================] - 4s 94ms/step - loss: 7.2531 - acc: 0.5500\n",
      "Epoch 31/150\n",
      "40/40 [==============================] - 4s 95ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 32/150\n",
      "40/40 [==============================] - 4s 99ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 33/150\n",
      "40/40 [==============================] - 4s 96ms/step - loss: 6.8502 - acc: 0.5750\n",
      "Epoch 34/150\n",
      "40/40 [==============================] - 4s 109ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 35/150\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 36/150\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 6.8502 - acc: 0.5750\n",
      "Epoch 37/150\n",
      "40/40 [==============================] - 4s 110ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 38/150\n",
      "40/40 [==============================] - 5s 114ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 39/150\n",
      "40/40 [==============================] - 5s 125ms/step - loss: 7.2531 - acc: 0.5500\n",
      "Epoch 40/150\n",
      "40/40 [==============================] - 5s 116ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 41/150\n",
      "40/40 [==============================] - 5s 114ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 42/150\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 43/150\n",
      "40/40 [==============================] - 4s 98ms/step - loss: 7.2531 - acc: 0.5500\n",
      "Epoch 44/150\n",
      "40/40 [==============================] - 4s 95ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 45/150\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 46/150\n",
      "40/40 [==============================] - 4s 92ms/step - loss: 7.2531 - acc: 0.5500\n",
      "Epoch 47/150\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 48/150\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 49/150\n",
      "40/40 [==============================] - 4s 96ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 50/150\n",
      "40/40 [==============================] - 4s 94ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 51/150\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 52/150\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 53/150\n",
      "40/40 [==============================] - 4s 100ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 54/150\n",
      "40/40 [==============================] - 4s 99ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 55/150\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 56/150\n",
      "40/40 [==============================] - 4s 103ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 57/150\n",
      "40/40 [==============================] - 4s 93ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 58/150\n",
      "40/40 [==============================] - 4s 89ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 59/150\n",
      "40/40 [==============================] - 4s 94ms/step - loss: 7.2531 - acc: 0.5500\n",
      "Epoch 60/150\n",
      "40/40 [==============================] - 4s 95ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 61/150\n",
      "40/40 [==============================] - 4s 94ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 62/150\n",
      "40/40 [==============================] - 4s 103ms/step - loss: 8.8650 - acc: 0.4500\n",
      "Epoch 63/150\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 64/150\n",
      "40/40 [==============================] - 5s 115ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 65/150\n",
      "40/40 [==============================] - 4s 111ms/step - loss: 8.8650 - acc: 0.4500\n",
      "Epoch 66/150\n",
      "40/40 [==============================] - 4s 105ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 67/150\n",
      "40/40 [==============================] - 5s 125ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 68/150\n",
      "40/40 [==============================] - 5s 129ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 69/150\n",
      "40/40 [==============================] - 5s 124ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 70/150\n",
      "40/40 [==============================] - 5s 113ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 71/150\n",
      "40/40 [==============================] - 5s 113ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 72/150\n",
      "40/40 [==============================] - 4s 102ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 73/150\n",
      "40/40 [==============================] - 4s 93ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 74/150\n",
      "40/40 [==============================] - 4s 95ms/step - loss: 7.2531 - acc: 0.5500\n",
      "Epoch 75/150\n",
      "40/40 [==============================] - 4s 95ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 76/150\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 77/150\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 78/150\n",
      "40/40 [==============================] - 4s 94ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 79/150\n",
      "40/40 [==============================] - 4s 92ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 80/150\n",
      "40/40 [==============================] - 4s 94ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 81/150\n",
      "40/40 [==============================] - 4s 92ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 82/150\n",
      "40/40 [==============================] - 4s 94ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 83/150\n",
      "40/40 [==============================] - 4s 102ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 84/150\n",
      "40/40 [==============================] - 4s 96ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 85/150\n",
      "40/40 [==============================] - 4s 93ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 86/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 4s 91ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 87/150\n",
      "40/40 [==============================] - 4s 94ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 88/150\n",
      "40/40 [==============================] - 4s 89ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 89/150\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 90/150\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 91/150\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 6.8502 - acc: 0.5750\n",
      "Epoch 92/150\n",
      "40/40 [==============================] - 4s 94ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 93/150\n",
      "40/40 [==============================] - 4s 103ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 94/150\n",
      "40/40 [==============================] - 4s 112ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 95/150\n",
      "40/40 [==============================] - 5s 115ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 96/150\n",
      "40/40 [==============================] - 4s 109ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 97/150\n",
      "40/40 [==============================] - 5s 121ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 98/150\n",
      "40/40 [==============================] - 6s 138ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 99/150\n",
      "40/40 [==============================] - 6s 150ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 100/150\n",
      "40/40 [==============================] - 6s 151ms/step - loss: 7.2531 - acc: 0.5500\n",
      "Epoch 101/150\n",
      "40/40 [==============================] - 5s 125ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 102/150\n",
      "40/40 [==============================] - 4s 110ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 103/150\n",
      "40/40 [==============================] - 4s 103ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 104/150\n",
      "40/40 [==============================] - 4s 102ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 105/150\n",
      "40/40 [==============================] - 4s 102ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 106/150\n",
      "40/40 [==============================] - 4s 96ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 107/150\n",
      "40/40 [==============================] - 4s 96ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 108/150\n",
      "40/40 [==============================] - 4s 97ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 109/150\n",
      "40/40 [==============================] - 4s 96ms/step - loss: 7.2531 - acc: 0.5500\n",
      "Epoch 110/150\n",
      "40/40 [==============================] - 4s 93ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 111/150\n",
      "40/40 [==============================] - 4s 89ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 112/150\n",
      "40/40 [==============================] - 4s 98ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 113/150\n",
      "40/40 [==============================] - 4s 111ms/step - loss: 9.2679 - acc: 0.4250\n",
      "Epoch 114/150\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 115/150\n",
      "40/40 [==============================] - 4s 104ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 116/150\n",
      "40/40 [==============================] - 5s 120ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 117/150\n",
      "40/40 [==============================] - 4s 109ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 118/150\n",
      "40/40 [==============================] - 4s 99ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 119/150\n",
      "40/40 [==============================] - 4s 95ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 120/150\n",
      "40/40 [==============================] - 4s 94ms/step - loss: 7.2531 - acc: 0.5500\n",
      "Epoch 121/150\n",
      "40/40 [==============================] - 4s 112ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 122/150\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 123/150\n",
      "40/40 [==============================] - 5s 127ms/step - loss: 7.2531 - acc: 0.5500\n",
      "Epoch 124/150\n",
      "40/40 [==============================] - 5s 132ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 125/150\n",
      "40/40 [==============================] - 5s 129ms/step - loss: 7.2531 - acc: 0.5500\n",
      "Epoch 126/150\n",
      "40/40 [==============================] - 5s 119ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 127/150\n",
      "40/40 [==============================] - 5s 124ms/step - loss: 7.2531 - acc: 0.5500\n",
      "Epoch 128/150\n",
      "40/40 [==============================] - 5s 116ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 129/150\n",
      "40/40 [==============================] - 4s 98ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 130/150\n",
      "40/40 [==============================] - 4s 93ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 131/150\n",
      "40/40 [==============================] - 4s 93ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 132/150\n",
      "40/40 [==============================] - 4s 93ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 133/150\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 134/150\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 135/150\n",
      "40/40 [==============================] - 4s 92ms/step - loss: 8.8650 - acc: 0.4500\n",
      "Epoch 136/150\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 137/150\n",
      "40/40 [==============================] - 4s 97ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 138/150\n",
      "40/40 [==============================] - 4s 97ms/step - loss: 7.2531 - acc: 0.5500\n",
      "Epoch 139/150\n",
      "40/40 [==============================] - 4s 92ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 140/150\n",
      "40/40 [==============================] - 4s 95ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 141/150\n",
      "40/40 [==============================] - 4s 92ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 142/150\n",
      "40/40 [==============================] - 4s 96ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 143/150\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 7.6561 - acc: 0.5250\n",
      "Epoch 144/150\n",
      "40/40 [==============================] - 4s 100ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 145/150\n",
      "40/40 [==============================] - 4s 94ms/step - loss: 8.0590 - acc: 0.5000\n",
      "Epoch 146/150\n",
      "40/40 [==============================] - 4s 89ms/step - loss: 8.8650 - acc: 0.4500\n",
      "Epoch 147/150\n",
      "40/40 [==============================] - 4s 89ms/step - loss: 7.2531 - acc: 0.5500\n",
      "Epoch 148/150\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 8.4620 - acc: 0.4750\n",
      "Epoch 149/150\n",
      "40/40 [==============================] - 4s 95ms/step - loss: 7.2531 - acc: 0.5500\n",
      "Epoch 150/150\n",
      "40/40 [==============================] - 4s 96ms/step - loss: 8.0590 - acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1aac628fc88>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original iteration epochs = 100 now epochs = 150\n",
    "adamV3 = keras.optimizers.Adam(lr = 0.0001) # 0.0001, 0.0002, 0.00001\n",
    "lstmGloVe_modelV3.compile(optimizer=adamV3, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "lstmGloVe_modelV3.fit(X_train_indices, y_train_labels, batch_size=4, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "10/10 [==============================] - 1s 61ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9.788687705993652, 0.30000001192092896]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstmGloVe_modelV3.evaluate(X_test_indices, y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  [[1.         0.         0.        ]\n",
      " [0.         1.         0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [0.4011797  0.29102105 0.30779922]\n",
      " [1.         0.         0.        ]\n",
      " [1.         0.         0.        ]]\n",
      "actual:  [[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "predsV3 = lstmGloVe_modelV3.predict(X_test_indices)\n",
    "print('predicted: ',predsV3)\n",
    "print('actual: ',y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "      Expect inflation       0.33      0.75      0.46         4\n",
      "Inflation will go away       0.00      0.00      0.00         5\n",
      "               Neutral       0.00      0.00      0.00         1\n",
      "\n",
      "              accuracy                           0.30        10\n",
      "             macro avg       0.11      0.25      0.15        10\n",
      "          weighted avg       0.13      0.30      0.18        10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Expect inflation', 'Inflation will go away', 'Neutral']\n",
    "clsf_rep_3 = classification_report(np.argmax(y_test_labels,axis=1),np.argmax(predsV3,axis=1), target_names=target_names)\n",
    "print(clsf_rep_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3- Maximum sentence len MaxLen from 300 to 150 and learning rate from 0.0001 to 0.00005**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 150, 100)          469300    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 603,447\n",
      "Trainable params: 603,447\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# With maxLen = 500 the loss function whent to infinity\n",
    "maxLenV4 = 150 ## Variable change meaningful, so I have to adjust other variables that depended on maxLen\n",
    "\n",
    "embedding_layer = Embedding(input_dim=vocab_len, output_dim=embed_vector_len, \n",
    "                            input_length=maxLenV4, weights = [emb_matrix], trainable=True)\n",
    "\n",
    "\n",
    "X_train_indicesV4 = tokenizer.texts_to_sequences(X_train_prepros)\n",
    "X_train_indicesV4 = pad_sequences(X_train_indices, maxlen=maxLenV4, padding='post')\n",
    "\n",
    "X_test_indicesV4 = tokenizer.texts_to_sequences(X_test_prepros)\n",
    "X_test_indicesV4 = pad_sequences(X_test_indices, maxlen=maxLenV4, padding='post')\n",
    "\n",
    "lstmGloVe_modelV4 = lstmArchitecture((maxLenV4,))\n",
    "lstmGloVe_modelV4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "40/40 [==============================] - 4s 92ms/step - loss: 1.0719 - acc: 0.5000\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 2s 56ms/step - loss: 1.0607 - acc: 0.5750\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 2s 56ms/step - loss: 1.0573 - acc: 0.5500\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 2s 55ms/step - loss: 1.0509 - acc: 0.5500\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 2s 56ms/step - loss: 1.0397 - acc: 0.6000\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 2s 57ms/step - loss: 1.0424 - acc: 0.5750\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 1.0290 - acc: 0.6000\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 1.0199 - acc: 0.6000\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 3s 63ms/step - loss: 1.0174 - acc: 0.5500\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 1.0108 - acc: 0.5750\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 2s 57ms/step - loss: 1.0115 - acc: 0.5500\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 2s 49ms/step - loss: 0.9937 - acc: 0.6500: 1s - loss: 0.9933 - acc: \n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 2s 53ms/step - loss: 0.9952 - acc: 0.5750\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 2s 53ms/step - loss: 0.9931 - acc: 0.6000\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 2s 51ms/step - loss: 0.9757 - acc: 0.6250\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 2s 52ms/step - loss: 0.9628 - acc: 0.6250\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.9712 - acc: 0.5750: 0s - loss: 1.0071 - acc: 0.5\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.9465 - acc: 0.6750\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.9496 - acc: 0.7500\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.9452 - acc: 0.7000\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 2s 44ms/step - loss: 0.9291 - acc: 0.6500\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 2s 44ms/step - loss: 0.9274 - acc: 0.6750: 1s - loss: 0.9366 - acc: \n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.9083 - acc: 0.7500\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.8972 - acc: 0.7750\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.8765 - acc: 0.7250\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 2s 45ms/step - loss: 0.8882 - acc: 0.7500\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 2s 45ms/step - loss: 0.8499 - acc: 0.7500\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 2s 45ms/step - loss: 0.8377 - acc: 0.8000\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 2.7287 - acc: 0.6500\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - 2s 53ms/step - loss: 2.6169 - acc: 0.6750\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - 2s 51ms/step - loss: 1.9032 - acc: 0.7000\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.8204 - acc: 0.8000\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.8180 - acc: 0.7500\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - 2s 44ms/step - loss: 0.8408 - acc: 0.7500\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.8305 - acc: 0.7500: 0s - loss: 0.8148 - acc: 0.75\n",
      "Epoch 36/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.8253 - acc: 0.7500\n",
      "Epoch 37/100\n",
      "40/40 [==============================] - 2s 53ms/step - loss: 0.8287 - acc: 0.7500\n",
      "Epoch 38/100\n",
      "40/40 [==============================] - 2s 54ms/step - loss: 0.8160 - acc: 0.7750\n",
      "Epoch 39/100\n",
      "40/40 [==============================] - 2s 50ms/step - loss: 0.8083 - acc: 0.7750\n",
      "Epoch 40/100\n",
      "40/40 [==============================] - 2s 49ms/step - loss: 0.8055 - acc: 0.7750\n",
      "Epoch 41/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.7999 - acc: 0.7750\n",
      "Epoch 42/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.7943 - acc: 0.7750\n",
      "Epoch 43/100\n",
      "40/40 [==============================] - 2s 45ms/step - loss: 0.8069 - acc: 0.7750\n",
      "Epoch 44/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.7938 - acc: 0.7500\n",
      "Epoch 45/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.7883 - acc: 0.7500\n",
      "Epoch 46/100\n",
      "40/40 [==============================] - 2s 45ms/step - loss: 0.7802 - acc: 0.7750\n",
      "Epoch 47/100\n",
      "40/40 [==============================] - 2s 45ms/step - loss: 0.8015 - acc: 0.7750\n",
      "Epoch 48/100\n",
      "40/40 [==============================] - 2s 44ms/step - loss: 0.7870 - acc: 0.7750\n",
      "Epoch 49/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.7679 - acc: 0.8000\n",
      "Epoch 50/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.7796 - acc: 0.8000\n",
      "Epoch 51/100\n",
      "40/40 [==============================] - 2s 45ms/step - loss: 0.7628 - acc: 0.8000\n",
      "Epoch 52/100\n",
      "40/40 [==============================] - 2s 45ms/step - loss: 0.7687 - acc: 0.8000\n",
      "Epoch 53/100\n",
      "40/40 [==============================] - 2s 45ms/step - loss: 0.7612 - acc: 0.8000\n",
      "Epoch 54/100\n",
      "40/40 [==============================] - 2s 45ms/step - loss: 0.7741 - acc: 0.8000\n",
      "Epoch 55/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.7559 - acc: 0.8000\n",
      "Epoch 56/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.7553 - acc: 0.8000\n",
      "Epoch 57/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.7608 - acc: 0.7750\n",
      "Epoch 58/100\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 0.7412 - acc: 0.7750\n",
      "Epoch 59/100\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.7539 - acc: 0.7750\n",
      "Epoch 60/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.7280 - acc: 0.8000\n",
      "Epoch 61/100\n",
      "40/40 [==============================] - 2s 57ms/step - loss: 0.7412 - acc: 0.7750\n",
      "Epoch 62/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.7333 - acc: 0.8000\n",
      "Epoch 63/100\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 0.7397 - acc: 0.7750\n",
      "Epoch 64/100\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.7351 - acc: 0.7750\n",
      "Epoch 65/100\n",
      "40/40 [==============================] - 3s 77ms/step - loss: 0.7259 - acc: 0.7750\n",
      "Epoch 66/100\n",
      "40/40 [==============================] - 4s 97ms/step - loss: 0.7252 - acc: 0.8000\n",
      "Epoch 67/100\n",
      "40/40 [==============================] - 3s 71ms/step - loss: 0.7244 - acc: 0.7750\n",
      "Epoch 68/100\n",
      "40/40 [==============================] - 3s 74ms/step - loss: 0.6984 - acc: 0.8000\n",
      "Epoch 69/100\n",
      "40/40 [==============================] - 3s 78ms/step - loss: 0.7330 - acc: 0.8000\n",
      "Epoch 70/100\n",
      "40/40 [==============================] - 3s 83ms/step - loss: 0.7046 - acc: 0.8000\n",
      "Epoch 71/100\n",
      "40/40 [==============================] - 3s 80ms/step - loss: 0.6900 - acc: 0.8000\n",
      "Epoch 72/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.6813 - acc: 0.8000\n",
      "Epoch 73/100\n",
      "40/40 [==============================] - 2s 55ms/step - loss: 0.6861 - acc: 0.8000\n",
      "Epoch 74/100\n",
      "40/40 [==============================] - 2s 54ms/step - loss: 0.6906 - acc: 0.8000\n",
      "Epoch 75/100\n",
      "40/40 [==============================] - 2s 57ms/step - loss: 0.6871 - acc: 0.8250\n",
      "Epoch 76/100\n",
      "40/40 [==============================] - 2s 57ms/step - loss: 0.7040 - acc: 0.8000\n",
      "Epoch 77/100\n",
      "40/40 [==============================] - 2s 54ms/step - loss: 0.6936 - acc: 0.7750\n",
      "Epoch 78/100\n",
      "40/40 [==============================] - 2s 54ms/step - loss: 0.6768 - acc: 0.8000\n",
      "Epoch 79/100\n",
      "40/40 [==============================] - 2s 55ms/step - loss: 0.6964 - acc: 0.8000\n",
      "Epoch 80/100\n",
      "40/40 [==============================] - 2s 55ms/step - loss: 0.6961 - acc: 0.8000\n",
      "Epoch 81/100\n",
      "40/40 [==============================] - 2s 53ms/step - loss: 0.6612 - acc: 0.8000\n",
      "Epoch 82/100\n",
      "40/40 [==============================] - 2s 56ms/step - loss: 0.6756 - acc: 0.8000\n",
      "Epoch 83/100\n",
      "40/40 [==============================] - 2s 55ms/step - loss: 0.6603 - acc: 0.8000\n",
      "Epoch 84/100\n",
      "40/40 [==============================] - 2s 51ms/step - loss: 0.6661 - acc: 0.8000\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 2s 52ms/step - loss: 0.6481 - acc: 0.8000\n",
      "Epoch 86/100\n",
      "40/40 [==============================] - 2s 52ms/step - loss: 0.6636 - acc: 0.8000\n",
      "Epoch 87/100\n",
      "40/40 [==============================] - 2s 49ms/step - loss: 0.6553 - acc: 0.8000\n",
      "Epoch 88/100\n",
      "40/40 [==============================] - 2s 49ms/step - loss: 0.6323 - acc: 0.8000\n",
      "Epoch 89/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.6616 - acc: 0.8000\n",
      "Epoch 90/100\n",
      "40/40 [==============================] - 2s 51ms/step - loss: 0.6323 - acc: 0.8250\n",
      "Epoch 91/100\n",
      "40/40 [==============================] - 2s 50ms/step - loss: 0.6258 - acc: 0.8000\n",
      "Epoch 92/100\n",
      "40/40 [==============================] - 2s 52ms/step - loss: 0.6059 - acc: 0.7750\n",
      "Epoch 93/100\n",
      "40/40 [==============================] - 2s 50ms/step - loss: 0.6223 - acc: 0.8000\n",
      "Epoch 94/100\n",
      "40/40 [==============================] - 2s 50ms/step - loss: 0.6095 - acc: 0.8000: 0s - loss: 0.5931 - acc: 0.805\n",
      "Epoch 95/100\n",
      "40/40 [==============================] - 2s 50ms/step - loss: 0.6085 - acc: 0.8000\n",
      "Epoch 96/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.6044 - acc: 0.8000\n",
      "Epoch 97/100\n",
      "40/40 [==============================] - 2s 49ms/step - loss: 0.6041 - acc: 0.8250\n",
      "Epoch 98/100\n",
      "40/40 [==============================] - 2s 51ms/step - loss: 0.5922 - acc: 0.8250\n",
      "Epoch 99/100\n",
      "40/40 [==============================] - 2s 58ms/step - loss: 0.5889 - acc: 0.8250\n",
      "Epoch 100/100\n",
      "40/40 [==============================] - 2s 57ms/step - loss: 0.5981 - acc: 0.8250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1aac6670e10>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original lr = 0.0001 now lr = 0.00005\n",
    "adamV4 = keras.optimizers.Adam(lr = 0.00005) # 0.0001, 0.0002, 0.00001\n",
    "lstmGloVe_modelV4.compile(optimizer=adamV4, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "lstmGloVe_modelV4.fit(X_train_indicesV4, y_train_labels, batch_size=4, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "10/10 [==============================] - 1s 87ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0156376361846924, 0.4000000059604645]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstmGloVe_modelV4.evaluate(X_test_indicesV4, y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  [[0.50240266 0.21173917 0.28585815]\n",
      " [0.50240266 0.21173917 0.28585815]\n",
      " [0.50240266 0.21173917 0.28585815]\n",
      " [0.5024856  0.21166041 0.28585395]\n",
      " [0.42380327 0.44082615 0.13537061]\n",
      " [0.502403   0.21173882 0.28585818]\n",
      " [0.51898193 0.34723926 0.13377874]\n",
      " [0.50240266 0.21173917 0.28585815]\n",
      " [0.5560447  0.279491   0.16446427]\n",
      " [0.29723078 0.58076614 0.12200305]]\n",
      "actual:  [[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "predsV4 = lstmGloVe_modelV4.predict(X_test_indicesV4)\n",
    "print('predicted: ',predsV4)\n",
    "print('actual: ',y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "      Expect inflation       0.38      0.75      0.50         4\n",
      "Inflation will go away       0.50      0.20      0.29         5\n",
      "               Neutral       0.00      0.00      0.00         1\n",
      "\n",
      "              accuracy                           0.40        10\n",
      "             macro avg       0.29      0.32      0.26        10\n",
      "          weighted avg       0.40      0.40      0.34        10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Expect inflation', 'Inflation will go away', 'Neutral']\n",
    "clsf_rep_4 = classification_report(np.argmax(y_test_labels,axis=1),np.argmax(predsV4,axis=1), target_names=target_names)\n",
    "print(clsf_rep_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the GRU model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting variables to best variables of the LSTM model\n",
    "maxLen = 150 ## 150 or 300\n",
    "\n",
    "embedding_layer = Embedding(input_dim=vocab_len, output_dim=embed_vector_len, \n",
    "                            input_length=maxLen, weights = [emb_matrix], trainable=True)\n",
    "\n",
    "\n",
    "X_train_indices = tokenizer.texts_to_sequences(X_train_prepros)\n",
    "X_train_indices = pad_sequences(X_train_indices, maxlen=maxLen, padding='post')\n",
    "\n",
    "X_test_indices = tokenizer.texts_to_sequences(X_test_prepros)\n",
    "X_test_indices = pad_sequences(X_test_indices, maxlen=maxLen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 150, 100)          469300    \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 128)               87936     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 574,135\n",
      "Trainable params: 574,135\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gruGloVe_model = gruArchitecture((maxLen,))\n",
    "gruGloVe_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "40/40 [==============================] - 4s 100ms/step - loss: 1.1418 - acc: 0.2250\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 1.1390 - acc: 0.2250\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 1.1295 - acc: 0.2750\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 1.1329 - acc: 0.2250\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 2s 44ms/step - loss: 1.1351 - acc: 0.2000\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 2s 44ms/step - loss: 1.1297 - acc: 0.2500\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 1.1208 - acc: 0.3500\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 1.1116 - acc: 0.3500\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - ETA: 0s - loss: 1.1191 - acc: 0.305 - 2s 49ms/step - loss: 1.1240 - acc: 0.2750\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 1.1196 - acc: 0.3500\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 1.1157 - acc: 0.3000\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 2s 42ms/step - loss: 1.1165 - acc: 0.3500\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 2s 44ms/step - loss: 1.1148 - acc: 0.4000\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 2s 43ms/step - loss: 1.1048 - acc: 0.3750\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 2s 39ms/step - loss: 1.0997 - acc: 0.4000\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 2s 40ms/step - loss: 1.1171 - acc: 0.2500\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 2s 45ms/step - loss: 1.1007 - acc: 0.3750\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 1.1045 - acc: 0.4250\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 1.0928 - acc: 0.4500\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 1.1049 - acc: 0.3500\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 1.0859 - acc: 0.4500\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 2s 50ms/step - loss: 1.0925 - acc: 0.4000\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 1.0976 - acc: 0.4250\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 1.0825 - acc: 0.4750\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 2s 49ms/step - loss: 1.0840 - acc: 0.5000\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 1.0833 - acc: 0.4500\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 1.0914 - acc: 0.4000\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 1.0756 - acc: 0.5250\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 2s 45ms/step - loss: 1.0878 - acc: 0.5000\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - 2s 44ms/step - loss: 1.0698 - acc: 0.5250\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - 2s 38ms/step - loss: 1.0633 - acc: 0.5000\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 1.0608 - acc: 0.5250\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 1.0683 - acc: 0.6000\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 1.0587 - acc: 0.5250\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 1.0649 - acc: 0.5500\n",
      "Epoch 36/100\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 1.0586 - acc: 0.5750\n",
      "Epoch 37/100\n",
      "40/40 [==============================] - 1s 35ms/step - loss: 1.0627 - acc: 0.5250\n",
      "Epoch 38/100\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 1.0526 - acc: 0.5750\n",
      "Epoch 39/100\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 1.0516 - acc: 0.5250\n",
      "Epoch 40/100\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 1.0383 - acc: 0.6250\n",
      "Epoch 41/100\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 1.0477 - acc: 0.5750\n",
      "Epoch 42/100\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 1.0415 - acc: 0.5000\n",
      "Epoch 43/100\n",
      "40/40 [==============================] - 2s 38ms/step - loss: 1.0476 - acc: 0.5500\n",
      "Epoch 44/100\n",
      "40/40 [==============================] - 1s 35ms/step - loss: 1.0388 - acc: 0.6000\n",
      "Epoch 45/100\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 1.0451 - acc: 0.6000\n",
      "Epoch 46/100\n",
      "40/40 [==============================] - 1s 35ms/step - loss: 1.0393 - acc: 0.6250\n",
      "Epoch 47/100\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 1.0390 - acc: 0.5750\n",
      "Epoch 48/100\n",
      "40/40 [==============================] - 1s 35ms/step - loss: 1.0308 - acc: 0.5250\n",
      "Epoch 49/100\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 1.0361 - acc: 0.5750\n",
      "Epoch 50/100\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 1.0355 - acc: 0.5750\n",
      "Epoch 51/100\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 1.0410 - acc: 0.5750\n",
      "Epoch 52/100\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 1.0340 - acc: 0.5500\n",
      "Epoch 53/100\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 1.0160 - acc: 0.5750\n",
      "Epoch 54/100\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 1.0224 - acc: 0.5750\n",
      "Epoch 55/100\n",
      "40/40 [==============================] - 1s 35ms/step - loss: 1.0333 - acc: 0.5750\n",
      "Epoch 56/100\n",
      "40/40 [==============================] - 1s 35ms/step - loss: 1.0303 - acc: 0.5750\n",
      "Epoch 57/100\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 1.0182 - acc: 0.5750\n",
      "Epoch 58/100\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 1.0199 - acc: 0.5500\n",
      "Epoch 59/100\n",
      "40/40 [==============================] - 2s 39ms/step - loss: 1.0259 - acc: 0.5500\n",
      "Epoch 60/100\n",
      "40/40 [==============================] - 2s 40ms/step - loss: 1.0212 - acc: 0.5750\n",
      "Epoch 61/100\n",
      "40/40 [==============================] - 2s 39ms/step - loss: 1.0074 - acc: 0.6000\n",
      "Epoch 62/100\n",
      "40/40 [==============================] - 2s 41ms/step - loss: 1.0238 - acc: 0.5750\n",
      "Epoch 63/100\n",
      "40/40 [==============================] - 2s 39ms/step - loss: 0.9967 - acc: 0.5500\n",
      "Epoch 64/100\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 1.0215 - acc: 0.5500\n",
      "Epoch 65/100\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 1.0037 - acc: 0.5500\n",
      "Epoch 66/100\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 1.0107 - acc: 0.5500\n",
      "Epoch 67/100\n",
      "40/40 [==============================] - 1s 35ms/step - loss: 1.0064 - acc: 0.5750\n",
      "Epoch 68/100\n",
      "40/40 [==============================] - 2s 40ms/step - loss: 0.9998 - acc: 0.5500\n",
      "Epoch 69/100\n",
      "40/40 [==============================] - 1s 34ms/step - loss: 1.0002 - acc: 0.5750\n",
      "Epoch 70/100\n",
      "40/40 [==============================] - 1s 35ms/step - loss: 0.9999 - acc: 0.5500\n",
      "Epoch 71/100\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 1.0048 - acc: 0.5500\n",
      "Epoch 72/100\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 0.9881 - acc: 0.5750\n",
      "Epoch 73/100\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 0.9952 - acc: 0.5500\n",
      "Epoch 74/100\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 0.9884 - acc: 0.5500\n",
      "Epoch 75/100\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 1.0003 - acc: 0.5750\n",
      "Epoch 76/100\n",
      "40/40 [==============================] - 2s 38ms/step - loss: 0.9867 - acc: 0.5750\n",
      "Epoch 77/100\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 0.9842 - acc: 0.5500\n",
      "Epoch 78/100\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 0.9890 - acc: 0.5500\n",
      "Epoch 79/100\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 0.9868 - acc: 0.5750\n",
      "Epoch 80/100\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 0.9824 - acc: 0.5500\n",
      "Epoch 81/100\n",
      "40/40 [==============================] - 1s 36ms/step - loss: 0.9868 - acc: 0.5500\n",
      "Epoch 82/100\n",
      "40/40 [==============================] - 2s 44ms/step - loss: 0.9815 - acc: 0.5500\n",
      "Epoch 83/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.9737 - acc: 0.5500: 0s - loss: 0.9811 - acc: 0.5\n",
      "Epoch 84/100\n",
      "40/40 [==============================] - 2s 44ms/step - loss: 0.9788 - acc: 0.5750\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 2s 44ms/step - loss: 0.9706 - acc: 0.5500\n",
      "Epoch 86/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.9722 - acc: 0.5500\n",
      "Epoch 87/100\n",
      "40/40 [==============================] - 2s 44ms/step - loss: 0.9732 - acc: 0.5500\n",
      "Epoch 88/100\n",
      "40/40 [==============================] - 2s 40ms/step - loss: 0.9743 - acc: 0.5500\n",
      "Epoch 89/100\n",
      "40/40 [==============================] - 2s 41ms/step - loss: 0.9711 - acc: 0.5500\n",
      "Epoch 90/100\n",
      "40/40 [==============================] - 2s 41ms/step - loss: 0.9683 - acc: 0.5500\n",
      "Epoch 91/100\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 0.9690 - acc: 0.5500\n",
      "Epoch 92/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.9692 - acc: 0.5500\n",
      "Epoch 93/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.9533 - acc: 0.5500\n",
      "Epoch 94/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.9677 - acc: 0.5750\n",
      "Epoch 95/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.9616 - acc: 0.5500\n",
      "Epoch 96/100\n",
      "40/40 [==============================] - 2s 49ms/step - loss: 0.9595 - acc: 0.5500\n",
      "Epoch 97/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.9668 - acc: 0.5500\n",
      "Epoch 98/100\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.9554 - acc: 0.555 - 2s 46ms/step - loss: 0.9540 - acc: 0.5500\n",
      "Epoch 99/100\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 0.9591 - acc: 0.5500\n",
      "Epoch 100/100\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.9588 - acc: 0.5500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1aacc110e10>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting with smallest lr from LSTM. If in doubt I will test 0.00005\n",
    "adam = keras.optimizers.Adam(lr = 0.00001) # 0.0001, 0.0002, 0.00001\n",
    "gruGloVe_model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "gruGloVe_model.fit(X_train_indices, y_train_labels, batch_size=4, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "10/10 [==============================] - 1s 92ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.078713059425354, 0.4000000059604645]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gruGloVe_model.evaluate(X_test_indices, y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds2 = gruGloVe_model.predict(X_test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  [[0.35226646 0.32542554 0.32230797]\n",
      " [0.35226762 0.32542557 0.3223068 ]\n",
      " [0.35226646 0.32542554 0.32230797]\n",
      " [0.47480887 0.2563035  0.26888764]\n",
      " [0.44469622 0.295655   0.25964877]\n",
      " [0.4428601  0.2804908  0.2766491 ]\n",
      " [0.43489844 0.28000346 0.28509805]\n",
      " [0.35226646 0.32542554 0.32230797]\n",
      " [0.44471386 0.3254286  0.2298576 ]\n",
      " [0.42666602 0.2941859  0.27914807]]\n",
      "actual:  [[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print('predicted: ',preds2)\n",
    "print('actual: ',y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "      Expect inflation       0.40      1.00      0.57         4\n",
      "Inflation will go away       0.00      0.00      0.00         5\n",
      "               Neutral       0.00      0.00      0.00         1\n",
      "\n",
      "              accuracy                           0.40        10\n",
      "             macro avg       0.13      0.33      0.19        10\n",
      "          weighted avg       0.16      0.40      0.23        10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Expect inflation', 'Inflation will go away', 'Neutral']\n",
    "clsf_rep_5 = classification_report(np.argmax(y_test_labels,axis=1),np.argmax(preds2,axis=1), target_names=target_names)\n",
    "print(clsf_rep_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "we still say the course until the job is done” said jerome powell the federal reserve’s chairman on december 14th shortly after the central bank’s latest interestrate rise as a statement of intent his words were both straightforward and utterly sensible but what it means for the job to be done is becoming a matter of controversy inflation remains uncomfortably high meanwhile the aggressive monetary tightening of the past year is only now filtering through to the economy complicating assessments of whether the fed has in fact done enough to rein in prices  promisingly after a difficult two years inflation does appear to be easing its grip on the american economy overall prices increased by a mere 01 monthonmonth in november according to data published on december 13th making for that rarest of recent occurrences a downside surprise most encouraging was a breakdown showing that core inflation which strips out volatile food and energy costs had decelerated for a second consecutive month see chart  investors and analysts scarred by america’s relentless run of inflation have learned to restrain their hopes after a single month of rosy data yearonyear rates of inflation remain elevated at 71 for headline inflation but the disinflation in november follows a similarly cheerful batch of data for october optimism is on the rise albeit still mostly of the cautious rather than the unbridled kind since midoctober the sp500 index of leading american firms has recovered some of the ground it lost earlier this year concerns are shifting to the prospect of weaker growth many economists forecast a recession early next year  for the fed these countervailing forces create a tricky balance on the one hand it has just administered the sharpest tightening of monetary policy in four decades lifting interest rates from a floor of 0 in march to more than 4 today with inflation ebbing it is prudent to slow the pace of rate increases on the other hand a perception of fed softening risks adding fuel to the market rally that in turn would cause financial conditions to ease thereby placing upward pressure on inflation  the fed has tried to resolve this conundrum by maintaining its hawkish tone at the same time as tweaking its policies on december 14th the fed raised rates by half a percentage point ending a string of jumbo threequarterpoint increases most fed officials believe that they will raise rates to more than 5 next year and refrain from cutting rates until 2024 according to the central bank’s latest projections “historical experience cautions strongly against prematurely easing” said mr powell  strikingly many investors think the fed will end up being more dovish bond pricing suggests that rates will peak at less than 5 and that the central bank will start cutting them before the end of 2023  ultimately the decision will come down to the data prices of consumer goods have started falling as pandemicera shortages melt away housing prices are also trending lower the big lingering concern is whether a tight labour market will push incomes and by extension prices higher investors are betting that wage increases will slow as the economy weakens the fed understandably is not popping the champagne just yet\n",
      "Inflation will fade away\n",
      "1\n",
      "acual:  [0. 1. 0.]\n",
      "predicted:  [0.4428601 0.2804908 0.2766491]\n"
     ]
    }
   ],
   "source": [
    "n = np.random.randint(0,10)\n",
    "print(n)\n",
    "print(X_test[n])\n",
    "print(y_test[n])\n",
    "print(y_test_encoded[n])\n",
    "print('acual: ',y_test_labels[n])\n",
    "print('predicted: ',preds2[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting up Bidirectional LSTM RNN and GRU RNN for text classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biDirect_lstmArch(input_shape):\n",
    "\n",
    "    X_indices = Input(input_shape)\n",
    "\n",
    "    embeddings = embedding_layer(X_indices) \n",
    "\n",
    "    lstmLayer = Bidirectional(LSTM(128,activation='relu'))(embeddings) # return_sequences=False\n",
    "\n",
    "    dropoutLayer = Dropout(0.1)(lstmLayer) \n",
    "    \n",
    "    dense_1 = Dense(128, activation='relu')(dropoutLayer)\n",
    "\n",
    "    dense_2 = Dense(3, activation='softmax')(dense_1)\n",
    "\n",
    "    model = Model(inputs=X_indices, outputs=dense_2) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biDirect_gruArch(input_shape):\n",
    "\n",
    "    X_indices = Input(input_shape)\n",
    "\n",
    "    embeddings = embedding_layer(X_indices) \n",
    "\n",
    "    gruLayer = Bidirectional(GRU(128,activation='relu'))(embeddings) \n",
    "\n",
    "    dropoutLayer = Dropout(0.1)(gruLayer) \n",
    "    \n",
    "    dense_1 = Dense(128, activation='relu')(dropoutLayer)\n",
    "\n",
    "    dense_2 = Dense(3, activation='softmax')(dense_1)\n",
    "\n",
    "    model = Model(inputs=X_indices, outputs=dense_2) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the Bidirectional LSTM model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 150, 100)          469300    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 737,079\n",
      "Trainable params: 737,079\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstmBiDirect_model = biDirect_lstmArch((maxLen,))\n",
    "lstmBiDirect_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "40/40 [==============================] - 8s 200ms/step - loss: 1.1459 - acc: 0.2750\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 1.1490 - acc: 0.2250\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 3s 87ms/step - loss: 1.1359 - acc: 0.2750\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 3s 86ms/step - loss: 1.1259 - acc: 0.3500\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 3s 87ms/step - loss: 1.1226 - acc: 0.2500\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 1.1127 - acc: 0.2500\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 1.1094 - acc: 0.2750\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 4s 89ms/step - loss: 1.1048 - acc: 0.3000\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 1.1007 - acc: 0.3250\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 1.0943 - acc: 0.3250\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 3s 87ms/step - loss: 1.0801 - acc: 0.3500\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 4s 92ms/step - loss: 1.0790 - acc: 0.3500\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 1.0590 - acc: 0.4250\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 4s 89ms/step - loss: 1.0715 - acc: 0.4750\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 1.0520 - acc: 0.5000\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 1.0591 - acc: 0.5000\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 4s 93ms/step - loss: 1.0543 - acc: 0.5750\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 4s 92ms/step - loss: 1.0567 - acc: 0.5750\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 4s 95ms/step - loss: 1.0542 - acc: 0.4500\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 1.0320 - acc: 0.6250\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 4s 101ms/step - loss: 1.0319 - acc: 0.6250\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 4s 99ms/step - loss: 1.0186 - acc: 0.6500\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 1.0359 - acc: 0.5750\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 4s 101ms/step - loss: 1.0297 - acc: 0.6000\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 4s 111ms/step - loss: 1.0158 - acc: 0.6000\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 5s 114ms/step - loss: 1.0251 - acc: 0.5000\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 5s 114ms/step - loss: 1.0167 - acc: 0.5500\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 1.0115 - acc: 0.6000\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 4s 110ms/step - loss: 1.0060 - acc: 0.6250\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - 4s 111ms/step - loss: 1.0084 - acc: 0.5750\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - 4s 96ms/step - loss: 0.9959 - acc: 0.5500\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - 4s 89ms/step - loss: 1.0016 - acc: 0.5750\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - 4s 89ms/step - loss: 0.9912 - acc: 0.5750\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.9904 - acc: 0.5500\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.9919 - acc: 0.5750\n",
      "Epoch 36/100\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 0.9894 - acc: 0.5750\n",
      "Epoch 37/100\n",
      "40/40 [==============================] - 3s 87ms/step - loss: 0.9924 - acc: 0.5500\n",
      "Epoch 38/100\n",
      "40/40 [==============================] - 3s 87ms/step - loss: 0.9822 - acc: 0.5750\n",
      "Epoch 39/100\n",
      "40/40 [==============================] - 3s 87ms/step - loss: 0.9779 - acc: 0.5750\n",
      "Epoch 40/100\n",
      "40/40 [==============================] - 3s 87ms/step - loss: 0.9824 - acc: 0.5750\n",
      "Epoch 41/100\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 0.9883 - acc: 0.5750\n",
      "Epoch 42/100\n",
      "40/40 [==============================] - 3s 86ms/step - loss: 0.9663 - acc: 0.5500\n",
      "Epoch 43/100\n",
      "40/40 [==============================] - 3s 85ms/step - loss: 0.9701 - acc: 0.5500\n",
      "Epoch 44/100\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 0.9692 - acc: 0.5750\n",
      "Epoch 45/100\n",
      "40/40 [==============================] - 4s 92ms/step - loss: 0.9710 - acc: 0.5500\n",
      "Epoch 46/100\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 0.9647 - acc: 0.5500\n",
      "Epoch 47/100\n",
      "40/40 [==============================] - 4s 89ms/step - loss: 0.9674 - acc: 0.5500\n",
      "Epoch 48/100\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 0.9597 - acc: 0.5500\n",
      "Epoch 49/100\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 0.9636 - acc: 0.5500\n",
      "Epoch 50/100\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 0.9517 - acc: 0.5500\n",
      "Epoch 51/100\n",
      "40/40 [==============================] - 4s 95ms/step - loss: 0.9516 - acc: 0.5750\n",
      "Epoch 52/100\n",
      "40/40 [==============================] - 4s 94ms/step - loss: 0.9405 - acc: 0.5500\n",
      "Epoch 53/100\n",
      "40/40 [==============================] - 4s 109ms/step - loss: 0.9525 - acc: 0.5500\n",
      "Epoch 54/100\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.9455 - acc: 0.5500\n",
      "Epoch 55/100\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.9366 - acc: 0.5500\n",
      "Epoch 56/100\n",
      "40/40 [==============================] - 4s 111ms/step - loss: 0.9397 - acc: 0.5500\n",
      "Epoch 57/100\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.9316 - acc: 0.5750\n",
      "Epoch 58/100\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 0.9473 - acc: 0.5500\n",
      "Epoch 59/100\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.9264 - acc: 0.5750\n",
      "Epoch 60/100\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.9320 - acc: 0.5500\n",
      "Epoch 61/100\n",
      "40/40 [==============================] - 4s 104ms/step - loss: 0.9364 - acc: 0.5500\n",
      "Epoch 62/100\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 0.9185 - acc: 0.5500\n",
      "Epoch 63/100\n",
      "40/40 [==============================] - 4s 89ms/step - loss: 0.9265 - acc: 0.5500\n",
      "Epoch 64/100\n",
      "40/40 [==============================] - 4s 96ms/step - loss: 0.9309 - acc: 0.5500\n",
      "Epoch 65/100\n",
      "40/40 [==============================] - 4s 96ms/step - loss: 0.9095 - acc: 0.5500\n",
      "Epoch 66/100\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 0.9241 - acc: 0.5500\n",
      "Epoch 67/100\n",
      "40/40 [==============================] - 3s 84ms/step - loss: 0.9190 - acc: 0.5500\n",
      "Epoch 68/100\n",
      "40/40 [==============================] - 4s 92ms/step - loss: 0.9188 - acc: 0.5500\n",
      "Epoch 69/100\n",
      "40/40 [==============================] - 4s 95ms/step - loss: 0.9162 - acc: 0.5500\n",
      "Epoch 70/100\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 0.9076 - acc: 0.5500\n",
      "Epoch 71/100\n",
      "40/40 [==============================] - 4s 89ms/step - loss: 0.9215 - acc: 0.5500\n",
      "Epoch 72/100\n",
      "40/40 [==============================] - 4s 93ms/step - loss: 0.9085 - acc: 0.5500\n",
      "Epoch 73/100\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 0.8933 - acc: 0.5500\n",
      "Epoch 74/100\n",
      "40/40 [==============================] - 4s 99ms/step - loss: 0.9128 - acc: 0.5750\n",
      "Epoch 75/100\n",
      "40/40 [==============================] - 3s 87ms/step - loss: 0.9034 - acc: 0.5500\n",
      "Epoch 76/100\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.8985 - acc: 0.5500\n",
      "Epoch 77/100\n",
      "40/40 [==============================] - 3s 85ms/step - loss: 0.9037 - acc: 0.5750\n",
      "Epoch 78/100\n",
      "40/40 [==============================] - 3s 87ms/step - loss: 0.9063 - acc: 0.5500\n",
      "Epoch 79/100\n",
      "40/40 [==============================] - 3s 87ms/step - loss: 0.8959 - acc: 0.5500\n",
      "Epoch 80/100\n",
      "40/40 [==============================] - 4s 89ms/step - loss: 0.8929 - acc: 0.5500\n",
      "Epoch 81/100\n",
      "40/40 [==============================] - 3s 86ms/step - loss: 0.8888 - acc: 0.5500\n",
      "Epoch 82/100\n",
      "40/40 [==============================] - 3s 87ms/step - loss: 0.8901 - acc: 0.5500\n",
      "Epoch 83/100\n",
      "40/40 [==============================] - 4s 99ms/step - loss: 0.8984 - acc: 0.5500\n",
      "Epoch 84/100\n",
      "40/40 [==============================] - 4s 106ms/step - loss: 0.8884 - acc: 0.5500\n",
      "Epoch 85/100\n",
      "40/40 [==============================] - 4s 111ms/step - loss: 0.8744 - acc: 0.5500\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 4s 100ms/step - loss: 0.8868 - acc: 0.5500\n",
      "Epoch 87/100\n",
      "40/40 [==============================] - 4s 98ms/step - loss: 0.8977 - acc: 0.5500\n",
      "Epoch 88/100\n",
      "40/40 [==============================] - 4s 101ms/step - loss: 0.8809 - acc: 0.5500\n",
      "Epoch 89/100\n",
      "40/40 [==============================] - 4s 110ms/step - loss: 0.8692 - acc: 0.5500\n",
      "Epoch 90/100\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.8723 - acc: 0.5500\n",
      "Epoch 91/100\n",
      "40/40 [==============================] - 4s 112ms/step - loss: 0.8619 - acc: 0.5750\n",
      "Epoch 92/100\n",
      "40/40 [==============================] - 4s 110ms/step - loss: 0.8569 - acc: 0.5750\n",
      "Epoch 93/100\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 0.8705 - acc: 0.5500\n",
      "Epoch 94/100\n",
      "40/40 [==============================] - 4s 93ms/step - loss: 0.8697 - acc: 0.5750\n",
      "Epoch 95/100\n",
      "40/40 [==============================] - 4s 92ms/step - loss: 0.8724 - acc: 0.5500\n",
      "Epoch 96/100\n",
      "40/40 [==============================] - 3s 85ms/step - loss: 0.8454 - acc: 0.6250\n",
      "Epoch 97/100\n",
      "40/40 [==============================] - 3s 85ms/step - loss: 0.8629 - acc: 0.5500\n",
      "Epoch 98/100\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.8564 - acc: 0.5750\n",
      "Epoch 99/100\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.8526 - acc: 0.6000\n",
      "Epoch 100/100\n",
      "40/40 [==============================] - 3s 85ms/step - loss: 0.8564 - acc: 0.5750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1aacc1b14a8>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = keras.optimizers.Adam(lr = 0.00001) # 0.0001, 0.0002, 0.00001\n",
    "lstmBiDirect_model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "lstmBiDirect_model.fit(X_train_indices, y_train_labels, batch_size=4, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "10/10 [==============================] - 1s 117ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0449336767196655, 0.4000000059604645]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstmBiDirect_model.evaluate(X_test_indices, y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds3 = lstmBiDirect_model.predict(X_test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  [[0.4511565  0.2950424  0.25380117]\n",
      " [0.42751592 0.2919193  0.28056479]\n",
      " [0.45007935 0.30925187 0.24066877]\n",
      " [0.5135472  0.30072927 0.18572363]\n",
      " [0.56941104 0.2521069  0.1784821 ]\n",
      " [0.56104904 0.2867718  0.15217914]\n",
      " [0.57387483 0.25228184 0.17384334]\n",
      " [0.3839347  0.31038865 0.3056767 ]\n",
      " [0.52486134 0.27523154 0.19990712]\n",
      " [0.5147425  0.28667885 0.19857861]]\n",
      "actual:  [[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print('predicted: ',preds3)\n",
    "print('actual: ',y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "      Expect inflation       0.40      1.00      0.57         4\n",
      "Inflation will go away       0.00      0.00      0.00         5\n",
      "               Neutral       0.00      0.00      0.00         1\n",
      "\n",
      "              accuracy                           0.40        10\n",
      "             macro avg       0.13      0.33      0.19        10\n",
      "          weighted avg       0.16      0.40      0.23        10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Expect inflation', 'Inflation will go away', 'Neutral']\n",
    "clsf_rep_6 = classification_report(np.argmax(y_test_labels,axis=1),np.argmax(preds3,axis=1), target_names=target_names)\n",
    "print(clsf_rep_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "the personal consumption expenditures index showed prices increased 55 percent last month as consumer spending pulled back  the federal reserves preferred inflation measure is showing signs of moderating after months of rapid price increases and a closely watched gauge of consumer spending slowed last month a sign that the economy may have less steam as it heads into 2023  the personal consumption expenditures price index climbed 55 percent in november from a year earlier a slowdown from 61 percent in the previous reading stripped of food and fuel costs which jump around a socalled core price measure climbed 47 percent down from 5 percent in the previous reading both figures were roughly in line with economist forecasts  although inflation is slowing it still has a long way to go to return to a more normal pace the fed has raised interest rates at the fastest clip in decades this year as it has tried to temper consumer and business demand hoping to force price increases to moderate those rate increases are now trickling through the economy slowing the housing market cooling demand for new business investments and potentially weakening the labor market  but it remains to be seen just how much the feds policy changes will slow down the overall economy so far spending and hiring have both been relatively resilient  which has left policymakers and economists alike closely watching each new data report like the one released friday for any hint at how consumers are faring  reducing inflation is likely to require a sustained period of belowtrend growth and some softening of labor market conditions jerome h powell the fed chair said at his final news conference of the year  the economic figures on friday showed that consumer spending slowed in november climbing just 01 percent from october less than the 02 percent economists had forecast but spending in october was revised up slightly and posted a robust 09 percent increase  evidence that it is still hard to get a handle on the trajectory for consumption  those figures do not account for inflation adjusted for price increases spending did not grow at all  and under the surface the data pointed to a continued shift in what americans are buying spending on services continues to grow even as people buy fewer goods like furniture and clothing  even if they are not yet conclusive signs of cooling consumer demand are likely to be welcome news for officials at the fed the economy slowed notably in 2022 from its rapid expansion in 2021 but policymakers at the fed believe that it needs to remain weaker than usual through next year to get inflation back down to the 2 percent that they shoot for on average over time  thats because rapid inflation  which began as pandemicinduced supply shortages collided with strong consumer demand  has become more stubborn over time it now spans a variety of service categories from dentist visits to meals out at restaurants those sorts of price gains tend to be fueled by increasing wages and can take time to stamp out  the lowhanging fruit is working out energy components and supply chain issues are coming off priya misra head of global rates strategy at td securities said of the recent cooling in inflation but services inflation she said is likely to be a more intractable problem  they still talk about inflation as public enemy no 1 but the narrative around inflation has moved to wages and the labor market ms misra said  the fed is hoping that weighing down the broader economy will help to bring demand for workers back into balance with the supply of available employees as conditions moderate policymakers think pay gains will slow and inflation will be able to return fully to normal paving the way for more sustainable growth  looking ahead we expect a deceleration in household spending as the fed hikes rates further in 2023 rubeela farooqi chief us economist at high frequency economics wrote in response to the data released on friday  but nailing that landing is sure to be difficult officials will have to guess just how high interest rates need to go and how long they need to stay there  to slow the economy and price increases sufficiently that is an inexact science and there is a risk that officials will cause a painful recession as they try to slow down the economy  as a result fed officials this month began to move rates at a more gradual pace and have hinted that they couldstop raising them altogether at some point in 2023 that will give them time to see how their policy changes so far are playing through the economy  its now not so important how fast we go its far more important to think what is the ultimate level mr powell said at his latest news conference and then its  at a certain point the question will become how long do we remain restrictive  among other challenges it is tough to guess how american consumers who drive about 70 percent of the economy will behave next year they are eking out income gains even counting for inflation in recent months and they are still sitting on savings amassed during the pandemic that could help to keep them shopping into 2023  but at the same time many families have been drawing down their extra funds and the nations savings rate how much people tuck away out of their disposable income  has dipped to low levels it is unclear how long people will be willing to spend out of their nest eggs before they begin to meaningfully pull back  the white house for its part has been welcoming any sign that the economy remains resilient at price increases fade  there will be more ups and downs in the year ahead but we are making progress building an economy from the bottom up and the middle out president biden said in a release after the report on friday im optimistic for theyear ahead  but as the feds policy changes play out many economists expect that the economy will lose momentum and eventually contract next year  analysts at capital economics continue to expect a mild recession next year andrew hunter the firms senior us economist wrote in a research note on friday he noted that consumption was pulling back and that business investment was likely to weaken more markedly next year as the full impact of the feds aggressive tightening this year feeds through\n",
      "Inflation will fade away\n",
      "1\n",
      "acual:  [0. 1. 0.]\n",
      "predicted:  [0.57387483 0.25228184 0.17384334]\n"
     ]
    }
   ],
   "source": [
    "n = np.random.randint(0,10)\n",
    "print(n)\n",
    "print(X_test[n])\n",
    "print(y_test[n])\n",
    "print(y_test_encoded[n])\n",
    "print('acual: ',y_test_labels[n])\n",
    "print('predicted: ',preds3[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the Bidirectional GRU model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 150, 100)          469300    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               175872    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 678,455\n",
      "Trainable params: 678,455\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gruBiDirect_model = biDirect_gruArch((maxLen,))\n",
    "gruBiDirect_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "40/40 [==============================] - 6s 162ms/step - loss: 1.0084 - acc: 0.5500\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 2s 60ms/step - loss: 1.0037 - acc: 0.5500\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 2s 54ms/step - loss: 1.0113 - acc: 0.5500\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 3s 81ms/step - loss: 0.9979 - acc: 0.5500\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.9907 - acc: 0.5500\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 2s 60ms/step - loss: 0.9941 - acc: 0.5500\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 2s 54ms/step - loss: 0.9917 - acc: 0.5500\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 2s 54ms/step - loss: 0.9929 - acc: 0.5500\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 0.9764 - acc: 0.5500\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 1.0083 - acc: 0.5500\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 3s 74ms/step - loss: 0.9753 - acc: 0.5500\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.0115 - acc: 0.5500\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.9762 - acc: 0.5500\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.9782 - acc: 0.5500\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.9820 - acc: 0.5500\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 3s 72ms/step - loss: 0.9714 - acc: 0.5500\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.9832 - acc: 0.5500\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.9773 - acc: 0.5500\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 3s 72ms/step - loss: 0.9903 - acc: 0.5500\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 3s 74ms/step - loss: 0.9580 - acc: 0.5500\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 3s 77ms/step - loss: 0.9444 - acc: 0.5500\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 3s 74ms/step - loss: 0.9705 - acc: 0.5500\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 3s 71ms/step - loss: 0.9737 - acc: 0.5500\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 3s 86ms/step - loss: 0.9513 - acc: 0.5500\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 3s 78ms/step - loss: 0.9513 - acc: 0.5500\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 4s 96ms/step - loss: 0.9537 - acc: 0.5500\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 4s 109ms/step - loss: 0.9491 - acc: 0.5500\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 4s 99ms/step - loss: 0.9383 - acc: 0.5500\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 4s 99ms/step - loss: 0.9615 - acc: 0.5500\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 0.9519 - acc: 0.5500\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.9343 - acc: 0.5500\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - 3s 83ms/step - loss: 0.9445 - acc: 0.5500\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - 4s 95ms/step - loss: 0.9417 - acc: 0.5500\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - 3s 81ms/step - loss: 0.9156 - acc: 0.5500\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 0.9396 - acc: 0.5500\n",
      "Epoch 36/100\n",
      "40/40 [==============================] - 3s 81ms/step - loss: 0.9108 - acc: 0.5500\n",
      "Epoch 37/100\n",
      "40/40 [==============================] - 3s 79ms/step - loss: 0.9347 - acc: 0.5500\n",
      "Epoch 38/100\n",
      "40/40 [==============================] - 3s 77ms/step - loss: 0.9128 - acc: 0.5500\n",
      "Epoch 39/100\n",
      "40/40 [==============================] - 3s 75ms/step - loss: 0.9024 - acc: 0.5500\n",
      "Epoch 40/100\n",
      "40/40 [==============================] - 3s 78ms/step - loss: 0.9088 - acc: 0.5500\n",
      "Epoch 41/100\n",
      "40/40 [==============================] - 3s 80ms/step - loss: 0.9187 - acc: 0.5500\n",
      "Epoch 42/100\n",
      "40/40 [==============================] - 3s 78ms/step - loss: 0.9194 - acc: 0.5500\n",
      "Epoch 43/100\n",
      "40/40 [==============================] - 3s 77ms/step - loss: 0.9214 - acc: 0.5500\n",
      "Epoch 44/100\n",
      "40/40 [==============================] - 3s 76ms/step - loss: 0.9182 - acc: 0.5500\n",
      "Epoch 45/100\n",
      "40/40 [==============================] - 3s 78ms/step - loss: 0.9007 - acc: 0.5500\n",
      "Epoch 46/100\n",
      "40/40 [==============================] - 3s 84ms/step - loss: 0.9123 - acc: 0.5500\n",
      "Epoch 47/100\n",
      "40/40 [==============================] - 3s 82ms/step - loss: 0.9005 - acc: 0.5500\n",
      "Epoch 48/100\n",
      "40/40 [==============================] - 3s 78ms/step - loss: 0.9071 - acc: 0.5500\n",
      "Epoch 49/100\n",
      "40/40 [==============================] - 3s 75ms/step - loss: 0.8973 - acc: 0.5500\n",
      "Epoch 50/100\n",
      "40/40 [==============================] - 3s 79ms/step - loss: 0.8895 - acc: 0.5500\n",
      "Epoch 51/100\n",
      "40/40 [==============================] - 3s 77ms/step - loss: 0.8864 - acc: 0.5500\n",
      "Epoch 52/100\n",
      "40/40 [==============================] - 3s 73ms/step - loss: 0.8967 - acc: 0.5500\n",
      "Epoch 53/100\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.8928 - acc: 0.5500\n",
      "Epoch 54/100\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 0.8779 - acc: 0.5500\n",
      "Epoch 55/100\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.8622 - acc: 0.5500\n",
      "Epoch 56/100\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 0.8917 - acc: 0.5500\n",
      "Epoch 57/100\n",
      "40/40 [==============================] - 3s 63ms/step - loss: 0.8713 - acc: 0.5500\n",
      "Epoch 58/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.8694 - acc: 0.5500\n",
      "Epoch 59/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.8764 - acc: 0.5500\n",
      "Epoch 60/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.8722 - acc: 0.5500\n",
      "Epoch 61/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.8689 - acc: 0.5500\n",
      "Epoch 62/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.8739 - acc: 0.5500\n",
      "Epoch 63/100\n",
      "40/40 [==============================] - 3s 75ms/step - loss: 0.8642 - acc: 0.5500\n",
      "Epoch 64/100\n",
      "40/40 [==============================] - 3s 86ms/step - loss: 0.8598 - acc: 0.5500\n",
      "Epoch 65/100\n",
      "40/40 [==============================] - 3s 82ms/step - loss: 0.8654 - acc: 0.5500\n",
      "Epoch 66/100\n",
      "40/40 [==============================] - 3s 79ms/step - loss: 0.8636 - acc: 0.5500\n",
      "Epoch 67/100\n",
      "40/40 [==============================] - 3s 79ms/step - loss: 0.8557 - acc: 0.5750\n",
      "Epoch 68/100\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 0.8336 - acc: 0.5500\n",
      "Epoch 69/100\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.8542 - acc: 0.5500\n",
      "Epoch 70/100\n",
      "40/40 [==============================] - 3s 73ms/step - loss: 0.8457 - acc: 0.5500\n",
      "Epoch 71/100\n",
      "40/40 [==============================] - 3s 87ms/step - loss: 0.8608 - acc: 0.5750\n",
      "Epoch 72/100\n",
      "40/40 [==============================] - 3s 75ms/step - loss: 0.8464 - acc: 0.5500\n",
      "Epoch 73/100\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.8365 - acc: 0.5500\n",
      "Epoch 74/100\n",
      "40/40 [==============================] - 3s 76ms/step - loss: 0.8215 - acc: 0.5500\n",
      "Epoch 75/100\n",
      "40/40 [==============================] - 5s 119ms/step - loss: 0.8641 - acc: 0.5500\n",
      "Epoch 76/100\n",
      "40/40 [==============================] - 3s 86ms/step - loss: 0.8134 - acc: 0.5500\n",
      "Epoch 77/100\n",
      "40/40 [==============================] - 3s 77ms/step - loss: 0.8475 - acc: 0.5500\n",
      "Epoch 78/100\n",
      "40/40 [==============================] - 3s 78ms/step - loss: 0.8288 - acc: 0.5750\n",
      "Epoch 79/100\n",
      "40/40 [==============================] - 4s 96ms/step - loss: 0.8374 - acc: 0.5500\n",
      "Epoch 80/100\n",
      "40/40 [==============================] - 4s 94ms/step - loss: 0.8484 - acc: 0.5500\n",
      "Epoch 81/100\n",
      "40/40 [==============================] - 4s 93ms/step - loss: 0.8266 - acc: 0.5750\n",
      "Epoch 82/100\n",
      "40/40 [==============================] - 3s 82ms/step - loss: 0.8193 - acc: 0.6000\n",
      "Epoch 83/100\n",
      "40/40 [==============================] - 4s 103ms/step - loss: 0.8252 - acc: 0.5500\n",
      "Epoch 84/100\n",
      "40/40 [==============================] - 3s 77ms/step - loss: 0.8149 - acc: 0.5500\n",
      "Epoch 85/100\n",
      "40/40 [==============================] - 3s 73ms/step - loss: 0.8161 - acc: 0.5500\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 4s 110ms/step - loss: 0.8154 - acc: 0.5750\n",
      "Epoch 87/100\n",
      "40/40 [==============================] - 4s 102ms/step - loss: 0.8181 - acc: 0.5500\n",
      "Epoch 88/100\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 0.8199 - acc: 0.5500\n",
      "Epoch 89/100\n",
      "40/40 [==============================] - 4s 100ms/step - loss: 0.8185 - acc: 0.5500\n",
      "Epoch 90/100\n",
      "40/40 [==============================] - 4s 98ms/step - loss: 0.8110 - acc: 0.5750\n",
      "Epoch 91/100\n",
      "40/40 [==============================] - 3s 75ms/step - loss: 0.8099 - acc: 0.5500\n",
      "Epoch 92/100\n",
      "40/40 [==============================] - 3s 78ms/step - loss: 0.7933 - acc: 0.5750\n",
      "Epoch 93/100\n",
      "40/40 [==============================] - 3s 82ms/step - loss: 0.8084 - acc: 0.5500\n",
      "Epoch 94/100\n",
      "40/40 [==============================] - 3s 71ms/step - loss: 0.8178 - acc: 0.5500\n",
      "Epoch 95/100\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 0.7685 - acc: 0.5500\n",
      "Epoch 96/100\n",
      "40/40 [==============================] - 3s 82ms/step - loss: 0.8089 - acc: 0.5750\n",
      "Epoch 97/100\n",
      "40/40 [==============================] - 3s 87ms/step - loss: 0.7872 - acc: 0.5500\n",
      "Epoch 98/100\n",
      "40/40 [==============================] - 3s 78ms/step - loss: 0.7819 - acc: 0.5500\n",
      "Epoch 99/100\n",
      "40/40 [==============================] - 3s 76ms/step - loss: 0.7733 - acc: 0.6000\n",
      "Epoch 100/100\n",
      "40/40 [==============================] - 3s 77ms/step - loss: 0.7772 - acc: 0.5750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1abdfed5160>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = keras.optimizers.Adam(lr = 0.00001) # 0.0001, 0.0002, 0.00001\n",
    "gruBiDirect_model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "gruBiDirect_model.fit(X_train_indices, y_train_labels, batch_size=4, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "10/10 [==============================] - 1s 114ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0506579875946045, 0.4000000059604645]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gruBiDirect_model.evaluate(X_test_indices, y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds4 = lstmBiDirect_model.predict(X_test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  [[0.4520835  0.29458362 0.25333294]\n",
      " [0.42846352 0.2913427  0.28019378]\n",
      " [0.45104796 0.30877382 0.24017815]\n",
      " [0.5135615  0.3007609  0.18567765]\n",
      " [0.5695871  0.25201064 0.17840219]\n",
      " [0.5610565  0.2867719  0.15217155]\n",
      " [0.5739395  0.2521159  0.17394456]\n",
      " [0.3847256  0.31008738 0.30518705]\n",
      " [0.52506983 0.27520606 0.19972406]\n",
      " [0.51486677 0.2865788  0.19855438]]\n",
      "actual:  [[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print('predicted: ',preds4)\n",
    "print('actual: ',y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "      Expect inflation       0.40      1.00      0.57         4\n",
      "Inflation will go away       0.00      0.00      0.00         5\n",
      "               Neutral       0.00      0.00      0.00         1\n",
      "\n",
      "              accuracy                           0.40        10\n",
      "             macro avg       0.13      0.33      0.19        10\n",
      "          weighted avg       0.16      0.40      0.23        10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Expect inflation', 'Inflation will go away', 'Neutral']\n",
    "clsf_rep_7 = classification_report(np.argmax(y_test_labels,axis=1),np.argmax(preds4,axis=1), target_names=target_names)\n",
    "print(clsf_rep_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "economic data on friday brought troubling news for federal reserve officials who are trying to rein in the fastest inflation in decades prices are still rising quickly wages are rising rapidly too and the strong consumer demand that is helping to fuel the inflationary fire shows little sign of letting up  the data from two separate government reports wasnt a surprise and included hints of progress but it was confirmation of the challenges facing policymakers and further evidence that their aggressive efforts to constrain the economy are taking time to have a significant effect  the feds preferred measure of inflation the personal consumption expenditures price index climbed 62 percent in the year through september in line with the increase the month before the commerce department said friday after stripping out food and fuel which can be volatile from month to month prices increased 51 percent over the past year a brisker increase than the 49 percent in the year through august  both of those inflation measures are rising faster than the 2 percent rate that the fed targets on average and over time  as central bankers try to predict when inflation will slow they are watching for any sign that the labor market is loosening and that rapid wage growth is moderating it would be difficult for inflation to decelerate with pay climbing at the pace it has recently companies facing heftier labor bills generally try to pass at least some of those cost increases onto consumers  the employment cost index a quarterly inflation measure from the labor department that tracks changes in wages and benefits climbed 12 percent from june to september matching what economists in a bloomberg survey had expected  that index picked up 50 percent on a yearly basis down slightly from 51 percent in the previous report in the decade leading up to the pandemic that figure averaged 22 percent yearly gains underscoring how rapid todays rate is  a measure that tracks wages and salaries in the private sector showed a more pronounced slowdown which some economists greeted as a hopeful sign even so the upshot was that pay growth while showing some hints of moderation under the surface remains unusually rapid  the level of wage growth is still very high even if it is moving in the right direction said laura rosnerwarburtona senior economist at macropolicy perspectives its probably putting upward pressure on services inflation  the fed meets next week and it is almost uniformly expected to announce an increase in interest rates of three quarters of a percentage point wednesday officials have previously indicated in economic forecasts that they expected to slow to a halfpoint rate increase in december and investors will closely watch fed chair jerome powells postmeeting news conference for any sign that such a step down is imminent  officials have been clear that they will at some point stop raising rates but that they then plan to leave them at a high level for some time the idea is that the high rates would continue to weigh on economic growth slowing the economy and reining in inflation while hopefully avoiding a severe recession  fridays figures are likely to reaffirm to officials that even as rate increases weigh on the housing market and stoke fears of a recession it will take more time to dampen the labor market\n",
      "Expect Inflation\n",
      "0\n",
      "acual:  [1. 0. 0.]\n",
      "predicted:  [0.5135615  0.3007609  0.18567765]\n"
     ]
    }
   ],
   "source": [
    "n = np.random.randint(0,10)\n",
    "print(n)\n",
    "print(X_test[n])\n",
    "print(y_test[n])\n",
    "print(y_test_encoded[n])\n",
    "print('acual: ',y_test_labels[n])\n",
    "print('predicted: ',preds4[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Revisiting GRU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting max len to 300\n",
    "maxLenG2 = 300 ## 150 or 300\n",
    "\n",
    "embedding_layer = Embedding(input_dim=vocab_len, output_dim=embed_vector_len, \n",
    "                            input_length=maxLenG2, weights = [emb_matrix], trainable=True)\n",
    "\n",
    "\n",
    "X_train_indicesG2 = tokenizer.texts_to_sequences(X_train_prepros)\n",
    "X_train_indicesG2 = pad_sequences(X_train_indices, maxlen=maxLenG2, padding='post')\n",
    "\n",
    "X_test_indicesG2 = tokenizer.texts_to_sequences(X_test_prepros)\n",
    "X_test_indicesG2 = pad_sequences(X_test_indices, maxlen=maxLenG2, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 300, 100)          469300    \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 128)               87936     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 574,135\n",
      "Trainable params: 574,135\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gruGloVe_modelG2 = gruArchitecture((maxLenG2,))\n",
    "gruGloVe_modelG2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "40/40 [==============================] - 9s 224ms/step - loss: 1.0986 - acc: 0.5500\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 6s 144ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 6s 140ms/step - loss: 1.0982 - acc: 0.5500\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 5s 129ms/step - loss: 1.0980 - acc: 0.5500\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 5s 135ms/step - loss: 1.0978 - acc: 0.5500\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 5s 127ms/step - loss: 1.0976 - acc: 0.5500\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 5s 128ms/step - loss: 1.0974 - acc: 0.5500\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 5s 130ms/step - loss: 1.0972 - acc: 0.5500\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 5s 130ms/step - loss: 1.0970 - acc: 0.5500\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 5s 137ms/step - loss: 1.0967 - acc: 0.5500\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 5s 121ms/step - loss: 1.0965 - acc: 0.5500\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 5s 131ms/step - loss: 1.0962 - acc: 0.5500\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 6s 154ms/step - loss: 1.0960 - acc: 0.5500\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 7s 169ms/step - loss: 1.0958 - acc: 0.5500\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 6s 151ms/step - loss: 1.0955 - acc: 0.5500\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 9s 214ms/step - loss: 1.0952 - acc: 0.5500\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 4s 103ms/step - loss: 1.0951 - acc: 0.5500\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 5s 125ms/step - loss: 1.0946 - acc: 0.5500\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 5s 128ms/step - loss: 1.0944 - acc: 0.5500\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 5s 132ms/step - loss: 1.0942 - acc: 0.5500\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 5s 136ms/step - loss: 1.0941 - acc: 0.5500\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 6s 139ms/step - loss: 1.0937 - acc: 0.5500\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 5s 133ms/step - loss: 1.0934 - acc: 0.5500\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 5s 131ms/step - loss: 1.0929 - acc: 0.5500\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 6s 138ms/step - loss: 1.0928 - acc: 0.5500\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 5s 131ms/step - loss: 1.0926 - acc: 0.5500\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 5s 128ms/step - loss: 1.0923 - acc: 0.5500\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 7s 179ms/step - loss: 1.0918 - acc: 0.5500\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 1.0914 - acc: 0.5500\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - 9s 229ms/step - loss: 1.0914 - acc: 0.5500\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - 5s 129ms/step - loss: 1.0911 - acc: 0.5500\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - 7s 166ms/step - loss: 1.0907 - acc: 0.5500\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - 6s 152ms/step - loss: 1.0905 - acc: 0.5500\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 1.0903 - acc: 0.5500\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - 8s 202ms/step - loss: 1.0897 - acc: 0.5500\n",
      "Epoch 36/100\n",
      "40/40 [==============================] - 6s 154ms/step - loss: 1.0896 - acc: 0.5500\n",
      "Epoch 37/100\n",
      "40/40 [==============================] - 7s 165ms/step - loss: 1.0891 - acc: 0.5500\n",
      "Epoch 38/100\n",
      "40/40 [==============================] - 8s 188ms/step - loss: 1.0887 - acc: 0.5500\n",
      "Epoch 39/100\n",
      "40/40 [==============================] - 4s 99ms/step - loss: 1.0884 - acc: 0.5500\n",
      "Epoch 40/100\n",
      "40/40 [==============================] - 5s 130ms/step - loss: 1.0880 - acc: 0.5500\n",
      "Epoch 41/100\n",
      "40/40 [==============================] - 5s 137ms/step - loss: 1.0878 - acc: 0.5500\n",
      "Epoch 42/100\n",
      "40/40 [==============================] - 6s 138ms/step - loss: 1.0870 - acc: 0.5500\n",
      "Epoch 43/100\n",
      "40/40 [==============================] - 6s 149ms/step - loss: 1.0869 - acc: 0.5500\n",
      "Epoch 44/100\n",
      "40/40 [==============================] - 6s 141ms/step - loss: 1.0865 - acc: 0.5500\n",
      "Epoch 45/100\n",
      "40/40 [==============================] - 6s 141ms/step - loss: 1.0858 - acc: 0.5500\n",
      "Epoch 46/100\n",
      "40/40 [==============================] - 7s 165ms/step - loss: 1.0857 - acc: 0.5500\n",
      "Epoch 47/100\n",
      "40/40 [==============================] - 6s 142ms/step - loss: 1.0849 - acc: 0.5500\n",
      "Epoch 48/100\n",
      "40/40 [==============================] - 5s 132ms/step - loss: 1.0850 - acc: 0.5500\n",
      "Epoch 49/100\n",
      "40/40 [==============================] - 5s 118ms/step - loss: 1.0845 - acc: 0.5500\n",
      "Epoch 50/100\n",
      "40/40 [==============================] - 4s 112ms/step - loss: 1.0841 - acc: 0.5500\n",
      "Epoch 51/100\n",
      "40/40 [==============================] - 5s 118ms/step - loss: 1.0839 - acc: 0.5500\n",
      "Epoch 52/100\n",
      "40/40 [==============================] - 4s 112ms/step - loss: 1.0834 - acc: 0.5500\n",
      "Epoch 53/100\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 1.0831 - acc: 0.5500\n",
      "Epoch 54/100\n",
      "40/40 [==============================] - 5s 115ms/step - loss: 1.0824 - acc: 0.5500\n",
      "Epoch 55/100\n",
      "40/40 [==============================] - 5s 118ms/step - loss: 1.0817 - acc: 0.5500\n",
      "Epoch 56/100\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 1.0811 - acc: 0.5500\n",
      "Epoch 57/100\n",
      "40/40 [==============================] - 4s 107ms/step - loss: 1.0809 - acc: 0.5500\n",
      "Epoch 58/100\n",
      "40/40 [==============================] - 5s 125ms/step - loss: 1.0799 - acc: 0.5500\n",
      "Epoch 59/100\n",
      "40/40 [==============================] - 5s 124ms/step - loss: 1.0803 - acc: 0.5500\n",
      "Epoch 60/100\n",
      "40/40 [==============================] - 5s 115ms/step - loss: 1.0787 - acc: 0.5500\n",
      "Epoch 61/100\n",
      "40/40 [==============================] - 5s 116ms/step - loss: 1.0789 - acc: 0.5500\n",
      "Epoch 62/100\n",
      "40/40 [==============================] - 5s 114ms/step - loss: 1.0788 - acc: 0.5500\n",
      "Epoch 63/100\n",
      "40/40 [==============================] - 4s 108ms/step - loss: 1.0768 - acc: 0.5500\n",
      "Epoch 64/100\n",
      "40/40 [==============================] - 5s 114ms/step - loss: 1.0777 - acc: 0.5500\n",
      "Epoch 65/100\n",
      "40/40 [==============================] - 5s 126ms/step - loss: 1.0771 - acc: 0.5500\n",
      "Epoch 66/100\n",
      "40/40 [==============================] - 5s 128ms/step - loss: 1.0774 - acc: 0.5500\n",
      "Epoch 67/100\n",
      "40/40 [==============================] - 5s 127ms/step - loss: 1.0759 - acc: 0.5500\n",
      "Epoch 68/100\n",
      "40/40 [==============================] - 5s 131ms/step - loss: 1.0744 - acc: 0.5500\n",
      "Epoch 69/100\n",
      "40/40 [==============================] - 5s 130ms/step - loss: 1.0741 - acc: 0.5500\n",
      "Epoch 70/100\n",
      "40/40 [==============================] - 5s 121ms/step - loss: 1.0738 - acc: 0.5500\n",
      "Epoch 71/100\n",
      "40/40 [==============================] - 5s 123ms/step - loss: 1.0738 - acc: 0.5500\n",
      "Epoch 72/100\n",
      "40/40 [==============================] - 5s 128ms/step - loss: 1.0731 - acc: 0.5500\n",
      "Epoch 73/100\n",
      "40/40 [==============================] - 5s 114ms/step - loss: 1.0723 - acc: 0.5500\n",
      "Epoch 74/100\n",
      "40/40 [==============================] - 5s 116ms/step - loss: 1.0723 - acc: 0.5500\n",
      "Epoch 75/100\n",
      "40/40 [==============================] - 5s 118ms/step - loss: 1.0727 - acc: 0.5500\n",
      "Epoch 76/100\n",
      "40/40 [==============================] - 4s 111ms/step - loss: 1.0708 - acc: 0.5500\n",
      "Epoch 77/100\n",
      "40/40 [==============================] - 4s 112ms/step - loss: 1.0705 - acc: 0.5500\n",
      "Epoch 78/100\n",
      "40/40 [==============================] - 5s 120ms/step - loss: 1.0688 - acc: 0.5500\n",
      "Epoch 79/100\n",
      "40/40 [==============================] - 5s 114ms/step - loss: 1.0695 - acc: 0.5500\n",
      "Epoch 80/100\n",
      "40/40 [==============================] - 4s 112ms/step - loss: 1.0664 - acc: 0.5500\n",
      "Epoch 81/100\n",
      "40/40 [==============================] - 5s 117ms/step - loss: 1.0666 - acc: 0.5500\n",
      "Epoch 82/100\n",
      "40/40 [==============================] - 5s 118ms/step - loss: 1.0648 - acc: 0.5500\n",
      "Epoch 83/100\n",
      "40/40 [==============================] - 4s 109ms/step - loss: 1.0659 - acc: 0.5500\n",
      "Epoch 84/100\n",
      "40/40 [==============================] - 4s 111ms/step - loss: 1.0655 - acc: 0.5500\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 5s 123ms/step - loss: 1.0666 - acc: 0.5500\n",
      "Epoch 86/100\n",
      "40/40 [==============================] - 5s 115ms/step - loss: 1.0636 - acc: 0.5500\n",
      "Epoch 87/100\n",
      "40/40 [==============================] - 4s 110ms/step - loss: 1.0626 - acc: 0.5500\n",
      "Epoch 88/100\n",
      "40/40 [==============================] - 5s 119ms/step - loss: 1.0609 - acc: 0.5500\n",
      "Epoch 89/100\n",
      "40/40 [==============================] - 5s 122ms/step - loss: 1.0614 - acc: 0.5500\n",
      "Epoch 90/100\n",
      "40/40 [==============================] - 5s 116ms/step - loss: 1.0624 - acc: 0.5500\n",
      "Epoch 91/100\n",
      "40/40 [==============================] - 5s 130ms/step - loss: 1.0589 - acc: 0.5500\n",
      "Epoch 92/100\n",
      "40/40 [==============================] - 5s 135ms/step - loss: 1.0589 - acc: 0.5500\n",
      "Epoch 93/100\n",
      "40/40 [==============================] - 5s 134ms/step - loss: 1.0587 - acc: 0.5500\n",
      "Epoch 94/100\n",
      "40/40 [==============================] - 5s 126ms/step - loss: 1.0580 - acc: 0.5500\n",
      "Epoch 95/100\n",
      "40/40 [==============================] - 5s 130ms/step - loss: 1.0561 - acc: 0.5500\n",
      "Epoch 96/100\n",
      "40/40 [==============================] - 5s 127ms/step - loss: 1.0565 - acc: 0.5500\n",
      "Epoch 97/100\n",
      "40/40 [==============================] - 4s 112ms/step - loss: 1.0549 - acc: 0.5500\n",
      "Epoch 98/100\n",
      "40/40 [==============================] - 5s 120ms/step - loss: 1.0544 - acc: 0.5500\n",
      "Epoch 99/100\n",
      "40/40 [==============================] - 5s 114ms/step - loss: 1.0545 - acc: 0.5500\n",
      "Epoch 100/100\n",
      "40/40 [==============================] - 4s 111ms/step - loss: 1.0539 - acc: 0.5500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1abe73d0978>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting with smallest lr from LSTM. If in doubt I will test 0.00005\n",
    "adamG2 = keras.optimizers.Adam(lr = 0.00001) # 0.0001, 0.0002, 0.00001\n",
    "gruGloVe_modelG2.compile(optimizer=adamG2, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "gruGloVe_modelG2.fit(X_train_indicesG2, y_train_labels, batch_size=4, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "10/10 [==============================] - 1s 81ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0799920558929443, 0.4000000059604645]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gruGloVe_modelG2.evaluate(X_test_indicesG2, y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  [[0.3881607  0.31308183 0.29875743]\n",
      " [0.3881607  0.31308183 0.29875743]\n",
      " [0.3881607  0.31308183 0.29875743]\n",
      " [0.3881607  0.31308183 0.29875743]\n",
      " [0.3881607  0.31308183 0.29875743]\n",
      " [0.3881607  0.31308183 0.29875743]\n",
      " [0.3881607  0.31308183 0.29875743]\n",
      " [0.3881607  0.31308183 0.29875743]\n",
      " [0.3881607  0.31308183 0.29875743]\n",
      " [0.3881607  0.31308183 0.29875743]]\n",
      "actual:  [[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "predsG2 = gruGloVe_modelG2.predict(X_test_indicesG2)\n",
    "print('predicted: ',predsG2)\n",
    "print('actual: ',y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "      Expect inflation       0.40      1.00      0.57         4\n",
      "Inflation will go away       0.00      0.00      0.00         5\n",
      "               Neutral       0.00      0.00      0.00         1\n",
      "\n",
      "              accuracy                           0.40        10\n",
      "             macro avg       0.13      0.33      0.19        10\n",
      "          weighted avg       0.16      0.40      0.23        10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Expect inflation', 'Inflation will go away', 'Neutral']\n",
    "clsf_rep_8 = classification_report(np.argmax(y_test_labels,axis=1),np.argmax(predsG2,axis=1), target_names=target_names)\n",
    "print(clsf_rep_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another try with new parameters GRU architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting max len to 300\n",
    "maxLenG3 = 150 ## 150 or 300\n",
    "\n",
    "embedding_layer = Embedding(input_dim=vocab_len, output_dim=embed_vector_len, \n",
    "                            input_length=maxLenG3, weights = [emb_matrix], trainable=True)\n",
    "\n",
    "\n",
    "X_train_indicesG3 = tokenizer.texts_to_sequences(X_train_prepros)\n",
    "X_train_indicesG3 = pad_sequences(X_train_indices, maxlen=maxLenG3, padding='post')\n",
    "\n",
    "X_test_indicesG3 = tokenizer.texts_to_sequences(X_test_prepros)\n",
    "X_test_indicesG3 = pad_sequences(X_test_indices, maxlen=maxLenG3, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 150, 100)          469300    \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (None, 128)               87936     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 574,135\n",
      "Trainable params: 574,135\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gruGloVe_modelG3 = gruArchitecture((maxLenG3,))\n",
    "gruGloVe_modelG3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "40/40 [==============================] - 5s 122ms/step - loss: 1.0786 - acc: 0.2750\n",
      "Epoch 2/150\n",
      "40/40 [==============================] - 3s 63ms/step - loss: 1.0871 - acc: 0.2750\n",
      "Epoch 3/150\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 1.0824 - acc: 0.2500\n",
      "Epoch 4/150\n",
      "40/40 [==============================] - 2s 60ms/step - loss: 1.0787 - acc: 0.3250\n",
      "Epoch 5/150\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 1.0734 - acc: 0.3500\n",
      "Epoch 6/150\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 1.0628 - acc: 0.3500\n",
      "Epoch 7/150\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 1.0639 - acc: 0.3750\n",
      "Epoch 8/150\n",
      "40/40 [==============================] - 3s 63ms/step - loss: 1.0571 - acc: 0.3250\n",
      "Epoch 9/150\n",
      "40/40 [==============================] - 3s 74ms/step - loss: 1.0765 - acc: 0.3000\n",
      "Epoch 10/150\n",
      "40/40 [==============================] - 3s 73ms/step - loss: 1.0821 - acc: 0.2750\n",
      "Epoch 11/150\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.0763 - acc: 0.3250\n",
      "Epoch 12/150\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 1.0560 - acc: 0.4000\n",
      "Epoch 13/150\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 1.0824 - acc: 0.3500\n",
      "Epoch 14/150\n",
      "40/40 [==============================] - 2s 59ms/step - loss: 1.0853 - acc: 0.2500\n",
      "Epoch 15/150\n",
      "40/40 [==============================] - 2s 59ms/step - loss: 1.0619 - acc: 0.3250\n",
      "Epoch 16/150\n",
      "40/40 [==============================] - 2s 60ms/step - loss: 1.0604 - acc: 0.3500\n",
      "Epoch 17/150\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 1.0896 - acc: 0.2500\n",
      "Epoch 18/150\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 1.0779 - acc: 0.2750\n",
      "Epoch 19/150\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 1.0645 - acc: 0.3000\n",
      "Epoch 20/150\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 1.0767 - acc: 0.2750\n",
      "Epoch 21/150\n",
      "40/40 [==============================] - 2s 60ms/step - loss: 1.0840 - acc: 0.3250\n",
      "Epoch 22/150\n",
      "40/40 [==============================] - 2s 60ms/step - loss: 1.0671 - acc: 0.4000\n",
      "Epoch 23/150\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 1.0701 - acc: 0.3500\n",
      "Epoch 24/150\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.0785 - acc: 0.3500\n",
      "Epoch 25/150\n",
      "40/40 [==============================] - 3s 72ms/step - loss: 1.0741 - acc: 0.3000\n",
      "Epoch 26/150\n",
      "40/40 [==============================] - 3s 71ms/step - loss: 1.0768 - acc: 0.3250\n",
      "Epoch 27/150\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 1.0756 - acc: 0.3750\n",
      "Epoch 28/150\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.0753 - acc: 0.3250\n",
      "Epoch 29/150\n",
      "40/40 [==============================] - 3s 72ms/step - loss: 1.0731 - acc: 0.3250\n",
      "Epoch 30/150\n",
      "40/40 [==============================] - 3s 76ms/step - loss: 1.0662 - acc: 0.3000\n",
      "Epoch 31/150\n",
      "40/40 [==============================] - 3s 72ms/step - loss: 1.0775 - acc: 0.3000\n",
      "Epoch 32/150\n",
      "40/40 [==============================] - 3s 71ms/step - loss: 1.0689 - acc: 0.3750\n",
      "Epoch 33/150\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 1.0644 - acc: 0.3750\n",
      "Epoch 34/150\n",
      "40/40 [==============================] - 3s 73ms/step - loss: 1.0715 - acc: 0.3500\n",
      "Epoch 35/150\n",
      "40/40 [==============================] - 3s 72ms/step - loss: 1.0799 - acc: 0.3500\n",
      "Epoch 36/150\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 1.0769 - acc: 0.3000\n",
      "Epoch 37/150\n",
      "40/40 [==============================] - 3s 71ms/step - loss: 1.0726 - acc: 0.3250\n",
      "Epoch 38/150\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.0591 - acc: 0.4250\n",
      "Epoch 39/150\n",
      "40/40 [==============================] - 2s 60ms/step - loss: 1.0615 - acc: 0.3750\n",
      "Epoch 40/150\n",
      "40/40 [==============================] - 3s 63ms/step - loss: 1.0705 - acc: 0.3250\n",
      "Epoch 41/150\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 1.0816 - acc: 0.3250\n",
      "Epoch 42/150\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 1.0659 - acc: 0.3750\n",
      "Epoch 43/150\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 1.0679 - acc: 0.4000\n",
      "Epoch 44/150\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 1.0604 - acc: 0.3250\n",
      "Epoch 45/150\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 1.0523 - acc: 0.4250\n",
      "Epoch 46/150\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 1.0567 - acc: 0.3750\n",
      "Epoch 47/150\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 1.0648 - acc: 0.3250\n",
      "Epoch 48/150\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 1.0690 - acc: 0.3750\n",
      "Epoch 49/150\n",
      "40/40 [==============================] - 3s 63ms/step - loss: 1.0461 - acc: 0.4000\n",
      "Epoch 50/150\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 1.0571 - acc: 0.3750\n",
      "Epoch 51/150\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 1.0651 - acc: 0.3250\n",
      "Epoch 52/150\n",
      "40/40 [==============================] - 2s 59ms/step - loss: 1.0524 - acc: 0.4250\n",
      "Epoch 53/150\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 1.0553 - acc: 0.4250\n",
      "Epoch 54/150\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 1.0582 - acc: 0.3500\n",
      "Epoch 55/150\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 1.0614 - acc: 0.3250\n",
      "Epoch 56/150\n",
      "40/40 [==============================] - 2s 59ms/step - loss: 1.0624 - acc: 0.2750\n",
      "Epoch 57/150\n",
      "40/40 [==============================] - 2s 60ms/step - loss: 1.0606 - acc: 0.3750\n",
      "Epoch 58/150\n",
      "40/40 [==============================] - 2s 60ms/step - loss: 1.0671 - acc: 0.3000\n",
      "Epoch 59/150\n",
      "40/40 [==============================] - 2s 62ms/step - loss: 1.0556 - acc: 0.3750\n",
      "Epoch 60/150\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 1.0622 - acc: 0.4000\n",
      "Epoch 61/150\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 1.0539 - acc: 0.4000\n",
      "Epoch 62/150\n",
      "40/40 [==============================] - 2s 62ms/step - loss: 1.0652 - acc: 0.3500\n",
      "Epoch 63/150\n",
      "40/40 [==============================] - 2s 59ms/step - loss: 1.0729 - acc: 0.3000\n",
      "Epoch 64/150\n",
      "40/40 [==============================] - 2s 60ms/step - loss: 1.0573 - acc: 0.3500\n",
      "Epoch 65/150\n",
      "40/40 [==============================] - 2s 60ms/step - loss: 1.0638 - acc: 0.3250\n",
      "Epoch 66/150\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 1.0567 - acc: 0.3750\n",
      "Epoch 67/150\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 1.0592 - acc: 0.4250\n",
      "Epoch 68/150\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 1.0565 - acc: 0.4000\n",
      "Epoch 69/150\n",
      "40/40 [==============================] - 3s 63ms/step - loss: 1.0538 - acc: 0.4000\n",
      "Epoch 70/150\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 1.0618 - acc: 0.3500\n",
      "Epoch 71/150\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.0576 - acc: 0.4000\n",
      "Epoch 72/150\n",
      "40/40 [==============================] - 3s 73ms/step - loss: 1.0606 - acc: 0.4000\n",
      "Epoch 73/150\n",
      "40/40 [==============================] - 3s 73ms/step - loss: 1.0565 - acc: 0.4250\n",
      "Epoch 74/150\n",
      "40/40 [==============================] - 3s 71ms/step - loss: 1.0582 - acc: 0.4250\n",
      "Epoch 75/150\n",
      "40/40 [==============================] - 3s 72ms/step - loss: 1.0571 - acc: 0.4250\n",
      "Epoch 76/150\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 1.0550 - acc: 0.4500\n",
      "Epoch 77/150\n",
      "40/40 [==============================] - 3s 72ms/step - loss: 1.0714 - acc: 0.3750\n",
      "Epoch 78/150\n",
      "40/40 [==============================] - 3s 76ms/step - loss: 1.0627 - acc: 0.3750\n",
      "Epoch 79/150\n",
      "40/40 [==============================] - 3s 72ms/step - loss: 1.0572 - acc: 0.4000\n",
      "Epoch 80/150\n",
      "40/40 [==============================] - 3s 73ms/step - loss: 1.0643 - acc: 0.3750\n",
      "Epoch 81/150\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 1.0521 - acc: 0.4000\n",
      "Epoch 82/150\n",
      "40/40 [==============================] - 2s 59ms/step - loss: 1.0633 - acc: 0.4500\n",
      "Epoch 83/150\n",
      "40/40 [==============================] - 3s 63ms/step - loss: 1.0468 - acc: 0.4250\n",
      "Epoch 84/150\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 1.0621 - acc: 0.3000\n",
      "Epoch 85/150\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 1.0597 - acc: 0.3500\n",
      "Epoch 86/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 2s 61ms/step - loss: 1.0498 - acc: 0.4750\n",
      "Epoch 87/150\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 1.0482 - acc: 0.4000\n",
      "Epoch 88/150\n",
      "40/40 [==============================] - 2s 59ms/step - loss: 1.0582 - acc: 0.3750\n",
      "Epoch 89/150\n",
      "40/40 [==============================] - 2s 60ms/step - loss: 1.0620 - acc: 0.3750\n",
      "Epoch 90/150\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 1.0629 - acc: 0.4250\n",
      "Epoch 91/150\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 1.0609 - acc: 0.3750\n",
      "Epoch 92/150\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 1.0532 - acc: 0.4250\n",
      "Epoch 93/150\n",
      "40/40 [==============================] - 2s 60ms/step - loss: 1.0527 - acc: 0.4000\n",
      "Epoch 94/150\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 1.0490 - acc: 0.4000\n",
      "Epoch 95/150\n",
      "40/40 [==============================] - 3s 77ms/step - loss: 1.0670 - acc: 0.3750\n",
      "Epoch 96/150\n",
      "40/40 [==============================] - 3s 75ms/step - loss: 1.0455 - acc: 0.5000\n",
      "Epoch 97/150\n",
      "40/40 [==============================] - 3s 75ms/step - loss: 1.0615 - acc: 0.4000\n",
      "Epoch 98/150\n",
      "40/40 [==============================] - 3s 74ms/step - loss: 1.0394 - acc: 0.4750\n",
      "Epoch 99/150\n",
      "40/40 [==============================] - 3s 77ms/step - loss: 1.0643 - acc: 0.3750\n",
      "Epoch 100/150\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.0547 - acc: 0.4500\n",
      "Epoch 101/150\n",
      "40/40 [==============================] - 3s 72ms/step - loss: 1.0534 - acc: 0.3500\n",
      "Epoch 102/150\n",
      "40/40 [==============================] - 3s 77ms/step - loss: 1.0522 - acc: 0.3750\n",
      "Epoch 103/150\n",
      "40/40 [==============================] - 3s 78ms/step - loss: 1.0604 - acc: 0.3250\n",
      "Epoch 104/150\n",
      "40/40 [==============================] - 3s 74ms/step - loss: 1.0392 - acc: 0.4500\n",
      "Epoch 105/150\n",
      "40/40 [==============================] - 3s 73ms/step - loss: 1.0443 - acc: 0.5250\n",
      "Epoch 106/150\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 1.0613 - acc: 0.4000\n",
      "Epoch 107/150\n",
      "40/40 [==============================] - 3s 74ms/step - loss: 1.0487 - acc: 0.4750\n",
      "Epoch 108/150\n",
      "40/40 [==============================] - 3s 73ms/step - loss: 1.0509 - acc: 0.4750\n",
      "Epoch 109/150\n",
      "40/40 [==============================] - 3s 73ms/step - loss: 1.0556 - acc: 0.4750\n",
      "Epoch 110/150\n",
      "40/40 [==============================] - 3s 74ms/step - loss: 1.0591 - acc: 0.4250\n",
      "Epoch 111/150\n",
      "40/40 [==============================] - 3s 72ms/step - loss: 1.0376 - acc: 0.4000\n",
      "Epoch 112/150\n",
      "40/40 [==============================] - 3s 77ms/step - loss: 1.0542 - acc: 0.3750\n",
      "Epoch 113/150\n",
      "40/40 [==============================] - 3s 78ms/step - loss: 1.0491 - acc: 0.4250\n",
      "Epoch 114/150\n",
      "40/40 [==============================] - 3s 81ms/step - loss: 1.0560 - acc: 0.4500\n",
      "Epoch 115/150\n",
      "40/40 [==============================] - 3s 85ms/step - loss: 1.0521 - acc: 0.4000\n",
      "Epoch 116/150\n",
      "40/40 [==============================] - 3s 85ms/step - loss: 1.0434 - acc: 0.5500\n",
      "Epoch 117/150\n",
      "40/40 [==============================] - 3s 84ms/step - loss: 1.0494 - acc: 0.5000\n",
      "Epoch 118/150\n",
      "40/40 [==============================] - 3s 84ms/step - loss: 1.0631 - acc: 0.3500\n",
      "Epoch 119/150\n",
      "40/40 [==============================] - 3s 85ms/step - loss: 1.0385 - acc: 0.4750\n",
      "Epoch 120/150\n",
      "40/40 [==============================] - 3s 82ms/step - loss: 1.0447 - acc: 0.4500\n",
      "Epoch 121/150\n",
      "40/40 [==============================] - 3s 83ms/step - loss: 1.0440 - acc: 0.5500\n",
      "Epoch 122/150\n",
      "40/40 [==============================] - 3s 84ms/step - loss: 1.0437 - acc: 0.4750\n",
      "Epoch 123/150\n",
      "40/40 [==============================] - 3s 84ms/step - loss: 1.0341 - acc: 0.5000\n",
      "Epoch 124/150\n",
      "40/40 [==============================] - 3s 80ms/step - loss: 1.0613 - acc: 0.4000\n",
      "Epoch 125/150\n",
      "40/40 [==============================] - 3s 77ms/step - loss: 1.0442 - acc: 0.4500\n",
      "Epoch 126/150\n",
      "40/40 [==============================] - 3s 76ms/step - loss: 1.0444 - acc: 0.4250\n",
      "Epoch 127/150\n",
      "40/40 [==============================] - 3s 75ms/step - loss: 1.0346 - acc: 0.4750\n",
      "Epoch 128/150\n",
      "40/40 [==============================] - 3s 76ms/step - loss: 1.0567 - acc: 0.4750\n",
      "Epoch 129/150\n",
      "40/40 [==============================] - 3s 76ms/step - loss: 1.0549 - acc: 0.4750\n",
      "Epoch 130/150\n",
      "40/40 [==============================] - 3s 76ms/step - loss: 1.0494 - acc: 0.4250\n",
      "Epoch 131/150\n",
      "40/40 [==============================] - 3s 73ms/step - loss: 1.0433 - acc: 0.5000\n",
      "Epoch 132/150\n",
      "40/40 [==============================] - 3s 75ms/step - loss: 1.0610 - acc: 0.4750\n",
      "Epoch 133/150\n",
      "40/40 [==============================] - 3s 74ms/step - loss: 1.0369 - acc: 0.4750\n",
      "Epoch 134/150\n",
      "40/40 [==============================] - 3s 75ms/step - loss: 1.0406 - acc: 0.5250\n",
      "Epoch 135/150\n",
      "40/40 [==============================] - 3s 73ms/step - loss: 1.0394 - acc: 0.5250\n",
      "Epoch 136/150\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.0425 - acc: 0.4500\n",
      "Epoch 137/150\n",
      "40/40 [==============================] - 3s 71ms/step - loss: 1.0371 - acc: 0.5500\n",
      "Epoch 138/150\n",
      "40/40 [==============================] - 3s 75ms/step - loss: 1.0500 - acc: 0.5000\n",
      "Epoch 139/150\n",
      "40/40 [==============================] - 3s 74ms/step - loss: 1.0318 - acc: 0.5000\n",
      "Epoch 140/150\n",
      "40/40 [==============================] - 3s 72ms/step - loss: 1.0571 - acc: 0.4250\n",
      "Epoch 141/150\n",
      "40/40 [==============================] - 3s 70ms/step - loss: 1.0328 - acc: 0.5000\n",
      "Epoch 142/150\n",
      "40/40 [==============================] - 3s 74ms/step - loss: 1.0455 - acc: 0.5000\n",
      "Epoch 143/150\n",
      "40/40 [==============================] - 3s 73ms/step - loss: 1.0331 - acc: 0.5750\n",
      "Epoch 144/150\n",
      "40/40 [==============================] - 3s 73ms/step - loss: 1.0519 - acc: 0.4750\n",
      "Epoch 145/150\n",
      "40/40 [==============================] - 3s 72ms/step - loss: 1.0479 - acc: 0.4750\n",
      "Epoch 146/150\n",
      "40/40 [==============================] - 3s 71ms/step - loss: 1.0366 - acc: 0.4750\n",
      "Epoch 147/150\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.0285 - acc: 0.5000\n",
      "Epoch 148/150\n",
      "40/40 [==============================] - 3s 69ms/step - loss: 1.0367 - acc: 0.5000\n",
      "Epoch 149/150\n",
      "40/40 [==============================] - 3s 72ms/step - loss: 1.0493 - acc: 0.5250\n",
      "Epoch 150/150\n",
      "40/40 [==============================] - 3s 73ms/step - loss: 1.0381 - acc: 0.4750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1abe7f7ffd0>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting with smallest lr from LSTM. If in doubt I will test 0.00005\n",
    "adamG3 = keras.optimizers.Adam(lr = 0.000001) # 0.0001, 0.0002, 0.00001\n",
    "gruGloVe_modelG3.compile(optimizer=adamG3, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "gruGloVe_modelG3.fit(X_train_indicesG3, y_train_labels, batch_size=4, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "10/10 [==============================] - 1s 120ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9966440200805664, 0.5]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gruGloVe_modelG3.evaluate(X_test_indicesG3, y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  [[0.3360129  0.33206347 0.33192357]\n",
      " [0.33601475 0.33206233 0.33192298]\n",
      " [0.3360129  0.33206347 0.33192357]\n",
      " [0.38113654 0.395516   0.22334747]\n",
      " [0.4143002  0.35358083 0.23211892]\n",
      " [0.40796062 0.38618457 0.20585483]\n",
      " [0.36244792 0.39137244 0.2461796 ]\n",
      " [0.3360129  0.33206347 0.33192357]\n",
      " [0.3634844  0.4212177  0.21529791]\n",
      " [0.43033394 0.37529197 0.1943741 ]]\n",
      "actual:  [[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "predsG3 = gruGloVe_modelG3.predict(X_test_indicesG3)\n",
    "print('predicted: ',predsG3)\n",
    "print('actual: ',y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "      Expect inflation       0.43      0.75      0.55         4\n",
      "Inflation will go away       0.67      0.40      0.50         5\n",
      "               Neutral       0.00      0.00      0.00         1\n",
      "\n",
      "              accuracy                           0.50        10\n",
      "             macro avg       0.37      0.38      0.35        10\n",
      "          weighted avg       0.50      0.50      0.47        10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Expect inflation', 'Inflation will go away', 'Neutral']\n",
    "clsf_rep_9 = classification_report(np.argmax(y_test_labels,axis=1),np.argmax(predsG3,axis=1), target_names=target_names)\n",
    "print(clsf_rep_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM\n",
      "MaxLen=300 | lr=0.0001 | epochs=100\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "      Expect inflation       0.67      0.50      0.57         4\n",
      "Inflation will go away       0.71      1.00      0.83         5\n",
      "               Neutral       0.00      0.00      0.00         1\n",
      "\n",
      "              accuracy                           0.70        10\n",
      "             macro avg       0.46      0.50      0.47        10\n",
      "          weighted avg       0.62      0.70      0.65        10\n",
      "\n",
      "MaxLen=300 | lr=0.00001 | epochs=100\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "      Expect inflation       0.50      1.00      0.67         4\n",
      "Inflation will go away       1.00      0.40      0.57         5\n",
      "               Neutral       0.00      0.00      0.00         1\n",
      "\n",
      "              accuracy                           0.60        10\n",
      "             macro avg       0.50      0.47      0.41        10\n",
      "          weighted avg       0.70      0.60      0.55        10\n",
      "\n",
      "MaxLen=300 | lr=0.0001 | epochs=150\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "      Expect inflation       0.33      0.75      0.46         4\n",
      "Inflation will go away       0.00      0.00      0.00         5\n",
      "               Neutral       0.00      0.00      0.00         1\n",
      "\n",
      "              accuracy                           0.30        10\n",
      "             macro avg       0.11      0.25      0.15        10\n",
      "          weighted avg       0.13      0.30      0.18        10\n",
      "\n",
      "MaxLen=150 | lr=0.00005 | epochs=100\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "      Expect inflation       0.38      0.75      0.50         4\n",
      "Inflation will go away       0.50      0.20      0.29         5\n",
      "               Neutral       0.00      0.00      0.00         1\n",
      "\n",
      "              accuracy                           0.40        10\n",
      "             macro avg       0.29      0.32      0.26        10\n",
      "          weighted avg       0.40      0.40      0.34        10\n",
      "\n",
      "GRU\n",
      "MaxLen=150 | lr=0.00001 | epochs=100\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "      Expect inflation       0.40      1.00      0.57         4\n",
      "Inflation will go away       0.00      0.00      0.00         5\n",
      "               Neutral       0.00      0.00      0.00         1\n",
      "\n",
      "              accuracy                           0.40        10\n",
      "             macro avg       0.13      0.33      0.19        10\n",
      "          weighted avg       0.16      0.40      0.23        10\n",
      "\n",
      "MaxLen=300 | lr=0.00001 | epochs=100\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "      Expect inflation       0.40      1.00      0.57         4\n",
      "Inflation will go away       0.00      0.00      0.00         5\n",
      "               Neutral       0.00      0.00      0.00         1\n",
      "\n",
      "              accuracy                           0.40        10\n",
      "             macro avg       0.13      0.33      0.19        10\n",
      "          weighted avg       0.16      0.40      0.23        10\n",
      "\n",
      "MaxLen=150 | lr=0.00001 | epochs=150\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "      Expect inflation       0.43      0.75      0.55         4\n",
      "Inflation will go away       0.67      0.40      0.50         5\n",
      "               Neutral       0.00      0.00      0.00         1\n",
      "\n",
      "              accuracy                           0.50        10\n",
      "             macro avg       0.37      0.38      0.35        10\n",
      "          weighted avg       0.50      0.50      0.47        10\n",
      "\n",
      "Bidirectional LSTM\n",
      "MaxLen=150 | lr=0.00001 | epochs=100\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "      Expect inflation       0.40      1.00      0.57         4\n",
      "Inflation will go away       0.00      0.00      0.00         5\n",
      "               Neutral       0.00      0.00      0.00         1\n",
      "\n",
      "              accuracy                           0.40        10\n",
      "             macro avg       0.13      0.33      0.19        10\n",
      "          weighted avg       0.16      0.40      0.23        10\n",
      "\n",
      "Bidirectional GRU\n",
      "MaxLen=150 | lr=0.00001 | epochs=100\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "      Expect inflation       0.40      1.00      0.57         4\n",
      "Inflation will go away       0.00      0.00      0.00         5\n",
      "               Neutral       0.00      0.00      0.00         1\n",
      "\n",
      "              accuracy                           0.40        10\n",
      "             macro avg       0.13      0.33      0.19        10\n",
      "          weighted avg       0.16      0.40      0.23        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('LSTM')\n",
    "print('MaxLen=300 | lr=0.0001 | epochs=100')\n",
    "print(clsf_rep_1)\n",
    "print('MaxLen=300 | lr=0.00001 | epochs=100')\n",
    "print(clsf_rep_2)\n",
    "print('MaxLen=300 | lr=0.0001 | epochs=150')\n",
    "print(clsf_rep_3)\n",
    "print('MaxLen=150 | lr=0.00005 | epochs=100')\n",
    "print(clsf_rep_4)\n",
    "\n",
    "print('GRU')\n",
    "print('MaxLen=150 | lr=0.00001 | epochs=100')\n",
    "print(clsf_rep_5)\n",
    "print('MaxLen=300 | lr=0.00001 | epochs=100')\n",
    "print(clsf_rep_8)\n",
    "print('MaxLen=150 | lr=0.00001 | epochs=150')\n",
    "print(clsf_rep_9)\n",
    "\n",
    "print('Bidirectional LSTM')\n",
    "print('MaxLen=150 | lr=0.00001 | epochs=100')\n",
    "print(clsf_rep_6)\n",
    "\n",
    "print('Bidirectional GRU')\n",
    "print('MaxLen=150 | lr=0.00001 | epochs=100')\n",
    "print(clsf_rep_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <--------- Project Ends Here --------->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting max len to 300\n",
    "maxLenG4 = 500 ## 150 or 300\n",
    "\n",
    "embedding_layer = Embedding(input_dim=vocab_len, output_dim=embed_vector_len, \n",
    "                            input_length=maxLenG4, weights = [emb_matrix], trainable=True)\n",
    "\n",
    "\n",
    "X_train_indicesG4 = tokenizer.texts_to_sequences(X_train_prepros)\n",
    "X_train_indicesG4 = pad_sequences(X_train_indices, maxlen=maxLenG4, padding='post')\n",
    "\n",
    "X_test_indicesG4 = tokenizer.texts_to_sequences(X_test_prepros)\n",
    "X_test_indicesG4 = pad_sequences(X_test_indices, maxlen=maxLenG4, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 500, 100)          469300    \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (None, 128)               87936     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 574,135\n",
      "Trainable params: 574,135\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gruGloVe_modelG4 = gruArchitecture((maxLenG4,))\n",
    "gruGloVe_modelG4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "40/40 [==============================] - 9s 214ms/step - loss: 1.0986 - acc: 0.3250\n",
      "Epoch 2/150\n",
      "40/40 [==============================] - 7s 169ms/step - loss: 1.0986 - acc: 0.5500\n",
      "Epoch 3/150\n",
      "40/40 [==============================] - 11s 266ms/step - loss: 1.0986 - acc: 0.5500\n",
      "Epoch 4/150\n",
      "40/40 [==============================] - 10s 242ms/step - loss: 1.0986 - acc: 0.5500\n",
      "Epoch 5/150\n",
      "40/40 [==============================] - 9s 234ms/step - loss: 1.0986 - acc: 0.5500\n",
      "Epoch 6/150\n",
      "40/40 [==============================] - 9s 235ms/step - loss: 1.0986 - acc: 0.5500\n",
      "Epoch 7/150\n",
      "40/40 [==============================] - 10s 239ms/step - loss: 1.0986 - acc: 0.5500\n",
      "Epoch 8/150\n",
      "40/40 [==============================] - 10s 248ms/step - loss: 1.0986 - acc: 0.5500\n",
      "Epoch 9/150\n",
      "40/40 [==============================] - 10s 245ms/step - loss: 1.0986 - acc: 0.5500\n",
      "Epoch 10/150\n",
      "40/40 [==============================] - 10s 242ms/step - loss: 1.0986 - acc: 0.5500\n",
      "Epoch 11/150\n",
      "40/40 [==============================] - 9s 214ms/step - loss: 1.0986 - acc: 0.5500\n",
      "Epoch 12/150\n",
      "40/40 [==============================] - 8s 206ms/step - loss: 1.0986 - acc: 0.5500\n",
      "Epoch 13/150\n",
      "40/40 [==============================] - 9s 218ms/step - loss: 1.0986 - acc: 0.5500\n",
      "Epoch 14/150\n",
      "40/40 [==============================] - 8s 203ms/step - loss: 1.0986 - acc: 0.5500\n",
      "Epoch 15/150\n",
      "40/40 [==============================] - 9s 217ms/step - loss: 1.0986 - acc: 0.5500\n",
      "Epoch 16/150\n",
      "40/40 [==============================] - 8s 206ms/step - loss: 1.0986 - acc: 0.5500\n",
      "Epoch 17/150\n",
      "40/40 [==============================] - 10s 238ms/step - loss: 1.0986 - acc: 0.5500\n",
      "Epoch 18/150\n",
      "40/40 [==============================] - 9s 218ms/step - loss: 1.0986 - acc: 0.5500\n",
      "Epoch 19/150\n",
      "40/40 [==============================] - 9s 234ms/step - loss: 1.0986 - acc: 0.5500\n",
      "Epoch 20/150\n",
      "40/40 [==============================] - 10s 262ms/step - loss: 1.0986 - acc: 0.5500\n",
      "Epoch 21/150\n",
      "40/40 [==============================] - 10s 249ms/step - loss: 1.0986 - acc: 0.5500\n",
      "Epoch 22/150\n",
      "40/40 [==============================] - 10s 256ms/step - loss: 1.0986 - acc: 0.5500\n",
      "Epoch 23/150\n",
      "40/40 [==============================] - 9s 221ms/step - loss: 1.0986 - acc: 0.5500\n",
      "Epoch 24/150\n",
      "40/40 [==============================] - 9s 228ms/step - loss: 1.0986 - acc: 0.5500\n",
      "Epoch 25/150\n",
      "40/40 [==============================] - 9s 225ms/step - loss: 1.0986 - acc: 0.5500\n",
      "Epoch 26/150\n",
      "40/40 [==============================] - 9s 219ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 27/150\n",
      "40/40 [==============================] - 9s 235ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 28/150\n",
      "40/40 [==============================] - 9s 215ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 29/150\n",
      "40/40 [==============================] - 10s 255ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 30/150\n",
      "40/40 [==============================] - 9s 224ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 31/150\n",
      "40/40 [==============================] - 9s 234ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 32/150\n",
      "40/40 [==============================] - 10s 257ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 33/150\n",
      "40/40 [==============================] - 11s 265ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 34/150\n",
      "40/40 [==============================] - 11s 281ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 35/150\n",
      "40/40 [==============================] - 10s 244ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 36/150\n",
      "40/40 [==============================] - 9s 223ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 37/150\n",
      "40/40 [==============================] - 9s 234ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 38/150\n",
      "40/40 [==============================] - 9s 219ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 39/150\n",
      "40/40 [==============================] - 10s 239ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 40/150\n",
      "40/40 [==============================] - 9s 227ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 41/150\n",
      "40/40 [==============================] - 9s 227ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 42/150\n",
      "40/40 [==============================] - 10s 239ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 43/150\n",
      "40/40 [==============================] - 10s 260ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 44/150\n",
      "40/40 [==============================] - 11s 265ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 45/150\n",
      "40/40 [==============================] - 11s 270ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 46/150\n",
      "40/40 [==============================] - 9s 228ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 47/150\n",
      "40/40 [==============================] - 10s 240ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 48/150\n",
      "40/40 [==============================] - 9s 229ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 49/150\n",
      "40/40 [==============================] - 10s 238ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 50/150\n",
      "40/40 [==============================] - 9s 233ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 51/150\n",
      "40/40 [==============================] - 10s 255ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 52/150\n",
      "40/40 [==============================] - 10s 251ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 53/150\n",
      "40/40 [==============================] - 9s 232ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 54/150\n",
      "40/40 [==============================] - 10s 251ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 55/150\n",
      "40/40 [==============================] - 11s 278ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 56/150\n",
      "40/40 [==============================] - 11s 265ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 57/150\n",
      "40/40 [==============================] - 11s 283ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 58/150\n",
      "40/40 [==============================] - 10s 242ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 59/150\n",
      "40/40 [==============================] - 9s 230ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 60/150\n",
      "40/40 [==============================] - 10s 251ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 61/150\n",
      "40/40 [==============================] - 9s 237ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 62/150\n",
      "40/40 [==============================] - 9s 230ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 63/150\n",
      "40/40 [==============================] - 9s 237ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 64/150\n",
      "40/40 [==============================] - 9s 225ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 65/150\n",
      "40/40 [==============================] - 10s 245ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 66/150\n",
      "40/40 [==============================] - 9s 233ms/step - loss: 1.0985 - acc: 0.5500\n",
      "Epoch 67/150\n",
      "40/40 [==============================] - 11s 272ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 68/150\n",
      "40/40 [==============================] - 11s 268ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 69/150\n",
      "40/40 [==============================] - 11s 269ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 70/150\n",
      "40/40 [==============================] - 9s 235ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 71/150\n",
      "40/40 [==============================] - 9s 237ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 72/150\n",
      "40/40 [==============================] - 9s 225ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 73/150\n",
      "40/40 [==============================] - 10s 239ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 74/150\n",
      "40/40 [==============================] - 9s 232ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 75/150\n",
      "40/40 [==============================] - 11s 264ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 76/150\n",
      "40/40 [==============================] - 10s 241ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 77/150\n",
      "40/40 [==============================] - 9s 225ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 78/150\n",
      "40/40 [==============================] - 10s 243ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 79/150\n",
      "40/40 [==============================] - 11s 269ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 80/150\n",
      "40/40 [==============================] - 11s 276ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 81/150\n",
      "40/40 [==============================] - 11s 278ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 82/150\n",
      "40/40 [==============================] - 10s 245ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 83/150\n",
      "40/40 [==============================] - 9s 234ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 84/150\n",
      "40/40 [==============================] - 10s 241ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 85/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 9s 225ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 86/150\n",
      "40/40 [==============================] - 10s 244ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 87/150\n",
      "40/40 [==============================] - 9s 233ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 88/150\n",
      "40/40 [==============================] - 10s 243ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 89/150\n",
      "40/40 [==============================] - 9s 237ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 90/150\n",
      "40/40 [==============================] - 9s 224ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 91/150\n",
      "40/40 [==============================] - 10s 261ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 92/150\n",
      "40/40 [==============================] - 11s 274ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 93/150\n",
      "40/40 [==============================] - 11s 272ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 94/150\n",
      "40/40 [==============================] - 10s 258ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 95/150\n",
      "40/40 [==============================] - 9s 230ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 96/150\n",
      "40/40 [==============================] - 10s 238ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 97/150\n",
      "40/40 [==============================] - 9s 237ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 98/150\n",
      "40/40 [==============================] - 9s 227ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 99/150\n",
      "40/40 [==============================] - 10s 243ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 100/150\n",
      "40/40 [==============================] - 9s 234ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 101/150\n",
      "40/40 [==============================] - 10s 242ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 102/150\n",
      "40/40 [==============================] - 9s 236ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 103/150\n",
      "40/40 [==============================] - 10s 248ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 104/150\n",
      "40/40 [==============================] - 11s 281ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 105/150\n",
      "40/40 [==============================] - 11s 270ms/step - loss: 1.0984 - acc: 0.5500\n",
      "Epoch 106/150\n",
      "40/40 [==============================] - 10s 257ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 107/150\n",
      "40/40 [==============================] - 10s 248ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 108/150\n",
      "40/40 [==============================] - 9s 229ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 109/150\n",
      "40/40 [==============================] - 9s 233ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 110/150\n",
      "40/40 [==============================] - 9s 235ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 111/150\n",
      "40/40 [==============================] - 10s 242ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 112/150\n",
      "40/40 [==============================] - 10s 243ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 113/150\n",
      "40/40 [==============================] - 9s 228ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 114/150\n",
      "40/40 [==============================] - 10s 241ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 115/150\n",
      "40/40 [==============================] - 10s 242ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 116/150\n",
      "40/40 [==============================] - 11s 274ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 117/150\n",
      "40/40 [==============================] - 11s 280ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 118/150\n",
      "40/40 [==============================] - 10s 262ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 119/150\n",
      "40/40 [==============================] - 9s 232ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 120/150\n",
      "40/40 [==============================] - 10s 243ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 121/150\n",
      "40/40 [==============================] - 9s 230ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 122/150\n",
      "40/40 [==============================] - 10s 240ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 123/150\n",
      "40/40 [==============================] - 10s 240ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 124/150\n",
      "40/40 [==============================] - 9s 234ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 125/150\n",
      "40/40 [==============================] - 10s 241ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 126/150\n",
      "40/40 [==============================] - 9s 224ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 127/150\n",
      "40/40 [==============================] - 10s 242ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 128/150\n",
      "40/40 [==============================] - 11s 267ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 129/150\n",
      "40/40 [==============================] - 11s 277ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 130/150\n",
      "40/40 [==============================] - 11s 284ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 131/150\n",
      "40/40 [==============================] - 10s 240ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 132/150\n",
      "40/40 [==============================] - 9s 225ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 133/150\n",
      "40/40 [==============================] - 10s 241ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 134/150\n",
      "40/40 [==============================] - 9s 227ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 135/150\n",
      "40/40 [==============================] - 10s 244ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 136/150\n",
      "40/40 [==============================] - 9s 235ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 137/150\n",
      "40/40 [==============================] - 9s 232ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 138/150\n",
      "40/40 [==============================] - 10s 241ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 139/150\n",
      "40/40 [==============================] - 9s 224ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 140/150\n",
      "40/40 [==============================] - 11s 267ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 141/150\n",
      "40/40 [==============================] - 11s 276ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 142/150\n",
      "40/40 [==============================] - 11s 274ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 143/150\n",
      "40/40 [==============================] - 10s 254ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 144/150\n",
      "40/40 [==============================] - 9s 237ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 145/150\n",
      "40/40 [==============================] - 9s 233ms/step - loss: 1.0983 - acc: 0.5500\n",
      "Epoch 146/150\n",
      "40/40 [==============================] - 10s 241ms/step - loss: 1.0982 - acc: 0.5500\n",
      "Epoch 147/150\n",
      "40/40 [==============================] - 9s 227ms/step - loss: 1.0982 - acc: 0.5500\n",
      "Epoch 148/150\n",
      "40/40 [==============================] - 10s 243ms/step - loss: 1.0982 - acc: 0.5500\n",
      "Epoch 149/150\n",
      "40/40 [==============================] - 9s 235ms/step - loss: 1.0982 - acc: 0.5500\n",
      "Epoch 150/150\n",
      "40/40 [==============================] - 9s 230ms/step - loss: 1.0982 - acc: 0.5500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1abebb4cf98>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting with smallest lr from LSTM. If in doubt I will test 0.00005\n",
    "adamG4 = keras.optimizers.Adam(lr = 0.000001) # 0.0001, 0.0002, 0.00001\n",
    "gruGloVe_modelG4.compile(optimizer=adamG4, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "gruGloVe_modelG4.fit(X_train_indicesG4, y_train_labels, batch_size=4, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "10/10 [==============================] - 2s 153ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.098435878753662, 0.4000000059604645]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gruGloVe_modelG4.evaluate(X_test_indicesG4, y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  [[0.33370975 0.3331983  0.33309197]\n",
      " [0.33370975 0.3331983  0.33309197]\n",
      " [0.33370975 0.3331983  0.33309197]\n",
      " [0.33370975 0.3331983  0.33309197]\n",
      " [0.33370975 0.3331983  0.33309197]\n",
      " [0.33370975 0.3331983  0.33309197]\n",
      " [0.33370975 0.3331983  0.33309197]\n",
      " [0.33370975 0.3331983  0.33309197]\n",
      " [0.33370975 0.3331983  0.33309197]\n",
      " [0.33370975 0.3331983  0.33309197]]\n",
      "actual:  [[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "predsG4 = gruGloVe_modelG4.predict(X_test_indicesG4)\n",
    "print('predicted: ',predsG4)\n",
    "print('actual: ',y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "      Expect inflation       0.40      1.00      0.57         4\n",
      "Inflation will go away       0.00      0.00      0.00         5\n",
      "               Neutral       0.00      0.00      0.00         1\n",
      "\n",
      "              accuracy                           0.40        10\n",
      "             macro avg       0.13      0.33      0.19        10\n",
      "          weighted avg       0.16      0.40      0.23        10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\rzamb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Expect inflation', 'Inflation will go away', 'Neutral']\n",
    "clsf_rep_10 = classification_report(np.argmax(y_test_labels,axis=1),np.argmax(predsG4,axis=1), target_names=target_names)\n",
    "print(clsf_rep_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bidirectional LSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follows Keras examples: https://keras.io/examples/nlp/bidirectional_lstm_imdb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000  # Only consider the top 20k words\n",
    "maxlen = 3000  # Only consider the first 3000 words of each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Input for variable-length sequences of integers: shape=None\n",
    "def biDirect_lstmArch(input_shape=(None,)): \n",
    "\n",
    "    inputs = Input(input_shape, dtype=\"int32\")\n",
    "\n",
    "    embeddings = embedding_layer(inputs)\n",
    "    \n",
    "    # Add 2 bidirectional LSTMs\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True))(embeddings)\n",
    "    x = Bidirectional(LSTM(64))(x)\n",
    "    \n",
    "    # Add a classifier\n",
    "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    #X = Dropout(0.6)(X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "biDict_lstmGloVe_model = biDirect_lstmArch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        multiple                  432200    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, None, 128)         84480     \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 615,625\n",
      "Trainable params: 183,425\n",
      "Non-trainable params: 432,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "biDict_lstmGloVe_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "40/40 [==============================] - 353s 9s/step - loss: 0.7408 - acc: 0.5250\n",
      "Epoch 2/3\n",
      "40/40 [==============================] - 291s 7s/step - loss: 0.6709 - acc: 0.5750\n",
      "Epoch 3/3\n",
      "40/40 [==============================] - 341s 9s/step - loss: 0.6292 - acc: 0.2500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x11351024da0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biDict_lstmGloVe_model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "biDict_lstmGloVe_model.fit(X_train_indices, y_train_encoded, batch_size=32, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "10/10 [==============================] - 2s 206ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7221247553825378, 0.5]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biDict_lstmGloVe_model.evaluate(X_test_indices, y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds2 = lstmGloVe_model.predict(X_test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  [[0.50194865]\n",
      " [0.50194865]\n",
      " [0.50194865]\n",
      " [0.50194865]\n",
      " [0.50194865]\n",
      " [0.50194865]\n",
      " [0.50194865]\n",
      " [0.50194865]\n",
      " [0.50194865]\n",
      " [0.50194865]]\n",
      "actual:  [1 0 1 0 0 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print('predicted: ',preds2)\n",
    "print('actual: ',y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "inflation used to be the scourge of the world economy and the bane of american presidents in 1971 amid an overheating economy richard nixon took to television to announce a freeze on “all prices and wages throughout the united states” a board of bureaucrats ruled on what this meant for everything from golf club memberships to commodity futures gerald ford nixon’s successor preferred a grassroots approach he distributed buttons bearing his slogan win for “whip inflation now” ronald reagan running for office four years later amid another surge in prices declared inflation to be “as violent as a mugger as frightening as an armed robber and as deadly as a hit man”  today the lethal assassin has gone missing most economies no longer struggle with runaway prices instead they find inflation is too low as judged by their inflation targets a decade of interest rates at or near rockbottom has not changed that nor has the printing of money by central banks in america the euro zone britain and japan that has expanded their balancesheets beyond a combined 15trn 35 of their combined gdp nor have unemployment rates that are in many countries the lowest they have been for decades  the imf counts among its members 41 countries in which monetarypolicy targets inflation add in the eurozone and america where the fed has multiple goals and you get 43 of those 28 will either undershoot their inflation targets in 2019 or have inflation in the bottom half of their target range according to the fund’s most recent round of forecasts when those forecasts are updated on october 15th after this special report goes to press that number will probably rise by gdp 91 of the inflationtargeting world is an inflation laggard on this measure that includes nearly all the advanced economies under examination—iceland is the sole exception—and more than half of the emerging markets  this shift in the inflation landscape reflects both the successes and the failures of economic policy the advent of inflation targeting central banks since the 1990s has gradually immunised economies against runaway prices but policymakers seem either unwilling or unable to stop inflation falling short of their targets this special report will argue that anchored inflation expectations technological change and the flowof goods and capital across borders have conspired to make inflation a less meaningful—and less malleable—economic indicator central banks are therefore finding their targets harder to hit at the same time constraints on monetary policy mean that the risk of inflation shortfalls looms larger than that of excessive price rises central bankers and politicians must find ways to adapt economic policy to this new world  disinflation nations  low inflation is striking over both the long term and the short term in the long term it is the culmination of adecadeslong trend the rich world conquered runaway prices by the late 1990s as governments made centralbanks independent and gave them inflation targets in the 2000s and the early 2010s commodityprice boom skept prices rising at a decent clip but since the oil price crashed in 2014 inflation above 2 has been rare in emerging markets it is higher but the direction of change is the same see chart for nearly two decades economists have talked of an era of “global disinflation”  in the short term low inflation is especially striking because it seems to defy the “phillips curve” the supposed inverse relationship between inflation and unemployment in twothirds of countries in the oecd a club of mostly rich countries a record proportion of 15 to 64yearolds have jobs according to the models taught in economics courses and used by central banks a jobs boom on this scale should have brought accelerating prices and wages for the most part it has not  central bankers have been caught out for years they have promised that jobs growth would soon be over and inflation would rise they have repeatedly been proved wrong and are conscious of their mistakes in february 2016 mario draghi the outgoing head of the european central bank ecb described whether inflation targets can be met as “the most fundamental question facing all major central banks” mark carney governor of the bank of england recently warned of an “increasingly untenable” economicpolicy consensus in march this year jerome powell the fed’s chairman said low global inflation was “one of the majorchallenges of our time” the fed’s failure to hit its inflation target has encouraged an assault by president donald trump who is incensed that in 2018 mr powell slowed growth by raising interest rates to see off aninflationary threat that has not yet materialised  the disease of the 1970s and 1980s was simultaneous high inflation and high unemployment that both are now low might seem like cause for celebration certainly inflation below target is a better problem to have than runaway prices but it poses problems for three reasons first it represents a missed opportunity monetary policy could have been looser and hence growth faster without price pressures taking off secondcentral banks missing their inflation targets undermines their credibility in europe markets’ long term inflation expectations have sunk to little over 1 lower than when the ecb started its quantitativeeasing programme in early 2015 despite an inflation target of below but close to 2 when inflation targets are not credible the future is more likely to spring a costly surprise unexpectedly low inflation causes lenders to profit and borrowers to suffer because debts do not shrink as fast in real terms as they were expected to when loans were agreed  most important low inflation can be selfreinforcing more significant than the nominal interest rate set by central banks is the real interest rate which adjusts for inflation as the public comes to expect lower inflation the real rate rises weakening demand and pushing inflation down even more that would not be aproblem if central banks could cut the nominal rate further to fight the disinflationary slump but they have little room to do so in europe and japan nominal interest rates are already below zero they are near zero in britain and only a little higher in america though the exact location of the lower bound on interest rates is uncertain it exists somewhere because the public always has the option of holding cash at a zero nominal return  why has inflation reached this curious—and precarious—point some would argue that inflation is falling short because governments have lost the ability to boost prices this cannot be true if it were they could cut taxes to zero boost spending print money to finance the resulting deficits and never see an inflationary downside inflation will always respond eventually to a determined policymaker who has access to interest rates and the printing presses governments can always debase their currencies as high inflation in argentina and turkey shows  this might suggest that belowtarget inflation reflects only a failure of ambition but that is not right either inflation has become harder to finetune because economies have changed in ways that are not yet fully understood monetary policy must not just become more ambitious but also adapt to rely less on failing models and to take a longerterm view and while central banks are hamstrung by low rates fighting low inflation will increasingly fall to fiscal policy the case for reform rests first on an understanding of where economic models have gone wrong\n",
      "Inflation will fade away\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "n = np.random.randint(0,10)\n",
    "print(n)\n",
    "print(X_test[n])\n",
    "print(y_test[n])\n",
    "print(y_test_encoded[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeledArticle(articleID=4, label='Inflation will fade away', body='inflation used to be the scourge of the world economy and the bane of american presidents in 1971 amid an overheating economy richard nixon took to television to announce a freeze on “all prices and wages throughout the united states” a board of bureaucrats ruled on what this meant for everything from golf club memberships to commodity futures gerald ford nixon’s successor preferred a grassroots approach he distributed buttons bearing his slogan win for “whip inflation now” ronald reagan running for office four years later amid another surge in prices declared inflation to be “as violent as a mugger as frightening as an armed robber and as deadly as a hit man”  today the lethal assassin has gone missing most economies no longer struggle with runaway prices instead they find inflation is too low as judged by their inflation targets a decade of interest rates at or near rockbottom has not changed that nor has the printing of money by central banks in america the euro zone britain and japan that has expanded their balancesheets beyond a combined 15trn 35 of their combined gdp nor have unemployment rates that are in many countries the lowest they have been for decades  the imf counts among its members 41 countries in which monetarypolicy targets inflation add in the eurozone and america where the fed has multiple goals and you get 43 of those 28 will either undershoot their inflation targets in 2019 or have inflation in the bottom half of their target range according to the fund’s most recent round of forecasts when those forecasts are updated on october 15th after this special report goes to press that number will probably rise by gdp 91 of the inflationtargeting world is an inflation laggard on this measure that includes nearly all the advanced economies under examination—iceland is the sole exception—and more than half of the emerging markets  this shift in the inflation landscape reflects both the successes and the failures of economic policy the advent of inflation targeting central banks since the 1990s has gradually immunised economies against runaway prices but policymakers seem either unwilling or unable to stop inflation falling short of their targets this special report will argue that anchored inflation expectations technological change and the flowof goods and capital across borders have conspired to make inflation a less meaningful—and less malleable—economic indicator central banks are therefore finding their targets harder to hit at the same time constraints on monetary policy mean that the risk of inflation shortfalls looms larger than that of excessive price rises central bankers and politicians must find ways to adapt economic policy to this new world  disinflation nations  low inflation is striking over both the long term and the short term in the long term it is the culmination of adecadeslong trend the rich world conquered runaway prices by the late 1990s as governments made centralbanks independent and gave them inflation targets in the 2000s and the early 2010s commodityprice boom skept prices rising at a decent clip but since the oil price crashed in 2014 inflation above 2 has been rare in emerging markets it is higher but the direction of change is the same see chart for nearly two decades economists have talked of an era of “global disinflation”  in the short term low inflation is especially striking because it seems to defy the “phillips curve” the supposed inverse relationship between inflation and unemployment in twothirds of countries in the oecd a club of mostly rich countries a record proportion of 15 to 64yearolds have jobs according to the models taught in economics courses and used by central banks a jobs boom on this scale should have brought accelerating prices and wages for the most part it has not  central bankers have been caught out for years they have promised that jobs growth would soon be over and inflation would rise they have repeatedly been proved wrong and are conscious of their mistakes in february 2016 mario draghi the outgoing head of the european central bank ecb described whether inflation targets can be met as “the most fundamental question facing all major central banks” mark carney governor of the bank of england recently warned of an “increasingly untenable” economicpolicy consensus in march this year jerome powell the fed’s chairman said low global inflation was “one of the majorchallenges of our time” the fed’s failure to hit its inflation target has encouraged an assault by president donald trump who is incensed that in 2018 mr powell slowed growth by raising interest rates to see off aninflationary threat that has not yet materialised  the disease of the 1970s and 1980s was simultaneous high inflation and high unemployment that both are now low might seem like cause for celebration certainly inflation below target is a better problem to have than runaway prices but it poses problems for three reasons first it represents a missed opportunity monetary policy could have been looser and hence growth faster without price pressures taking off secondcentral banks missing their inflation targets undermines their credibility in europe markets’ long term inflation expectations have sunk to little over 1 lower than when the ecb started its quantitativeeasing programme in early 2015 despite an inflation target of below but close to 2 when inflation targets are not credible the future is more likely to spring a costly surprise unexpectedly low inflation causes lenders to profit and borrowers to suffer because debts do not shrink as fast in real terms as they were expected to when loans were agreed  most important low inflation can be selfreinforcing more significant than the nominal interest rate set by central banks is the real interest rate which adjusts for inflation as the public comes to expect lower inflation the real rate rises weakening demand and pushing inflation down even more that would not be aproblem if central banks could cut the nominal rate further to fight the disinflationary slump but they have little room to do so in europe and japan nominal interest rates are already below zero they are near zero in britain and only a little higher in america though the exact location of the lower bound on interest rates is uncertain it exists somewhere because the public always has the option of holding cash at a zero nominal return  why has inflation reached this curious—and precarious—point some would argue that inflation is falling short because governments have lost the ability to boost prices this cannot be true if it were they could cut taxes to zero boost spending print money to finance the resulting deficits and never see an inflationary downside inflation will always respond eventually to a determined policymaker who has access to interest rates and the printing presses governments can always debase their currencies as high inflation in argentina and turkey shows  this might suggest that belowtarget inflation reflects only a failure of ambition but that is not right either inflation has become harder to finetune because economies have changed in ways that are not yet fully understood monetary policy must not just become more ambitious but also adapt to rely less on failing models and to take a longerterm view and while central banks are hamstrung by low rates fighting low inflation will increasingly fall to fiscal policy the case for reform rests first on an understanding of where economic models have gone wrong')\n"
     ]
    }
   ],
   "source": [
    "sampleBody = X_test[n]\n",
    "for i in range(len(articlesSample)):\n",
    "    if articlesSample[i][2] == sampleBody:\n",
    "        print(articlesSample[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
